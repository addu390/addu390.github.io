[
  
  
  
  
    {

      "title"    : "Spatial Index: R Trees",
      "url"      : "/spatial-index-r-tree",
      "index"    : "watermelon",
      "content"  : "In this post, let's explore the R-Tree data structure, which is popularly used to store multi-dimensional data, such as data points, segments, and rectangles.\n\n1. R-Trees and Rectangles\n\nFor example, consider the plan of a university layout below. We can use the R-Tree data structure to index the buildings on the map.\n\nTo do so, we can place rectangles around a building or group of buildings and then index them. Suppose there's a much bigger section of the map signifying a larger department, and we need to query all the buildings within a department. We can use the R-Tree to find all the buildings within (partially or fully contained) the larger section (query rectangle).\n\n\nFigure 0: Layout with MBRs and Query Rectangle\n\nIn the above figure, the red rectangle represent the query rectangle, used to ask the R-Tree to get all the buildings that intersect with the query rectangle (R2, R3, R6).\n\n2. R-Tree - Intuition\n\nThe main idea in R-trees is the minimum bounding rectangles. We'll come to what \"minimum\" implies in a second.\n\nThe inner node of an R-tree is as follows: We start with the root node, representing the large landscape. The inner nodes are guideposts that hold pointers to the child nodes we need to go down to in the tree. i.e. each entry of a node points to an area of the data space (described by MBR).\n\n\nFigure 1: R-Tree Inner Node\n\nFor instance, think of a Binary Search Tree. From the root node, we make a decision to go left or right. The R-tree is similar, but more of an M-way tree, where each node can have multiple entries as seen above. Instead of having integer or string values (one-dimensional), the inner nodes consist of entries (multi-dimensional). In the example, there are 4 entries of rectangles.\n\n2.1. MBR - Minimum Bounding Rectangle\n\n\nFigure 2: R-Tree Minimum Bounding Rectangle\n\nMinimum Bounding Rectangles, R1, R2, R3, R4, contain the objects which are stored in the sub-trees in a minimal way. For instance, say we have 3 rectangles R11, R12, R13. R1 is the smallest rectangle that can be created to completely contain all three rectangles, hence the name \"minimum.\"\n\n2.2. Search Process and Overlapping MBRs\n\nThe search process in an R-tree is simple: for a query object/query rectangle; at an inner node, it is the decision to check if any of the entries in a node intersect with the query rectangle.\n\n\nFigure 3: R-Tree Query Rectangle(s)\n\nFor example, consider a query rectangle Q1. It's clear that R1 intersects with Q1, so we would follow down the tree from R1. Similarly, Q2 intersects with R2. However, in scenarios where the query rectangle intersects with multiple entries/rectangles (Q3 with R2, R3, R4), all the intersecting rectangles have to be searched. This can happen if the indexing is not optimized and has to be avoided as it defeats the purpose of indexing in the first place.\n\n2.3. R-Tree - Properties\n\nHere's a bit of a larger example of an R-tree.\n\n\nFigure 3: R-Tree Level-2\n\nEvery node in an R-tree has between m and M entries. More specifically, each node has between m ≤ ⌈M/2⌉ and M entries. The node has at least 2 entries unless it's a leaf.\n\nBy now, if you also read the blog post on B-Trees and B+ Trees, you’ll see that an R-Tree is quite similar to a B+ Tree. It uses a similar idea to split the space at each (inner) node into multiple areas. However, B+ Trees mostly work with one-dimensional data, and the data ranges do not overlap.\n\n3. Search using an R-Tree\n\nNow that we know the idea behind R-Trees and the search process, Let's put a clear-cut definition to the search process:\n\n\nGoal: Find all rectangles that overlap with the given rectangle S (query rectangle).\nLet T denote the node (at the current level/sub-tree).\nS1 (Search in sub-trees): If T is not a leaf, check all the entries E in T. If the MBR of E overlaps with S, then continue the search in the sub-tree to which E points.\nS2 (Search in Leaves): If T is a leaf node, inspect all entries of E. All entries that overlap with S are part of the query result.\n\n\n4. Inserting to an R-Tree\n\nComing to inserts, consider a leaf node (MBR) as shown below with 3 entries/objects, R1, R2, and R3. Let's assume that the leaf is not full yet (MBR has a threshold capacity on the number of objects it can hold).\n\nSay, there's a new rectangle R4 coming and it has to be inserted inside the leaf node. As you can see, in order to capture the new objects, the MBR is adjusted, i.e., enlarged to minimally contain R1 to R4. Going on and inserting another object R5, the MBR is once again adjusted.\n\n\nFigure 4: R-Tree Insert (Adjusting MBR)\n\nOn an insert, when the MBR is updated, i.e., contains more objects, the new MBR has to be updated not only for the node but also propagated to other lower levels and potentially (not always) up to the root node. This is to reflect that the sub-tree now contains more information.\n\n4.1. Choice for Insert\n\nUnlike the example, it's not always clear in which node/sub-tree an object should be inserted. Here: MBR1, MBR2, or MBR3.\n\n\n\nThe question is, in which MBR should we insert R1 into? Setting aside any rules or justification for a second, R1 can be inserted into any MBR.\n\n\n\nInserting into MBR1 would need to immensely grow/expand MBR1 to fully contain R1. The implication? Say there's a query rectangle Q1. After leading down the sub-tree to MBR1, we find that there's nothing (no objects). This is because, to contain R1, we have expanded MBR1 so much that there is a lot of space without any objects. So, it's fair to conclude that one criterion to add is to insert into MBRs that need to expand the least.\n\n\n\nGoing by that, inserting into MBR2 is a better option as opposed to MBR1. Similarly, MBR3 may not be a bad option either, depending on the expansion factor.\n\n\n\nStating the obvious (for implementation), the minimum-bounding-rectangle (MBR) is defined as the rectangle that has the maximal and minimal values of all rectangles in each dimension.\n\n\n\n\n\nSummarizing the insertion into R-Tree so far:\n\nIn principle, a new rectangle can be inserted into any node.\nIf the node is full, a split needs to be performed (more on that in the next section).\nIf not, the MBR may have to be adjusted/expanded to accommodate new objects (as seen ).\n\n\nObservations:\n\nExtending bounding boxes is a critical factor for the performance of the R-Tree.\nTry to minimize overlap (of the MBRs).\nTry to minimize spread (the size of the MBR, as seen in section 4.1).\n\n\n4.2. Insert - Algorithm\n\nHere's the algorithm proposed by the author of the R-Tree paper \"A Dynamic Index Structure for Spatial Searching,\" by A. Guttman, 1984.\n\nThe rest of this section is mostly going over snippets of code and explanations from this paper, but with more examples and visualization.\n\nAlgorithm: Search for leaf to insert (ChooseLeaf):\n\nCS1: Let N be the root.\nCS2:\n\n    If N is a leaf, return N.\n    If N is not a leaf: Search for an entry in N whose rectangle (MBR) requires the least area increase in order to accommodate the new rectangle. In the case where there are multiple options, consider an entry that has the smallest (in area) MBR.\n\n\nCS3: Let N be the child node, then continue to step CS2 (repeat).\n\n\n\n\nA much simpler example of 8 objects, each object with one multidimensional attribute (Range or line-segments on x-axis) and one identity (Color). To insert these objects one by one in an empty R-tree of degree M = 3 (maximum number of entries at each node) and m ≥ M/2 (minimum number of entries at each node = 2).\n\n\n\nObservation: in the case where the selected leaf is already full, a splitting operation is performed. Let's understand the overflow problem better (the split problem):\n\n4.3. Handling Overflow\n\nIn the case a node/leaf is full and a new entry cannot be stored anymore, a split needs to be performed, just as for a B+ Tree. The difference is that the split can be done arbitrarily and not only in the middle as for a B+ Tree.\n\n\n\n4.3.1. The Split Problem\nGiven M + 1 entries in a node (exceeded maximum capacity per node), which two subsets of these entries should be considered as new and old nodes?\n\nTo better understand the split problem, let's take a step back and consider 4 rectangles (R1, R2, R3, R4) that need to be assigned to two nodes (MBRs) in a meaningful way.\n\n\n\nWhy is one better than the other? As mentioned before (Section 4.1), the area of expansion of the poor split is much larger compared to the good split (despite the overlap). This leads to more empty spaces in the node/MBR that do not have any objects.\n\nA realistic use case for an R-Tree is M = 50 and there are 2^(M-1) possibilities. Hence, a naive approach to look at all possible subsets and choose the best one is not practical (too expensive!).\n\n4.3.2. The Split Problem: Quadratic Cost\n\n\nSearch for split with smallest possible area\nCost is Quadratic in M and linear in number of dimensions d.\nIdea:\n\nSearch for pairs of entries that would cause the largest MBR area if placed in the same node. Then put these entries in two different nodes\nThen: Consider all remaining entries and consider the one (among the 2 nodes) for which the increase in area (of MBR) has the largest possible difference between the two nodes.\nThis entry is assigned to the node with the smallest increase. Repeat until all entries are assigned\n\n\n\n\n\nIn this example, two nodes, MBR1 and MBR2, are created. R1 and R2 in the same MBR would lead to creating the largest MBR. R3 is then inserted into MBR1 and not MBR2, as the area increase of MBR1 is smaller compared to MBR2.\n\nMethod \"AdjustTree,\" is called whenever a new entry is inserted. It is responsible for adapting the parent's MBR and propagating the changes bottom up, handling splits as well as changes to MBRs. In the worst case, the propagation can be up to the root node.\n\n5. R-Tree Variants\n\nR-trees do not guarantee good worst-case performance, but generally speaking, they perform well with real-world data. Addressing this specific problem, the Priority R-tree is a worst-case asymptotically optimal alternative to the spatial tree R-tree, which is essentially a hybrid between a k-dimensional tree (k-d tree) and an R-tree.\n\nAnother commonly used variant is the R*-Tree, which uses the same algorithm as the regular R-tree for query and delete operations. However, while inserting, the R*-tree uses a combined strategy: for leaf nodes, overlap is minimized, and for inner nodes, enlargement and area are minimized, making the tree construction slightly more expensive.\n\nThe R+-Tree, on the other hand, solves one main problem to ensure nodes do not overlap with each other, leading to better point query performance. However, it does so by inserting an object into multiple leaves if necessary, which is a disadvantage due to duplicate entries and larger tree size.\n\nThe Hilbert R-Tree uses space-filling curves, specifically the Hilbert curve, to impose a linear ordering on the data rectangles. It has two variants: Packed Hilbert R-trees, suitable for static databases in which updates are very rare, and dynamic Hilbert R-trees, suitable for dynamic databases where insertions, deletions, or updates may occur in real time.\n\n6. Conclusion\n\nR-trees have come a long way since the first paper was published in 1984. Today, their applications span over multi-dimensional indexes, computer graphics, video games, spatial data management systems, and many more.\n\nOn the flip side, R-trees can degrade badly with discrete data. Hence, it's highly recommended to understand the data representation before using R-trees. R-trees are also relatively slow when there's a very high mutation rate, i.e., where the index changes often; this is because of the higher cost for constructing and updating the index (due to tree rebalancing) and they are more optimized for various search operations. Lastly, R-trees can be a poor algorithm choice when primarily dealing with points as opposed to polygons/regions.\n\n7. References\n[1] A. Guttman, \"A Dynamic Index Structure for Spatial Searching,\" presented at the ACM SIGMOD International Conference on Management of Data, 1984. [Online]. Available: https://www.researchgate.net/publication/220805321_A_Dynamic_Index_Structure_for_Spatial_Searching.\n[2] \"R-Tree,\" Wikipedia. [Online]. Available: https://en.wikipedia.org/wiki/R-tree.\n[3] \"B-Trees and B+ Trees,\" PyBlog. [Online]. Available: https://www.pyblog.xyz/b-trees-b-plus-trees.\n[4] \"Spatial Index R-Tree,\" YouTube, https://www.youtube.com/watch?v=U0jUvvQkaFw."

    },
  
  
  
  
    {

      "title"    : "Spatial Index: Tessellation",
      "url"      : "/spatial-index-tessellation",
      "index"    : "peach",
      "content"  : "Brewing! this post a continuation of Spatial Index: Grid Systems where we will set the foundation for tessellation and delve into the details of Uber H3\n\n0. Foundation\nTessellation or tiling is the process of covering/dividing a space into smaller, non-overlapping shapes that fit together perfectly without gaps or overlaps. In spatial indexing, tessellation is used to break down the Earth's surface into manageable units for efficient data storage, querying, and analysis.\n\nThe rationale behind why a geographical grid system (Tessellation system) is necessary: The real world is cluttered with various geographical elements, both natural and man-made, none of which follow any consistent structure. To perform geographic algorithms or analyses on it, we need a more abstract form.\n\nMaps are a good start and are the most common abstraction, with which most people are familiar. However, maps still contain all sorts of inconsistencies. This calls for a grid system, which takes the cluttered geographic space and provides a more clean and structured mathematical space, making it much easier to perform computations and queries.\n\n\nFigure 0: Tessellated View of Halifax\n\nThe primary principle of the grid is to break the space into uniform cells. These cells are the units of analysis used in geographic systems. Think of it as pixels in an image.\n\nA grid system adds a couple more layers on top of this, consisting of a series of nested grids, usually at increasingly fine resolutions. They include a way to uniquely identify any cell in the system. Other common grid systems include Graticule (latitude and longitude), Quad Key  (Mercator projection), Geohash (Equirectangular projection) and Google S2 (Spherical projection).\n\n\n\n\n1. Uber H3 - Intuition\nMost systems use four-sided polygons (Square, Rectangle and Quadrilateral). H3 is the grid system developed by Uber, which uses hexagon cells as its base. It covers the space/world with hexagons and has different levels of resolution, with the smallest cells representing about 1 cm² of space.\n\n1.1. Why Hexagons?\n\nStarting off by adding rules or needs for choosing a tile, such as:\n\n(a) Uniform shape\n(b) Uniform edge length\n(c) Uniform angles\n\nBrings down the number of options, with the most commonly used shapes being squares, equilateral triangles, and hexagons.\n\n\nFigure 1: Triangle vs Square vs Hexagon (neighbors)\n\nAnother important property of tiles is uniform adjacency, i.e., how unambiguous the neighbors are. For example, squares have 4 unambiguous neighbors but also have 4 ambiguous neighbors at the corners, which may not provide the best perception of neighbors if you consider a circular radius. \n\nEquilateral triangles are much worse, with 3 unambiguous neighbors and 9 ambiguous neighbors, which is one of the reasons why triangles are not commonly used, along with the rotation of cells necessary for tessellation. Lastly, hexagons are the best, with 6 unambiguous neighbors and a structure very close to finding neighbors by radius.\n\n\nFigure 2: Square vs Hexagon (Optimal Space-Filling)\n\nHexagons are more space-efficient and have optimal space-filling properties. This means that when filling a polygon with uniform cells, hexagons generally result in less over/under filling compared to squares.\n\n\nFigure 3: Square vs Hexagon (Child Containment)\n\nHierarchical relationships between resolutions are another important property. Evidently, squares have hierarchical relationships with perfect child containment and can use algorithms such as quad trees to navigate up and down the hierarchy and space-filling curves to traverse the grid. Hexagons, while not having perfect child containment, can still function effectively with a tolerable margin of error.\n\nWithout taking triangles into account, the summary of the comparison between squares and hexagons:\n\n\nFigure 4: Squares vs Hexagons (Full Comparison)\n\nMore on Hexagons vs Squares at Conceptualization of a Cartogram\n\n\n\n1.2. Why Icosahedron?\n\nLastly, low shape and area distortion is more related to the projection than the shape of the tile. There are many types of projections, but the most commonly used are polyhedra. One such projection is the cylindrical projection, used in Geohash, which works well for squares but has the problem of distortion near the poles, making it hard to get equal surface area cells across the projection.\n\n\nFigure 5: Uniform Shape Polyhedrons\n\nThe smaller the face, the lesser the distortion. An icosahedron, with 20 faces, is the better option among the uniform-face polyhedrons for fitting hexagons and triangles on them. Fitting squares on an icosahedron or even a tetrahedron is not ideal. Squares are mostly suitable for cubes (as seen in S2). Taking the best of both worlds, an icosahedron with hexagons is the way to go.\n\n1.3. H3 Grid System\n\nPutting it all together, we take the polyhedron, the icosahedron, project it on the surface of the Earth, then each face on the icosahedron is split into hexagon cells. More specifically, 4 full hexagon cells are completely contained by the face, 3 cells are half contained, and 3 corners form the pentagon.\n\n\nEach hexagonal cell can be further subdivided into 7 hexagon cells with marginal error for containment. The number of levels decides the resolution.\n\nFigure 6: H3 Projection and Tessellation\n\nThe H3 grid system divides the surface of the Earth into 122 (110 hexagons and 12 icosahedron vertex-centered pentagons) base cells (resolution 0), which are used as the foundation for higher resolution cells. Each base cell has a specific orientation relative to the face of the icosahedron it is on. This orientation determines how cells at higher resolutions are positioned and indexed.\n\n1.4. Why Pentagons?\n\nLooking at the icosahedron, the 5 faces come together at every vertex, and truncating that creates the base cell. Pentagons are unavoidable at the vertices. However, there are only 12 of them at every resolution. But again, for most cases dealing with spaces within a city where the resolution is higher than 9, the pentagons, if far off in the water, they are safe to ignore.\n\n\nFigure 7: Dymaxion layout (12 Vertices in Water)\n\nWhile the layout of the faces on the icosahedron can be done in any fashion, H3 uses the layout developed by Buckminster Fuller called the Dymaxion layout.\n\n\nFigure 8: H3 Projection and Tessellation (Animated)\n\nThe benefit is that all the vertices end up in the water. For most applications, land is more important than water, and since the vertices are in the water, it reduces the need to deal with pentagons.\n\n1.5. Cell ID\nA cell ID is a 64-bit integer that uniquely identifies a hexagonal cell at a particular resolution. The composition of an H3 cell ID is as follows:\n\n\nMode (4 bits): Identifies the H3 mode, which indicates the type of the identifier. For cell IDs, this value defaults set to 1.\nEdge Mode (Reserved, 3 bits): Indicates the edge mode, which is 0 for cell IDs.\nResolution (4 bits): Specifies the resolution of the cell. H3 supports resolutions from 0 (coarsest) to 15 (finest).\nBase Cell (7 bits): Identifies the base cell, which is one of the 122 base cells that form the foundation of the H3 grid.\nCell Index (45 bits): Contains the specific index of the cell within the base cell and resolution.\n\n\nThis structure (Figure 14) allows H3 to efficiently encode the hierarchical location and resolution of each hexagonal cell in a compact 64-bit integer.\n\n\n\n\n2. H3 - Implementation\n\nThe implementation below, loosely follows the steps of the actual H3 index calculation for demonstration purposes (to better understand the H3 Index). Here's a step-by-step process with reasonable simplifications:\n\n2.1. LatLong to Vec3D\nConvert latitude and longitude to 3D Cartesian coordinates using the formulas (similar to Section 4.2.1 in S2):.\n\n \nFigure 9: (lat, long) to (x, y, z) Transformation\n\n2.1a. LatLong to Vec3D - Snippet\nprivate static double[] latLonToVec3D(double lat, double lon) {\n    double r = Math.cos(Math.toRadians(lat));\n    double x = r * Math.cos(Math.toRadians(lon));\n    double y = r * Math.sin(Math.toRadians(lon));\n    double z = Math.sin(Math.toRadians(lat));\n    return new double[]{x, y, z};\n}\n\n\n\n\n\n2.2. Icosahedron Properties\nWe can identify the 12 vertices of the icosahedron using the golden ratio (ϕ). It a well known property of a regular icosahedron, where three mutually perpendicular rectangles of aspect ratio (ϕ) are arranged such that they share a common center.\n\nThe icosahedron has 12 vertices, 20 faces, and 30 edges. The 12 vertices are given by: (±1, ±ϕ, 0), (±ϕ, 0, ±1), (0, ±1, ±ϕ). Lastly, the vertices need to be normalized to lie on the surface of a unit sphere.\n\n \nFigure 10: Golden Ratio Rectangles\n\nTo calculate the 20 face centers of the icosahedron:\nFor each face, average the coordinates of its three vertices and normalize the resulting vector to lie on the unit sphere. Use the formula:\n\n \nFigure 11: Icosahedron Face Center\n\n2.2a. Icosahedron Vertices - Snippet\ndouble PHI = (1.0 + Math.sqrt(5.0)) / 2.0;\ndouble[][] vertices = {\n        {-1, PHI, 0}, {1, PHI, 0}, {-1, -PHI, 0}, {1, -PHI, 0},\n        {0, -1, PHI}, {0, 1, PHI}, {0, -1, -PHI}, {0, 1, -PHI},\n        {PHI, 0, -1}, {PHI, 0, 1}, {-PHI, 0, -1}, {-PHI, 0, 1}\n};\n\n// Normalize the vertices to lie on the unit sphere\nfor (int i = 0; i &lt; vertices.length; i++) {\n    vertices[i] = normalize(vertices[i]);\n}\n\n\n// Computes the center of a face defined by three vertices.\nprivate static double[] computeFaceCenter(double[] a, double[] b, double[] c) {\n    double[] center = new double[3];\n    center[0] = (a[0] + b[0] + c[0]) / 3.0;\n    center[1] = (a[1] + b[1] + c[1]) / 3.0;\n    center[2] = (a[2] + b[2] + c[2]) / 3.0;\n    return normalize(center);\n}\n\n\n// Normalizes a vector to lie on the unit sphere.\nprivate static double[] normalize(double[] v) {\n    double length = Math.sqrt(v[0] * v[0] + v[1] * v[1] + v[2] * v[2]);\n    return new double[]{v[0] / length, v[1] / length, v[2] / length};\n}\n\n\n\n\n\n2.3. Vec3D to Vec2D\nThe Vec2D represents the cartesian coordinates on the face of the icosahedron. It provides a 2D projection (Figure 7) of a point on the spherical surface of the Earth onto one of the icosahedron's faces, used to map geographic coordinates (latitude and longitude) onto a planar hexagonal grid. The conversion involves gnomonic projection, which translates 3D coordinates to a 2D plane by projecting from the center of the sphere to the plane tangent to the face of the icosahedron.\n\n\nCalculate r (Radial Distance): Convert the distance from the face center to an angle using the inverse cosine function.\nGnomonic Scaling: Scale the angle r for the hexagonal grid at the given resolution.\nCalculate θ (Azimuthal Angle): Determine the angle from the face center, adjusting for face orientation and resolution.\nConvert to local 2D Coordinates: Transform polar coordinates (r, θ) into Cartesian coordinates (x, y).   \n\n\n \nFigure 12: Gnomonic Projection (XYZ to rθ)\n\n2.3a. Vec3D to Vec2D - Snippet\n// faceAxesAzRadsCII: Icosahedron face `ijk` axes as azimuth in radians from face center to vertex\n// faceCenterGeo: Icosahedron face centers in lat/lng radians.\n// RES0_U_GNOMONIC: Scaling factor from `Vec2d` resolution 0 unit length (or distance between adjacent cell center points on the plane) to gnomonic unit length.\n// SQRT7_POWERS: Power of √7 for each resolution.\n// AP7_ROT_RADS: Rotation angle between Class II and Class III resolution axes: asin(sqrt(3/28))\n\npublic Vec2d toVec2d(int resolution, int face, double distance) {\n    // cos(r) = 1 - 2 * sin^2(r/2) = 1 - 2 * (sqd / 4) = 1 - sqd/2\n    double r = acos(1.0 - distance / 2.0);\n    if (r &lt; EPSILON) {\n        return new Vec2d(0.0, 0.0);\n    }\n    \n    // Perform gnomonic scaling of `r` (`tan(r)`) and scale for current\n    r = (tan(r) / RES0_U_GNOMONIC) * SQRT7_POWERS[resolution];\n    \n    // Compute counter-clockwise `theta` from Class II i-axis.\n    double theta = faceAxesAzRadsCII[face][0] - this.azimuth(faceCenterGeo[face]);\n    \n    // Adjust `theta` for Class III.\n    if ((resolution % 2) != 0) {\n        theta -= AP7_ROT_RADS;\n    }\n    \n    // Convert to local x, y.\n    return new Vec2d(r * cos(theta), r * sin(theta));\n}\n\n\n\nAbout SQRT7_POWERS. Each resolution beyond 0 is created using an aperture 7 resolution spacing, i.e. number of cells in the next finer resolution (Figure 1 and 3). So, as resolution increases, unit length is scaled by sqrt(7). H3 has 15 resolutions/levels (+resolution 0).\n\n\n\n2.4. Vec2D to FaceIJK\nHexagonal grids have three primary axes, unlike the two we have for square grids. In Axial coordinates or the Cube coordinates, the three coordinates (i, j, k) ensure that any point in the hexagonal grid can be described without ambiguity.\n\n \nFigure 13: Axial Coordinates (Class II and Class III)\n\nThere are several other hex coordinate systems based, in this case, the constraints are i + j + k = 0, with 120° axis separation.\n\nThe faceIJK represents the position/location of a hexagon within a face of the icosahedron using three coordinates (i, j, k).\n\n\nReverse Conversion: Translate Cartesian coordinates into the hexagonal coordinate system by aligning them with the hex grid's axes.\nQuantize and Round: Convert floating-point coordinates to integer grid positions, determining the closest hexagon center.\n\n \n\nCheck Hex Center and Round: Use remainders to accurately determine which hexagon the point falls into by rounding to the nearest hex center.\n// Determine i and j based on r1 and r2\nIF r1 &lt; 0.5 THEN\n    IF r1 &lt; 1 / 3 THEN\n        i = m1\n        j = m2 + (r2 &gt;= (1 + r1) / 2)\n    ELSE\n        i = m1 + ((1 - r1) &lt;= r2 &amp;&amp; r2 &lt; (2 * r1))\n        j = m2 + (r2 &gt;= (1 - r1))\nELSE IF r1 &lt; 2 / 3 THEN\n    j = m2 + (r2 &gt;= (1 - r1))\n    i = m1 + ((2 * r1 - 1) &gt;= r2 || r2 &gt;= (1 - r1))\nELSE\n    i = m1 + 1\n    j = m2 + (r2 &gt;= (r1 / 2))\n\n\nFold Across Axes if Necessary: Correct the coordinates if they fall into negative regions, ensuring the coordinates remain within the valid grid.\nIF value.x &lt; 0 THEN\n    offset = j % 2\n    axis_i = (j + offset) / 2\n    diff = i - axis_i\n    i = i - 2 * diff - offset\n\nIF value.y &lt; 0 THEN\n    i = i - (2 * j + 1) / 2\n    j = -j\n\nNormalize: Purpose: Adjust the coordinates to maintain the properties of the hexagonal grid, ensuring i + j + k = 0.\n\n\n2.4a. Vec2D to FaceIJK - Snippet\npublic static CoordIJK fromVec2d(Vec2d value) {\n    int k = 0;\n\n    double a1 = Math.abs(value.x);\n    double a2 = Math.abs(value.y);\n\n    // Reverse conversion\n    double x2 = a2 / SIN60;\n    double x1 = a1 + x2 / 2.0;\n\n    // Quantize and round\n    int m1 = (int) x1;\n    int m2 = (int) x2;\n\n    double r1 = x1 - m1;\n    double r2 = x2 - m2;\n\n    int i, j;\n    if (r1 &lt; 0.5) {\n        if (r1 &lt; 1.0 / 3.0) {\n            i = m1;\n            j = m2 + (r2 &gt;= (1.0 + r1) / 2.0 ? 1 : 0);\n        } else {\n            i = m1 + ((1.0 - r1) &lt;= r2 &amp;&amp; r2 &lt; (2.0 * r1) ? 1 : 0);\n            j = m2 + (r2 &gt;= (1.0 - r1) ? 1 : 0);\n        }\n    } else if (r1 &lt; 2.0 / 3.0) {\n        j = m2 + (r2 &gt;= (1.0 - r1) ? 1 : 0);\n        i = m1 + ((2.0 * r1 - 1.0) &gt;= r2 || r2 &gt;= (1.0 - r1) ? 1 : 0);\n    } else {\n        i = m1 + 1;\n        j = m2 + (r2 &gt;= (r1 / 2.0) ? 1 : 0);\n    }\n\n    // Fold Across Axes if Necessary\n    if (value.x &lt; 0) {\n        int offset = j % 2;\n        int axis_i = (j + offset) / 2;\n        int diff = i - axis_i;\n        i = i - 2 * diff - offset;\n    }\n\n    if (value.y &lt; 0) {\n        i = i - (2 * j + 1) / 2;\n        j = -j;\n    }\n\n    return new CoordIJK(i, j, k).normalize();\n}\n\n\n\nEach grid resolution is rotated ~19.1° relative to the next coarser resolution. The rotation alternates between counterclockwise (CCW) and clockwise (CW) at each successive resolution, so that each resolution will have one of two possible orientations as shown in Figure 13: Class II or Class III. The base cells, which make up resolution 0, are Class II.\n\n\n\n2.5. FaceIJK to H3 Index\nLastly, the face and face-centered ijk coordinates are converted to H3 Index. \n\n \nFigure 14: H3 Index Structure\n\nIf the resolution is not uptill level 15, rest of the vits are set to 1s, for example: 83001dfffffffff. The binary representation is as below (Figure 15); Index mode = 1 i.e. indexes the regular hexagon type. Resolution = 3; Base Cell = 0; Resolution 1, 2 and 3 are 0, 3 and 5, rest are 1s.\n\n \nFigure 15: H3 Index Structure (Example: 83001dfffffffff)\n\nThis primarily involves coverting to Direction bits, representing the hierarchical path from a base cell to a specific cell at a given resolution. These bits encode the sequence of directional steps taken within the hexagonal grid to reach the target cell from the base cell.\n\n\nHandle Base Cell: If the resolution is 0 (base cell), directly set the base cell in the index.\n// Convert IJK to Direction Bits\nfaceIJK.coord = directions_bits_from_ijk(faceIJK.coord, resolution)\n\n// Set the Base Cell\nbase_cell = get_base_cell(faceIJK)\nbits = set_base_cell(bits, base_cell)\n\nBuild from Finest Resolution Up and Set Base Cell: Convert IJK coordinates to direction bits starting from the finest resolution (r), updating the index progressively. Identify and set the correct base cell for the given IJK coordinates.\n// Handle Pentagon Cells\nIF base_cell.is_pentagon() THEN\n    IF first_axe(bits) == Direction.K THEN\n        // Check for a CW/CCW offset face (default is CCW).\n        IF base_cell.is_cw_offset(faceIJK.face) THEN\n            bits = rotate60(bits, 1, CW)\n        ELSE\n            bits = rotate60(bits, 1, CCW)\n        END IF\n    END IF\n    FOR i = 0 TO rotation_count DO\n        bits = pentagon_rotate60(bits, CCW)\n    END FOR\nELSE\n    bits = rotate60(bits, rotation_count, CCW)\nEND IF\n\nHandle Pentagon Cells: Apply necessary rotations if the base cell is a pentagon to ensure the correct orientation and avoid the missing k-axes subsequence (if the direction bits indicate a move along the k-axis).\n\n\nSince each base cell can be oriented differently (Section 1.3) on the icosahedron's faces, rotations are needed to standardize these orientations. rotation_count refers to the number of 60-degree rotations that need to be applied to the H3 cell index to align it with the canonical orientation of the base cell (also refer).\n\n\n\n\n2.6. Official H3 library\nHere's a Java snippet using the official H3 library provided by Uber:\n2.7a. Official H3 - Snippet\nimport com.uber.h3core.H3Core;\n\npublic class H3Index {\n    public static void main(String[] args) throws Exception {\n        H3Core h3 = H3Core.newInstance();\n        double lat = 37.7749;\n        double lon = -122.4194;\n        int resolution = 9;\n\n        long h3Index = h3.geoToH3(lat, lon, resolution);\n        System.out.println(Long.toHexString(h3Index));\n    }\n}\n\n\n\n\n\n\n\n\n3. H3 - Conclusion\nSo far, in the Spatial Index Series, we have seen the use of space-filling curves and their application in grid systems like Geohash and S2. Finally, we explored Uber's H3, which falls under grid systems and more specifically relies on tessellation. By now, it's likely clear that H3 indexes are not directly queryable on the database by ranges or prefixes, but they have more importance towards the accuracy of filling a polygon, nearby search by radius, high resolution, and many more.\n\n \nFigure 16: H3 grid segmentation (Level 0 and Level 1)\n\nIf you missed the series, it starts with Spatial Index: Space-Filling Curves, followed by Spatial Index: Grid Systems, and finally, the current post, Spatial Index: Tessellation.\n\n\n\n\n4. References\n1. Uber Technologies, Inc., \"H3: A Hexagonal Hierarchical Spatial Index,\" GitHub. [Online]. Available: https://github.com/uber/h3.\n2. Wikipedia, \"Graticule,\" [Online]. Available: https://en.wikipedia.org/wiki/Graticule.\n3. Microsoft, \"QuadKey,\" Microsoft Docs. [Online]. Available: https://learn.microsoft.com/en-us/bingmaps/articles/bing-maps-tile-system.\n4. Wikipedia, \"Geohash,\" [Online]. Available: https://en.wikipedia.org/wiki/Geohash.\n5. Google, \"Google S2 Geometry Library,\" [Online]. Available: https://s2geometry.io/.\n6. Wikipedia, \"Icosahedron,\" [Online]. Available: https://en.wikipedia.org/wiki/Icosahedron.\n7. Wikipedia, \"Dot product,\" [Online]. Available: https://en.wikipedia.org/wiki/Dot_product.\n8. Wikipedia, \"Basis vectors,\" [Online]. Available: https://en.wikipedia.org/wiki/Basis_(linear_algebra).\n9. Wikipedia, \"3D Cartesian coordinates,\" [Online]. Available: https://en.wikipedia.org/wiki/Cartesian_coordinate_system.\n10. A. N. Adimurthy, \"Spatial Index: Tessellation,\" PyBlog. [Online]. Available: https://www.pyblog.xyz/spatial-index-tessellation.\n11. Wikipedia, \"Conceptualization of a Cartogram,\" [Online]. Available: https://en.wikipedia.org/wiki/Cartogram.\n12. Wikipedia, \"Golden ratio,\" [Online]. Available: https://en.wikipedia.org/wiki/Golden_ratio.\n13. Wikipedia, \"Icosahedron vertices,\" [Online]. Available: https://en.wikipedia.org/wiki/Icosahedron#Vertices.\n14. Wikipedia, \"H3: A Hexagonal Hierarchical Spatial Index,\" [Online]. Available: https://en.wikipedia.org/wiki/H3_(spatial_index).\n15. Wikipedia, \"Dymaxion map,\" [Online]. Available: https://en.wikipedia.org/wiki/Dymaxion_map.\n16. K. Sahr, \"Geodesic Discrete Global Grid Systems,\" Southern Oregon University. [Online]. Available: https://webpages.sou.edu/~sahrk/sqspc/pubs/gdggs03.pdf.\n17. D. F. Marble, \"The Fundamental Data Structures for Implementing Digital Tessellation,\" University of Edinburgh. [Online]. Available: https://www.geos.ed.ac.uk/~gisteac/gis_book_abridged/files/ch36.pdf.\n18. J. Castner, \"The Application of Tessellation in Geographic Data Handling,\" Semantic Scholar. [Online]. Available: https://pdfs.semanticscholar.org/feb2/3e69e19875817848ac8694b15f58d2ef52b0.pdf.\n19. \"Hexagonal Tessellation and Its Application in Geographic Information Systems,\" YouTube. [Online]. Available: https://www.youtube.com/watch?v=wDuKeUkNLkQ&amp;list=PL0HGds8aHQsAYm86RzQdZtFFeLpIOjk00.\n20. Hydronium Labs. \"h3o: A safer, faster, and more flexible H3 library written in Rust.\" GitHub Repository. Available: https://github.com/HydroniumLabs/h3o/tree/master."

    },
  
  
  
  
    {

      "title"    : "Spatial Index: Grid Systems",
      "url"      : "/spatial-index-grid-system",
      "index"    : "redapple",
      "content"  : "This post is a continuation of Stomping Grounds: Spatial Indexes, but don’t worry if you missed the first part—you’ll still find plenty of new insights right here.\n\n3. Geohash\n\nGeohash: Invented in 2008 by Gustavo Niemeyer, encodes a geographic location into a short string of letters and digits. It's a hierarchical spatial data structure that subdivides space into buckets of grid shape using a Z-order curve (Section 2.).\n\n3.1. Geohash - Intuition\n\nEarth is round or more accurately, an ellipsoid. Map projection is a set of transformations represent the globe on a plane. In a map projection. Coordinates (latitude and longitude) of locations from the surface of the globe are transformed to coordinates on a plane. And GeoHash Uses Equirectangular projection\n\n \nFigure 21: Equirectangular projection/ Equidistant Cylindrical Projection\n\nThe core of GeoHash is just an clever use of Z-order curves. Split the map-projection (rectangle) into 2 equal rectangles, each identified by unique bit strings.\n\n \nFigure 22: GeoHash Level 1 - Computation\n\nObservation: the divisions along X and Y axes are interleaved between bit strings. For example: an arbitrary bit string 01110 01011 00000, follows:\n\n\n\nBy futher encoding this to Base32 (0123456789bcdefghjkmnpqrstuvwxyz), we map a unique string to a quadrant in a grid and quadrants that share the same prefix are closer to each other; e.g. 000000 and 000001. By now we know that interleaving trace out a Z-order curve.\n\n \nFigure 23: GeoHash Level 1 - Z-Order Curve\n\nHigher levels (higher order z-curves) lead to higher precision. The geohash algorithm can be iteratively repeated for higher precision. That's one cool property of geohash, adding more characters increase precision of the location.\n\n \n \nFigure 24: GeoHash Level 2\n\nDespite the easy implementation and wide usage of geohash, it inherits the disadvantages of Z-order curves (Section 2.5): weakly preserved latitude-longitude proximity; does not always guarantee that locations that are physically close are also close on the Z-curve. \n\nAdding on to it, is the use of equirectangular projection, where the division of the map into equal subspaces leads to unequal/disproportional surface areas, especially near the poles (northern and southern hemisphere). However, there are alternatives such as Geohash-EAS (Equal-Area Spaces).\n\n\n\n3.2. Geohash - Implementation\nTo Convert a geographical location (latitude, longitude) into a concise string of characters and vice versa:\n\nConvert latitude and longitude to a binary strings.\nInterleave the binary strings of latitude and longitude.\nGeohash: Convert the interleaved binary string into a base32 string.\n\n\n\n\n3.2a. Geohash Encoder - Snippet\n\npublic class GeohashEncoder {\n\n    public static String encodeGeohash(double latitude, double longitude, int precision) {\n        // 1. Convert Lat and Long into a binary string based on the range.\n        String latBin = convertToBinary(latitude, -90, 90, precision * 5 / 2);\n        String lonBin = convertToBinary(longitude, -180, 180, precision * 5 / 2);\n\n        // 2. Interweave the binary strings.\n        String interwovenBin = interweave(lonBin, latBin);\n\n        // 3. Converts a binary string to a base32 geohash.\n        String geohash = binaryToBase32(interwovenBin);\n\n        return geohash.substring(0, precision);\n    }\n\n    private static String convertToBinary(double value, double min, double max, int precision) {\n        StringBuilder binaryStr = new StringBuilder();\n        for (int i = 0; i &lt; precision; i++) {\n            double mid = (min + max) / 2;\n            if (value &gt;= mid) {\n                binaryStr.append('1');\n                min = mid;\n            } else {\n                binaryStr.append('0');\n                max = mid;\n            }\n        }\n        return binaryStr.toString();\n    }\n\n    private static String interweave(String str1, String str2) {\n        StringBuilder interwoven = new StringBuilder();\n        for (int i = 0; i &lt; str1.length(); i++) {\n            interwoven.append(str1.charAt(i));\n            interwoven.append(str2.charAt(i));\n        }\n        return interwoven.toString();\n    }\n\n    private static String binaryToBase32(String binaryStr) {\n        String base32Alphabet = \"0123456789bcdefghjkmnpqrstuvwxyz\";\n        StringBuilder base32Str = new StringBuilder();\n        for (int i = 0; i &lt; binaryStr.length(); i += 5) {\n            String chunk = binaryStr.substring(i, Math.min(i + 5, binaryStr.length()));\n            int decimalVal = Integer.parseInt(chunk, 2);\n            base32Str.append(base32Alphabet.charAt(decimalVal));\n        }\n        return base32Str.toString();\n    }\n\n    public static void main(String[] args) {\n        double latitude = 37.7749;\n        double longitude = -122.4194;\n        int precision = 5;\n        String geohash = encodeGeohash(latitude, longitude, precision);\n        System.out.println(\"Geohash: \" + geohash);\n    }\n}\n\n\n\n\n\n\n3.3. Geohash - Conclusion\nSimilar to Section 2.7 (Indexing the Z-values); Geohashes convert latitude and longitude into a single, sortable string, simplifying spatial data management. A B-trees or search tree such as GiST/SP-GiST (Generalized Search Tree) index are commonly used for geohash indexing in databases.\n\nPrefix Search: Nearby locations share common geohash prefixes, enabling efficient filtering of locations by performing prefix searches on the geohash column\n\nNeighbor Searches: Generate geohashes for a target location and its neighbors to quickly retrieve nearby points. Which also extends to Area Searches: Calculate geohash ranges that cover a specific area and perform range queries to find all relevant points within that region.\n\nPopular databases such as ClickHouse, MySQL, PostGIS, BigQuery, RedShift and many others offer built-in geohash function. And many variations have been developed, such as the 64-bit Geohash and Hilbert-Geohash\n\nInteractive Geohash Visualization: /geohash\n\n\n\n\n4. Google S2\n\n\n4.1. S2 - Intuition\n\nGoogle's S2 library was released more than 10 years ago and didn't exactly the get the attention it deserved, much later in 2017, Google announced the release of open-source C++ s2geometry library. With the use of Hilbert Curve (Section 2.2) and cube face (spherical) projection instead of geohash's Z-order curve and equirectangular projection; S2 addresses (to an extent) the large jumps (Section 2.5) problem with Z-order curves and disproportional surface areas associated with equirectangular projection.\n\nThe core of S2 is the hierarchical decomposition of the sphere into \"cells\"; done using a Quad-tree, where a quadrant is recursively subdivided into four equal sub-cells and the use of Hilbet Curve goes hand-in-hand - runs across the centers of the quad-tree’s leaf nodes.\n\n\n\n\n4.2. S2 - Implementation\n\nThe overview of solution is to:\n\n    Enclose sphere in cube\n    Project point(s) p onto the cube\n    Build a quad-tree/hilbert-curve on each cube face (6 faces)\n    Assign ID to the quad-tree cell that contains the projection of point(s) p\n\n\nStarting with the input co-ordinates, latitude (Degrees: -90° to +90°. Radians: -π/2 to π/2) and longitude (-180° to +180°. Radians: 0 to 2π). And WGS84 is a commmonly standard used in geocentric coordinate system.\n\n\n\n4.2.1. (Lat, Long) to (X,Y,Z)\n\nCovert p = (lattitude,longitude) =&gt; (x,y,z) XYZ co-ordinate system (x = [-1.0, 1.0], y = [-1.0, 1.0], z = [-1.0, -1.0]), based on coordinates on the unit sphere (unit radius), which is similar to Earth-centered, Earth-fixed coordinate system.\n\n \nFigure 25: (lat, long) to (x, y, z) Transformation with ECEF\n\nWhere, (x, y, z): X-axis at latitude 0°, longitude 0° (equator and prime meridian intersection), Y-axis at latitude 0°, longitude 90° (equator and 90°E meridian intersection), Z-axis at latitude 90° (North Pole), Altitude (PM on Figure 25) = Height to the reference ellipsoid/Sphere (Zero for a Round Planet approximation)\n\n\n\n4.2.2. (X,Y,Z) to (Face,U,V)\n\nTo map (x,y,z) to (face, u,v), each of the six faces of the cube is projected onto the sphere. The process is similar to UV Mapping: to project 3D model surface into a 2D coordinate space. where u and v denote the axes of the 2D plane. In this case, U,V represent the location of a point on one face of the cube.\n\nThe projection can simply be imagined as a unit sphere circumscribed by a cube. And a ray is emitted from the center of the sphere to obtain the projection of the point on the sphere to the 6 faces of the cube, that is, the sphere is projected into a cube.\n\n \nFigure 26: (lat, long) to (x, y, z) and (x, y, z) to (face, u, v)\n\nThe face denotes which of the 6 (0 to 5) cube faces a point on the sphere is mapped onto. Figure 27, shows the 6 faces of the cube (cube mapping) after the projection. For a unit-sphere, for each face, the point u,v = (0,0) represent the center of the face.\n\n \nFigure 27: Cube Face (Spherical) Projection\n\nThe evident problem here is that, the linear projection leads to same-area cells on the cube having different sizes on the sphere (Length and Area Distortion), with the ratio of highest to lowest area of 5.2 (areas on the cube can be up to 5.2 times longer or shorter than the corresponding distances on the sphere).\n\n4.2.2a. S2 FaceXYZ to UV - Snippet\npublic static class Vector3 {\n    public double x;\n    public double y;\n    public double z;\n\n    public Vector3(double x, double y, double z) {\n        this.x = x;\n        this.y = y;\n        this.z = z;\n    }\n}\n\npublic static int findFace(Vector3 r) {\n    double absX = Math.abs(r.x);\n    double absY = Math.abs(r.y);\n    double absZ = Math.abs(r.z);\n\n    if (absX &gt;= absY &amp;&amp; absX &gt;= absZ) {\n        return r.x &gt; 0 ? 0 : 3;\n    } else if (absY &gt;= absX &amp;&amp; absY &gt;= absZ) {\n        return r.y &gt; 0 ? 1 : 4;\n    } else {\n        return r.z &gt; 0 ? 2 : 5;\n    }\n}\n\npublic static double[] validFaceXYZToUV(int face, Vector3 r) {\n    switch (face) {\n        case 0:\n            return new double[]{r.y / r.x, r.z / r.x};\n        case 1:\n            return new double[]{-r.x / r.y, r.z / r.y};\n        case 2:\n            return new double[]{-r.x / r.z, -r.y / r.z};\n        case 3:\n            return new double[]{r.z / r.x, r.y / r.x};\n        case 4:\n            return new double[]{r.z / r.y, -r.x / r.y};\n        default:\n            return new double[]{-r.y / r.z, -r.x / r.z};\n    }\n}\n\npublic static void main(String[] args) {\n    Vector3 r = new Vector3(1.0, 2.0, 3.0);\n    int face = 0;\n    double[] uv = validFaceXYZToUV(face, r);\n    System.out.println(\"u: \" + uv[0] + \", v: \" + uv[1]);\n}\n\n\n\nThe Cube Face is the largest absolute X,Y,Z component, when component is -ve, back faces are used.\n \nFace and XYZ is mapped to UV by using the other two X, Y, Z components (other than largest component of face) and diving it by the largest component, a value between [-1, 1]. Additionally, some faces of the cube are transposed (-ve) to produce the single continuous hilbert curve on the cube.\n\n\n\n4.2.3. (Face,U,V) to (Face,S,T)\n\nThe ST coordinate system is an extension of UV with an additional non-linear transformation layer to address the (Area Preservation) disproportionate sphere surface-area to cube cell mapping. Without which, cells near the cube face edges would be smaller than those near the cube face centers.\n\n \nFigure 28: (u, v) to (s, t)\n\nS2 uses Quadratic projection for (u,v) =&gt; (s,t). Comparing tan and quadratic projections: The tan projection has the least Area/Distance Distortion. However, quadratic projection, which is an approximation of the tan projection - is much faster and almost as good as tangent.\n\n        \n            \n            Area Ratio\n            Cell → Point (µs)\n            Point → Cell (µs)\n        \n        \n            Linear\n            5.20\n            0.087\n            0.085\n        \n        \n            Tangent\n            1.41\n            0.299\n            0.258\n        \n        \n            Quadratic\n            2.08\n            0.096\n            0.108\n        \n    \n\nCell → Point and Point → Cell represents the transformation from (U, V) to (S, T) coordinates and vice versa.\n\n \nFigure 29: (face, u, v) to (face, s, t); for face = 0\n\nFor the quadratic transformation: Apply a square root transformation; sqrt(1 + 3 * u) and to maintain the uniformity of the grid cells\n\n4.2.3a. S2 UV to ST - Snippet\npublic static double uvToST(double u) {\n    if (u &gt;= 0) {\n        return 0.5 * Math.sqrt(1 + 3 * u);\n    } else {\n        return 1 - 0.5 * Math.sqrt(1 - 3 * u);\n    }\n}\n\npublic static void main(String[] args) {\n    // (u, v) values in the range [-1, 1]\n    double u1 = 0.5;\n    double v1 = -0.5;\n    \n    // Convert (u, v) to (s, t)\n    double s1 = uvToST(u1);\n    double t1 = uvToST(v1);\n\n    System.out.println(\"For (u, v) = (\" + u1 + \", \" + v1 + \"):\");\n    System.out.println(\"s: \" + s1);\n    System.out.println(\"t: \" + t1);\n}\n\n\n\n\n\n4.2.4. (Face,S,T) to (Face,I,J)\n\nThe IJ coordinates are discretized ST coordinates and divides the ST plane into 230 × 230, i.e. the i and j coordinates in S2 range from 0 to 230 - 1. And represent the two dimensions of the leaf-cells (lowest-level cells) on a cube face.\n\nWhy 230? The i and j coordinates are each represented using 30 bits, which is 230 distinct values for both i and j coordinates (every cm² of the earth), this large range allows precise positioning within each face of the cube (high spatial resolution). The total number of unique cells is 6 x (230 × 230)\n\n\nFigure 30: (face, s, t) to (face, i, j); for face = 0\n\n4.2.4a. S2 ST to IJ - Snippet\npublic static int stToIj(double s) {\n  return Math.max(\n    0, Math.min(1073741824 - 1, (int) Math.round(1073741824 * s))\n  );\n}\n\n\n\n\n\n4.2.5. (Face,I,J) to S2 Cell ID\nThe hierarchical sub-division of each cube face into 4 equal quadrants calls for Hilbert Space-Filling Curve (Section 2.2): to enumerate cells along a Hilbert space-filling curve.\n\n\nFigure 31: (face, i, j) to Hilbert Curve Position\n\nHilbert Curve preserves spatial locality, meaning, the values that are close on the cube face/surface, are numerically close in the Hilbert curve position (illustration in Figure 31 - Level 3).\n\nTransformation: The Hilbert curve transforms the IJ coordinate position on the cube face from 2D to 1D and is given by a 60 bit integer (0 to 260).\n\n4.2.5a. S2 IJ to S2 Cell ID - Snippet\npublic class S2CellId {\n    private static final long MAX_LEVEL = 30;\n    private static final long POS_BITS = 2 * MAX_LEVEL + 1;\n    private static final long FACE_BITS = 3;\n    private static final long FACE_MASK = (1L &lt;&lt; FACE_BITS) - 1;\n    private static final long POS_MASK = (1L &lt;&lt; POS_BITS) - 1;\n\n    public static long faceIjToCellId(int face, int i, int j) {\n        // Face Encoding\n        long cellId = ((long) face) &lt;&lt; POS_BITS;\n        // Loop from MAX_LEVEL - 1 down to 0\n        for (int k = MAX_LEVEL - 1; k &gt;= 0; --k) {\n            // Hierarchical Position Encoding\n            int mask = 1 &lt;&lt; k;\n            long bits = (((i &amp; mask) != 0) ? 1 : 0) &lt;&lt; 1 | (((j &amp; mask) != 0) ? 1 : 0);\n            cellId |= bits &lt;&lt; (2 * k);\n        }\n        return cellId;\n    }\n\n    public static void main(String[] args) {\n        int face = 2; \n        int i = 536870912;\n        int j = 536870912;\n\n        long cellId = faceIjToCellId(face, i, j);\n        System.out.println(\"S2 Cell ID: \" + cellId);\n    }\n}\n\n\n\nThe S2 Cell ID is represented by a 64-bit integer, \n\n\nFigure 32: (face, i, j) to S2 Cell ID\nthe left 3 bits are used to represent the cube face [0-5],\nthe next following 60 bits represents the Hilbert Curve position,\nwith [0-30] levels; two bits for every higher order/level, followed by a trailing 1 bit, which is a marker to identify the level of the cell (by position).\nand the last digits are padded with 0s\n\n\nfffpppp...pppppppp1  # Level 30 cell ID\nfffpppp...pppppp100  # Level 29 cell ID\nfffpppp...pppp10000  # Level 28 cell ID\n...\n...\n...\nfffpp10...000000000  # Level 1 cell ID\nfff1000...000000000  # Level 0 cell ID\n\nNotice the position of trailing 1 and padded 0s, correlated to the level.\n\n\nS2 Tokens are a string representation of S2 Cell IDs (uint64), which can be more convenient for storage.\n\n4.2.5b. S2 Cell ID to S2 Token - Snippet\npublic static String cellIdToToken(long cellId) {\n    // The zero token is encoded as 'X' rather than as a zero-length string\n    if (cellId == 0) {\n        return \"X\";\n    }\n\n    // Convert cell ID to a hex string and strip any trailing zeros\n    String hexString = Long.toHexString(cellId).replaceAll(\"0*$\", \"\");\n    return hexString;\n}\n\npublic static void main(String[] args) {\n    long cellId = 3383821801271328768L; // Given example value\n\n    // Convert S2 Cell ID to S2 Token\n    String token = cellIdToToken(cellId);\n\n    System.out.println(\"S2 Cell ID: \" + cellId);\n    System.out.println(\"S2 Token: \" + token);\n}\n\n\nIt's similar to Geohash, however, prefixes from a high-order S2 token does not yield a parent lower-order token, because the trailing 1 bit in S2 cell ID wouldn't be set correctly. Convert S2 Cell ID to an S2 Token by encoding the ID into a base-16 (hexadecimal) string.\n\n\n\n\n4.3. S2 - Conclusion\nGoogle's S2 provides spatial indexing by using hierarchical decomposition of the sphere into cells through a combination of Hilbert curves and cube face (spherical) projection. This approach mitigates some of the spatial locality issues present in Z-order curves and offers more balanced surface area representations. S2's use of (face, u, v) coordinates, quadratic projection, and Hilbert space-filling curves ensures efficient and precise spatial indexing.\n\n\n\nClosing with a strong pro and a con, S2 offers a high resolution of as low as 0.48 cm² cell size (level 30), but the number of cells required to cover a given polygon isn't the best. This makes it a good transition to talk about Uber's H3. The question is, Why Hexagons?\n\n\n\n\n3. References\n\n6. Christian S. Perone, \"Google’s S2, geometry on the sphere, cells and Hilbert curve,\" in Terra Incognita, 14/08/2015, https://blog.christianperone.com/2015/08/googles-s2-geometry-on-the-sphere-cells-and-hilbert-curve/. [Accessed: 12-Jun-2024].\n7. B. Feifke, \"Geospatial Indexing Explained,\" Ben Feifke, Dec. 2022. [Online]. Available: https://benfeifke.com/posts/geospatial-indexing-explained/. [Accessed: 12-Jun-2024].\n8. \"S2 Concepts,\" S2 Geometry Library Documentation, 2024. [Online]. Available: https://docs.s2cell.aliddell.com/en/stable/s2_concepts.html. [Accessed: 13-Jun-2024].\n9. \"Geospatial Indexing: A Look at Google's S2 Library,\" CNIter Blog, Mar. 2023. [Online]. Available: https://cniter.github.io/posts/720275bd.html. [Accessed: 13-Jun-2024].\n10. \"S2 Geometry Library,\" S2 Geometry, 2024. [Online]. Available: https://s2geometry.io/. [Accessed: 13-Jun-2024]."

    },
  
  
  
  
    {

      "title"    : "Spatial Index: Space-Filling Curves",
      "url"      : "/spatial-index-space-filling-curve",
      "index"    : "avocado",
      "content"  : "0. Overview\nSpatial data has grown (/is growing) rapidly thanks to web services tracking where and when users do things. Most applications add location tags and often allow users check in specific places and times. This surge is largely due to smartphones, which act as location sensors, making it easier than ever to capture and analyze this type of data.\n\nThe goal of this post is to dive into the different spatial indexes that are widely used in both relational and non-relational databases. We'll look at the pros and cons of each type, and also discuss which indexes are the most popular today.\n\n \nFigure 0: Types of Spatial Indexes\n\nSpatial indexes fall into two main categories: space-driven and data-driven structures. Data-driven structures, like the R-tree family, are tailored to the distribution of the data itself. Space-driven structures include partitioning trees (kd-trees, quad-trees), space-filling curves (Z-order, Hilbert), and grid systems (H3, S2, Geohash), each partitioning space to optimize spatial queries. This classification isn't exhaustive, as many other methods cater to specific needs in spatial data management.\n\n\n\n\n\n1. Foundation\nTo understand the need for spatial indexes, or more generally, a way to index multi-dimensional data.\n \nFigure 1: Initial Table Structure\nConsider a table with the following fields: device, X, and Y, all of which are integers ranging from 1 to 4. Data is inserted into this table randomly by an external application.\n\n \nFigure 2: Unpartitioned and Unsorted Table\nCurrently, the table is neither partitioned nor sorted. As a result, the data is distributed across all files (8 files), each containing a mix of all ranges. This means all files are similar in nature. Running a query like Device = 1 and X = 2 requires a full scan of all files, which is inefficient.\n\n \nFigure 3: Partitioning by Device\nTo optimize this, we partition the table by the device field into 4 partitions: Device = 1, Device = 2, Device = 3, and Device = 4. Now, the same query (Device = 1 and X = 2) only needs to scan the relevant partition. This reduces the scan to just 2 files.\n\n \nFigure 4: Sorting Data Within Partitions\nFurther optimization can be achieved by sorting the data within each partition by the X field. With this setup, each file in a partition holds a specific range of X values. For example, one file in the Device = 1 partition hold X = 1 to 2. This makes the query Device = 1 and X = 2 even more efficient.\n\n \nFigure 5: Limitation with Sorting on a Single Field\nHowever, if the query changes to Device = 1 and Y = 2, the optimization is lost because the sorting was done on X and not Y. This means the query will still require scanning the entire partition for Device = 1, bringing us back to a less efficient state.\n\nAt this point, there's a clear need for efficiently partitioning 2-dimensional data. Why not use B-tree with a composite index? A composite index prioritizes the first column in the index, leading to inefficient querying for the second column. This leads us back to the same problem, particularly when both dimensions need to be considered simultaneously for efficient querying.\n\n\n\n\n2. Space-Filling Curves\n\nX and Y from 1 to 4 on a 2D axis. The goal is to traverse the data and number them accordingly (the path). using Space-Filling Curves AKA squiggly lines.\n\n \nFigure 6: Exploring Space-Filling Curve and Traversing the X-Y Axis\n\nStarting from Y = 1 and X = 1, as we traverse up to X = 1 and Y = 4, it's evident that there is no locality preservation (Lexicographical Order). The distance between points (1, 4) and (1, 3) is 6, a significant difference for points that are quite close to each other. Grouping this data into files keeps unrelated data together and ended up sorting by one column while ignoring the information in the other column (back to square one). i.e. X = 2 leads to a full scan.\n\n\n2.1. Z-Order Curve - Intuition\nA recursive Z pattern, also known as the Z-order curve, is an effective way to preserve locality in many cases.\n\n \nFigure 7: Z-Order Curve Types\nThe Z-order curve can take many shapes, depending on which coordinate goes first. The typical Z-shape occurs when the Y-coordinate goes first (most significant bit), and the upper left corner is the base. A mirror image Z-shape occurs when the Y-coordinate goes first and the lower left corner is the base. An N-shape occurs when the X-coordinate goes first and the lower left corner is the base.\n\nZ-order curve grows exponentially, and the next size is the second-order curve that has 2-bit sized dimensions. Duplicate the first-order curve four times and connect them together to form a continuous curve.\n\n \nFigure 8: Z-Order Curve\n\nPoints (1, 4) and (1, 3) are separated by a single square. With 4 files based on this curve, the data is not spread out along a single dimension. Instead, the 4 files are clustered across both dimensions, making the data selective on both X and Y dimensions.\n\n\n\n\n2.2. Hilbert Curve - Intuition\n\nThe Hilbert curve is another type of space-filling curve that serve a similar purpose, rather than using a Z-shaped pattern like the Z-order curve, it uses a gentler U-shaped pattern. When compared with the Z-order curve in Figure 9, it’s quite clear that the Hilbert curve always maintains the same distance between adjacent data points.\n\n\nFigure 9: First Order and Second Order Hilbert Curve\nHilbert curve also grows exponentially, to do so, duplicate the first-order curve and connect them. Additionally, some of the first-order curves are rotated to ensure that the interconnections are not larger than 1 point.\n\n \nComparing with the Z-curves (from Figure 8, higher-order in Figure 18), the Z-order curve is longer than the Hilbert curve at all levels, for the same area.\n\n \nFigure 10: Hilbert Curve Types\nAlthough there are quite a lot of varaints of Hilbert curve, the common pattern is to rotate by 90 degrees and repeat the pattern in next higher order(s).\n\n \nFigure 11: Hilbert Curve\nHilbert curves traverse through the data, ensuring that multi-dimensional data points that are close together in 2D space remain close together along the 1D line or curve, thus preserving locality and enhancing query efficiency across both dimensions.\n\n\n\n\n2.3. Z-Order Curve and Hilbert Curve - Comparison\n\nTaking an example, if we query for X = 3, we only need to search 2 of the files. Similarly, for Y = 3, the search is also limited to 2 files in both Z-order and Hilbert Curves\n\n \nFigure 12: Z-Order Curve - Example\n\nUnlike a hierarchical sort on only one dimension, the data is selective across both dimensions, making the multi-dimensional search more efficient.\n\n \nFigure 13: Hilbert Curve - Example\n\nAlthough both the curves give a similar advantage, the main shortcoming with Z-order curve: it fails to maintain perfect data locality across all the data points in the curve. In Figure 12, notice the data points between index 8 and 9 are further apart. As the size of the Z-curve increases, so does the distance between such points that connect different parts of curve together.\n\nHilbert curve is more preferred over the Z-order curve for ensuring better data locality and Z-order curve is still widely used because of it's simplicity.\n\n\n\n\n2.4. Optimizing with Z-Values\n\nIn the examples so far, we have presumed that the X and Y values are dense, meaning that there is a value for every combination of X and Y. However, in real-world scenarios, data can be sparse, with many X, Y combinations missing\n\n \nFigure 14: Flexibility in Number of Files\nThe number of files (4 in the prior examples) isn't necessarily dictated. Here's what 3 files would look like using both Z-order and Hilbert curves. The benefits still holds to an extent because of the space-filling curve, which efficiently clusters related data points.\n\n \nFigure 15: Optimizing with Z-Values\nTo improve efficiency, we can use Z-values. If files are organized by Z-values, each file has a min-max Z-value range. Filters on X and Y can be transformed into Z-values, enabling efficient querying by limiting the search to relevant files based on their Z-value ranges.\n\n \nFigure 16: Efficient Querying with Min-Max Z-Values\nConsider a scenario where the min-max Z-values of 3 files are 1 to 5, 6 to 9, and 13 to 16. Querying by 2 ≤ X ≤ 3 and 3 ≤ Y ≤ 4 would initially require scanning 2 files. However, if we convert these ranges to their Z-value equivalent, which is 10 ≤ Z ≤ 15, we only need to scan one file, since the min-max Z-values are known.\n\n\n\n\n2.5. Z-Order Curve - Implementation\n\nSo far, wkt, Z-ordering arranges the 2D pairs on a 1-dimensional line. More importantly, values that were close together in the 2D plane would still be close to each other on the Z-order line. The implementation goal is to derive Z-Values that preserves spatial locality from M-dimensional data-points (Z-ordering is not limited to 2-dimensional space and it can be abstracted to work in any number of dimensions)\n\nZ-order bit-interleaving is a technique that interleave bits of two or more values to create a 1-D value while spatial locality is preserved:\n\n \nFigure 17: Bit Interleaving\nExample: 4-bit values X = 10, Y = 12 on a 2D grid, X = 1010, Y = 1100, then interleaved value Z = 1110 0100 (228)\n\n2.5a. Z-Order Curve - Snippet\n\npublic class ZOrderCurve {\n\n    // Function to interleave bits of two integers x and y\n    public static long interleaveBits(int x, int y) {\n        long z = 0;\n        for (int i = 0; i &lt; 32; i++) {\n            z |= (long)((x &amp; (1 &lt;&lt; i)) &lt;&lt; i) | ((y &amp; (1 &lt;&lt; i)) &lt;&lt; (i + 1));\n        }\n        return z;\n    }\n\n    // Function to compute the Z-order curve values for a list of points\n    public static long[] zOrderCurve(int[][] points) {\n        long[] zValues = new long[points.length];\n        for (int i = 0; i &lt; points.length; i++) {\n            int x = points[i][0];\n            int y = points[i][1];\n            zValues[i] = interleaveBits(x, y);\n        }\n        return zValues;\n    }\n\n    public static void main(String[] args) {\n        int[][] points = { {1, 2}, {3, 4}, {5, 6} };\n        long[] zValues = zOrderCurve(points);\n\n        System.out.println(\"Z-order values:\");\n        for (long z : zValues) {\n            System.out.println(z);\n        }\n    }\n}\n\n\n\n\n\n \nFigure 18: 2-D Z-Order Curve Space\n\nFrom the above Z-order keys, we see that points that are close to each other in the original space have close Z-order keys. For instance, points sharing the prefix 000 in their Z-order keys are close in 2D space, while points with the prefix 110 indicate greater distance.\n\n \nFigure 19: 2-D Z-Order Curve Space and a Query Region\nNow that we know how to calculate the z-order keys, we can use the z-order keys to define a range of values to read (reange-query), to do so, we have to find the lower and upper counds. For example: The query rectangle: 2 ≤ X ≤ 3 to 4 ≤ Y ≤ 5, the lower bound is Z-Order(X = 2, Y = 4) = 100100 and upper bound is (X = 3, Y = 5) = 100111, translates to Z-order values of 36 and 39.\n\n \nFigure 20: 2-D Z-Order Curve Space and a Query Region (The Problem)\nHowever, range queries based on Z-Order keys are not always present in a continuous Z path. For example: The query rectangle 1 ≤ X ≤ 3 to 3 ≤ Y ≤ 4, the lower bound Z-Order(X = 1, Y = 3) = 001011 and upper bound is (X = 3, Y = 4) = 100101, translates to Z-order values of 11 and 37 - optimized using subranges.\n\nThe Z-order curve weakly preserves latitude-longitude proximity, i.e. two locations that are close in physical distance are not guaranteed to be close following the Z-curve\n\n\n\n\n2.6. Hilbert Curve - Implementation\nFrom Section 2.2, wkt: The Hilbert curve implementation converts 2D coordinates to a single scalar value that preserves spatial locality by recursively rotating and transforming the coordinate space.\n\nIn the code snippet: The xyToHilbert function computes this scalar value using bitwise operations, while the hilbertToXy function reverses this process. This method ensures that points close in 2D space remain close in the 1D Hilbert curve index, making it useful for spatial indexing.\n\n2.6a. Hilbert Curve - Snippet\npublic class HilbertCurve {\n    // Rotate/flip a quadrant appropriately\n    private static void rot(int n, int[] x, int[] y, int rx, int ry) {\n        if (ry == 0) {\n            if (rx == 1) {\n                x[0] = n - 1 - x[0];\n                y[0] = n - 1 - y[0];\n            }\n            // Swap x and y\n            int temp = x[0];\n            x[0] = y[0];\n            y[0] = temp;\n        }\n    }\n\n    // Convert (x, y) to Hilbert curve distance\n    public static int xyToHilbert(int n, int x, int y) {\n        int d = 0;\n        int[] ix = { x };\n        int[] iy = { y };\n\n        for (int s = n / 2; s &gt; 0; s /= 2) {\n            int rx = (ix[0] &amp; s) &gt; 0 ? 1 : 0;\n            int ry = (iy[0] &amp; s) &gt; 0 ? 1 : 0;\n            d += s * s * ((3 * rx) ^ ry);\n            rot(s, ix, iy, rx, ry);\n        }\n\n        return d;\n    }\n\n    // Convert Hilbert curve distance to (x, y)\n    public static void hilbertToXy(int n, int d, int[] x, int[] y) {\n        int rx, ry, t = d;\n        x[0] = y[0] = 0;\n        for (int s = 1; s &lt; n; s *= 2) {\n            rx = (t / 2) % 2;\n            ry = (t ^ rx) % 2;\n            rot(s, x, y, rx, ry);\n            x[0] += s * rx;\n            y[0] += s * ry;\n            t /= 4;\n        }\n    }\n\n    public static void main(String[] args) {\n        int n = 16; // size of the grid (must be a power of 2)\n        int x = 5;\n        int y = 10;\n        int d = xyToHilbert(n, x, y);\n        System.out.println(\"The Hilbert curve distance for (\" + x + \", \" + y + \") is: \" + d);\n\n        int[] point = new int[2];\n        hilbertToXy(n, d, point, point);\n        System.out.println(\"The coordinates for Hilbert curve distance \" + d + \" are: (\" + point[0] + \", \" + point[1] + \")\");\n    }\n}\n\n\n\n\n\n\n2.7. Z-Order Curve and Hilbert Curve - Conclusion\n\nUsage: Insert data points and their Z-order keys/Hilbert Keys (let's call it Z and H keys) into a one-dimensional hierarchical index structure, such as a B-Tree or Quad-Tree. For range or nearest neighbor queries, convert the search criteria into Z/H keys or range of keys. After retrieval, further filter the results as necessary to remove any garbage values.\n\nTo conclude: Space-Filling Curves such as Z-Order/Hilbert indexing is a powerful technique to query higher-dimensional data, especially as the data volumes grows. By combining bits from multiple dimensions into a single value, space-Filling Curves indexing preserves spatial locality, enabling efficient data indexing and retrieval.\n\nHowever, as seen in Section 2.5, large jumps along the Z-Order curve can affect certain types of queries (better with Hilbert curves Section 2.2). The success of Z-Order indexing relies on the data's distribution and cardinality. Therefore, it is essential to evaluate the nature of the data, query patterns, performance needs and limitation(s) of indexing strategies.\n\n\n\n\n\n\n3. References\n\n1. \"Programming the Hilbert curve\" (American Institue of Physics (AIP) Conf. Proc. 707, 381 (2004)).\n2. Wikipedia. “Z-order curve,” [Online]. Available: https://en.wikipedia.org/wiki/Z-order_curve.\n3. Amazon Web Services, “Z-order indexing for multifaceted queries in Amazon DynamoDB – Part 1,” [Online]. Available: https://aws.amazon.com/blogs/database/z-order-indexing-for-multifaceted-queries-in-amazon-dynamodb-part-1/. [Accessed: 10-Jun-2024].\n4. N. Chandra, “Z-order indexing for efficient queries in Data Lake,” Medium, 20-Sep-2021. [Online]. Available: https://medium.com/@nishant.chandra/. [Accessed: 10-Jun-2024]z-order-indexing-for-efficient-queries-in-data-lake-48eceaeb2320. [Accessed: 10-Jun-2024].\n5. YouTube, “Z-order indexing for efficient queries in Data Lake,” [Online]. Available: https://www.youtube.com/watch?v=YLVkITvF6KU. [Accessed: 10-Jun-2024]."

    },
  
  
  
  
    {

      "title"    : "Real-time insights: Telemetry Pipeline",
      "url"      : "/telemetry-pipeline",
      "index"    : "strawberry",
      "content"  : "0. Overview\n\n0.1. Architecture\nA telemetry pipeline is a system that collects, ingests, processes, stores, and analyzes telemetry data (metrics, logs, traces) from various sources in real-time or near real-time to provide insights into the performance and health of applications and infrastructure.\n\n \nFigure 0: Barebone Telemetry Pipeline Architecture\n\nIt typically involves tools like Telegraf for data collection, Kafka for ingestion, Flink for processing, and Cassandra and VictoriaMetrics for storage and analysis.\n\n \nFigure 1: Detailed Telemetry Pipeline Architecture\n\n\n\n\n0.2. Stages\n\nCollection: Telemetry data is collected from various sources using agents like Telegraf and Fluentd.\nIngestion: Data is ingested through message brokers such as Apache Kafka or Kinesis to handle high throughput.\nProcessing: Real-time processing is done using stream processing frameworks like Apache Flink for filtering, aggregating, and enriching data.\nStorage and Analysis: Processed data is stored in systems like Cassandra, ClickHouse and Elasticsearch, and analyzed using tools like Grafana and Kibana for visualization and alerting.\n\n\n\n\n\n\n\n1. Collection\n\n1.1. Collection Agent\n\nTo start, we'll use Telegraf, a versatile open-source agent that collects metrics from various sources and writes them to different outputs. Telegraf supports a wide range of input and output plugins, making it easy to gather data from sensors, servers, GPS systems, and more.\n\n \nFigure 2: Telegraf for collecting metrics &amp; data\n\nFor this example, we'll focus on collecting the CPU temperature and Fan speed from a macOS system using the exec plugin in Telegraf. And leverage the osx-cpu-temp command line tool to fetch the CPU temperature.\n\n🌵 Inlets allows devices behind firewalls or NAT to securely expose local services to the public internet by tunneling traffic through a public-facing Inlets server\n\n\n\n\n1.2. Dependencies\n\nUsing Homebrew: brew install telegraf\nFor other OS, refer: docs.influxdata.com/telegraf/v1/install. \nOptionally, download the latest telegraf release from: https://www.influxdata.com/downloads\n\nUsing Homebrew: brew install osx-cpu-temp\nRefer: github.com/lavoiesl/osx-cpu-temp\n\n\n\n\n1.3. Events\n\nHere's a custom script to get the CPU and Fan Speed:\n#!/bin/bash\ntimestamp=$(date +%s)000000000\nhostname=$(hostname | tr \"[:upper:]\" \"[:lower:]\")\ncpu=$(osx-cpu-temp -c | sed -e 's/\\([0-9.]*\\).*/\\1/')\nfans=$(osx-cpu-temp -f | grep '^Fan' | sed -e 's/^Fan \\([0-9]\\) - \\([a-zA-Z]*\\) side *at \\([0-9]*\\) RPM (\\([0-9]*\\)%).*/\\1,\\2,\\3,\\4/')\necho \"cpu_temp,device_id=$hostname temp=$cpu $timestamp\"\nfor f in $fans; do\n  side=$(echo \"$f\" | cut -d, -f2 | tr \"[:upper:]\" \"[:lower:]\")\n  rpm=$(echo \"$f\" | cut -d, -f3)\n  pct=$(echo \"$f\" | cut -d, -f4)\n  echo \"fan_speed,device_id=$hostname,side=$side rpm=$rpm,percent=$pct $timestamp\"\ndone\n\n\n\n\nOutput Format: measurement,host=foo,tag=measure val1=5,val2=3234.34 1609459200000000000\n\n\nThe output is of Line protocol syntax\nWhere measurement is the “table” (“measurement\" in InfluxDB terms) to which the metrics are written.\nhost=foo,tag=measure are tags to can group and filter by.\nval1=5,val2=3234.34 are values, to display in graphs.\n1716425990000000000 is the current unix timestamp + 9 x \"0\" — representing nanosecond timestamp.\n\n\nSample Output: cpu_temp,device_id=adeshs-mbp temp=0.0 1716425990000000000\n\n\n\n\n1.4. Configuration\nThe location of telegraf.conf installed using homebrew: /opt/homebrew/etc/telegraf.conf\n\nTelegraf's configuration file is written using TOML and is composed of three sections: global tags, agent settings, and plugins (inputs, outputs, processors, and aggregators).\n\nOnce Telegraf collects the data, we need to transmit it to a designated endpoint for further processing. For this, we'll use the HTTP output plugin in Telegraf to send the data in JSON format to a Flask application (covered in the next section).\n\nBelow is what the telegraf.conf file looks like, with exec input plugin (format: influx) and HTTP output plugin (format: JSON).\n\n[agent]\n  interval = \"10s\"\n  round_interval = true\n  metric_buffer_limit = 10000\n  flush_buffer_when_full = true\n  collection_jitter = \"0s\"\n  flush_interval = \"10s\"\n  flush_jitter = \"0s\"\n  precision = \"\"\n  debug = false\n  quiet = false\n  logfile = \"/path to telegraf log/telegraf.log\"\n  hostname = \"host\"\n  omit_hostname = false\n\n[[inputs.exec]]\n  commands = [\"/path to custom script/osx_metrics.sh\"]\n  timeout = \"5s\"\n  name_suffix = \"_custom\"\n  data_format = \"influx\"\n  interval = \"10s\"\n\n[[outputs.http]]\n  url = \"http://127.0.0.1:5000/metrics\"\n  method = \"POST\"\n  timeout = \"5s\"\n  data_format = \"json\"\n  [outputs.http.headers]\n    Content-Type = \"application/json\"\n\n\nEdit telegraf.conf (use above config): vi /opt/homebrew/etc/telegraf.conf\n\n🚧: Don't forget to expore tons of other input and output plugins: docs.influxdata.com/telegraf/v1/plugins\n\n\n\n\n1.5. Start Capture\nRun telegraf (when installed from Homebrew): /opt/homebrew/opt/telegraf/bin/telegraf -config /opt/homebrew/etc/telegraf.conf\n\n\n\n\n\n\n2. Ingestion\n\n2.1. Telemetry Server\n\nThe telemetry server layer is designed to be lightweight. Its primary function is to authenticate incoming requests and publish raw events directly to Message Broker/Kafka. Further processing of these events will be carried out by the stream processing framework.\n\nFor our example, the Flask application serves as the telemetry server, acting as the entry point (via load-balancer) for the requests. It receives the data from a POST request, validates it, and publishes the messages to a Kafka topic.\n\nTopic partition is the unit of parallelism in Kafka. Choose a partition key (ex: client_id) that evenly distributes records to avoid hotspots and number of partitions to achieve good throughput.\n\n🚧 Message Broker Alternatives: Amazon Kinesis, Redpanda\n\n\n\n\n2.2. Dependencies\n\nUsing PIP: pip3 install Flask flask-cors kafka-python\nFor Local Kafka Set-up (Or use Docker from next sub-section):\nUsing Homebrew: brew install kafka Refer: Homebrew Kafka\nStart Zookeeper: zookeeper-server-start /opt/homebrew/etc/kafka/zookeeper.properties\nStart Kafka: brew services restart kafka\nCreate Topic: kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic learn Usage: Kafka CLI\n\n\n\n\n\n\n2.3. Docker Compose\n\nTo set up Kafka using Docker Compose, ensure Docker is installed on your machine by following the instructions on the Docker installation page. Once Docker is installed, create a docker-compose.yml for Kafka and Zookeeper:\n\nversion: '3.7'\n\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.3.5\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n    ports:\n      - \"2181:2181\"\n\n  kafka:\n    image: confluentinc/cp-kafka:7.3.5\n    ports:\n      - \"9092:9092\"  # Internal port\n      - \"9094:9094\"  # External port\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,OUTSIDE:PLAINTEXT\n      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,OUTSIDE://localhost:9094\n      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,OUTSIDE://0.0.0.0:9094\n      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      CONFLUENT_SUPPORT_METRICS_ENABLE: \"false\"\n    depends_on:\n      - zookeeper\n\n  kafka-topics-creator:\n    image: confluentinc/cp-kafka:7.3.5\n    depends_on:\n      - kafka\n    entrypoint: [\"/bin/sh\", \"-c\"]\n    command: |\n      \"\n      # blocks until kafka is reachable\n      kafka-topics --bootstrap-server kafka:9092 --list\n\n      echo -e 'Creating kafka topics'\n      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic raw-events --replication-factor 1 --partitions 1\n\n      echo -e 'Successfully created the following topics:'\n      kafka-topics --bootstrap-server kafka:9092 --list\n      \"\n\n  schema-registry:\n    image: confluentinc/cp-schema-registry:7.3.5\n    environment:\n      - SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL=zookeeper:2181\n      - SCHEMA_REGISTRY_HOST_NAME=schema-registry\n      - SCHEMA_REGISTRY_LISTENERS=http://schema-registry:8085,http://localhost:8085\n    ports:\n      - 8085:8085\n    depends_on: [zookeeper, kafka]\n\nRun docker-compose up to start the services (Kafka + Zookeeper).\n\n\n\n\n2.4. Start Server\n\nThe Flask application includes a /metrics endpoint, as configured in telegraf.conf output to collect metrics. When data is sent to this endpoint, the Flask app receives and publishes the message to Kafka.\n\nNew to Flask? Refer: Flask Quickstart\n\nimport os\nfrom flask_cors import CORS\nfrom flask import Flask, jsonify, request\nfrom dotenv import load_dotenv\nfrom kafka import KafkaProducer\nimport json\n\n\napp = Flask(__name__)\ncors = CORS(app)\nload_dotenv()\n\nproducer = KafkaProducer(bootstrap_servers='localhost:9094', \n                         value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n\n@app.route('/metrics', methods=['POST'])\ndef process_metrics():\n    data = request.get_json()\n    print(data)\n    producer.send('raw-events', data)\n    return jsonify({'status': 'success'}), 200\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True, host=\"0.0.0.0\", port=int(os.environ.get(\"PORT\", 8080)))\n\n\nStart all services 🚀:\n\nRun Flask App (Telemetry Server): flask run\nEnsure telegraf is running (Refer: Section 1.5)\n\n\n\n\n\n\n\n3. Processing\n\n3.1. Stream Processor\nThe Stream Processor is responsible for data transformation, enrichment, stateful computations/updates over unbounded (push-model) and bounded (pull-model) data streams and sink enriched and transformed data to various data stores or applications. Key Features to Look for in a Stream Processing Framework:\n\nScalability and Performance: Scale by adding nodes, efficiently use resources, process data with minimal delay, and handle large volumes\nFault Tolerance and Data Consistency: Ensure fault tolerance with state saving for failure recovery and exactly-once processing.\nEase of Use and Community Support: Provide user-friendly APIs in multiple languages, comprehensive documentation, and active community support.\n\n\nFigure 3: Stateful Stream Processing\n\nIntegration and Compatibility: Seamlessly integrate with various data sources and sinks, and be compatible with other tools in your tech stack.\nWindowing and Event Time Processing: Support various windowing strategies (tumbling, sliding, session) and manage late-arriving data based on event timestamps.\nSecurity and Monitoring: Include security features like data encryption and robust access controls, and provide tools for monitoring performance and logging.\n\nAlthough I have set the context to use Flink for this example;\n☢️ Note: While Apache Flink is a powerful choice for stream processing due to its rich feature set, scalability, and advanced capabilities, it can be overkill for a lot of use cases, particularly those with simpler requirements and/or lower data volumes.\n\n🚧 Open Source Alternatives: Apache Kafka Streams, Apache Storm, Apache Samza\n\n\n\n\n3.2. Dependencies\n\nInstall PyFlink Using PIP: pip3 install apache-flink==1.18.1Usage examples: flink-python/pyflink/examples\n\nFor Local Flink Set-up: (Or use Docker from next sub-section)\nDownload Flink and extract the archive: www.apache.org/dyn/closer.lua/flink/flink-1.18.1/flink-1.18.1-bin-scala_2.12.tgz☢️ At the time of writing this post Flink 1.18.1 is the latest stable version that supports kafka connector plugin.\nDownload Kafka Connector and extract the archive: www.apache.org/dyn/closer.lua/flink/flink-connector-kafka-3.1.0/flink-connector-kafka-3.1.0-src.tgzCopy/Move the flink-connector-kafka-3.1.0-1.18.jar to flink-1.18.1/lib ($FLINK_HOME/lib)\nEnsure Flink Path is set export FLINK_HOME=/full-path/flink-1.18.1 (add to .bashrc/.zshrc)\nStart Flink Cluster: cd flink-1.18.1 &amp;&amp; ./bin/start-cluster.sh\nFlink dashboard at: localhost:8081\nTo Stop Flink Cluster: ./bin/stop-cluster.sh\n\n\n\n\n\n3.3. Docker Compose\n\nCreate flink_init/Dockerfile file for Flink and Kafka Connector:\nFROM flink:1.18.1-scala_2.12\n\nRUN wget -P /opt/flink/lib https://repo.maven.apache.org/maven2/org/apache/flink/flink-connector-kafka/3.1.0-1.18/flink-connector-kafka-3.1.0-1.18.jar\n\nRUN chown -R flink:flink /opt/flink/lib\n\n\n\nAdd Flink to docker-compose.yml (in-addition to Kafka, from Section 2.3)\nversion: '3.8'\nservices:\n  jobmanager:\n    build: flink_init/.\n    ports:\n      - \"8081:8081\"\n    command: jobmanager\n    environment:\n      - |\n        FLINK_PROPERTIES=\n        jobmanager.rpc.address: jobmanager\n\n  taskmanager:\n    build: flink_init/.\n    depends_on:\n      - jobmanager\n    command: taskmanager\n    environment:\n      - |\n        FLINK_PROPERTIES=\n        jobmanager.rpc.address: jobmanager\n        taskmanager.numberOfTaskSlots: 2\n\n\nRun docker-compose up to start the services (Kafka + Zookeeper, Flink).\n\n\n\n\n\n3.4. Start Cluster\n⚠️ PyFlink Job:\n\nStart all services 🚀:\n\nEnsure all the services are running (Refer: Section 1.5, 2.4, 3.3)\n\n\n\n\n\n\n\n4. Storage and Analysis \nThe code snippets - stops here! The rest of the post covers key conventions, strategies, and factors for selecting the right data store, performing real-time analytics, and alerts.\n4.1. Datastore \nWhen choosing the right database for telemetry data, it's crucial to consider several factors:\n\nRead and Write Patterns: Understanding the frequency and volume of read and write operations is key. High write and read throughput require different database optimizations and consistencies.\nData Amplification: Be mindful of how the data volume might grow over time (+Write Amplification) and how the database handles this increase without significant performance degradation.\nCost: Evaluate the cost implications, including storage, processing, and any associated services.\nAnalytics Use Cases: Determine whether the primary need is for real-time analytics, historical data analysis, or both.\nTransactions: Consider the nature and complexity of transactions that will be performed. For example: Batch write transactions\nRead and Write Consistency: Decide on the level of consistency required for the application. For example, OLTP (Online Transaction Processing) systems prioritize consistency and transaction integrity, while OLAP (Online Analytical Processing) systems are optimized for complex queries and read-heavy workloads.\n\n\n🌵 LSM-Tree favors write-intensive applications.\n\n\n\nFor example, to decide between Row-based vs Columar Storage. Or OLTP (Online Transaction Processing), OLAP (Online Analytical Processing), or a Hybrid approach:\n\n\nFigure 4: Row vs Columnnar Storage\n\n\nTransactional and High Throughput Needs: For high write throughput and transactional batches (all or nothing), with queries needing wide-column family fetches and indexed queries within the partition, Cassandra/ScyllaDB is better suited.\n\nComplex Analytical Queries: For more complex analytical queries, aggregations on specific columns, and machine learning models, data store(s) such as ClickHouse or Druid is more appropriate. Its optimized columnar storage and powerful query capabilities make it ideal for handling large-scale analytical tasks. Several others include: VictoriaMetrics and InfluxDB (emphasis on time-series); closed-source: Snowflake, BigQuery and Redshift\n\nHybrid Approach: In scenarios requiring both fast write-heavy transactional processing and complex analytics, a common approach is to use Cassandra for real-time data ingestion and storage, and periodically perform ETL (Extract, Transform, Load) or CDC (Change Data Capture) processes to batch insert data into OLAP DB for analytical processing. This leverages the strengths of both databases, ensuring efficient data handling and comprehensive analytical capabilities. Proper indexing and data modeling goes unsaid 🧐\n\n\n🌵 Debezium: Distributed platform for change data capture (more on previous post).\n\n\n\nUsing a HTAP (Hybrid Transactional/Analytical Processing) database that's suitable for both transactional and analytical workloads is worth considering. Example: TiDB, TimescaleDB (Kind of).\n\nWhile you get some of the best from both worlds 🌎, you also inherit a few of the worst from each! Lucky for you, I have first hand experience with it 🤭:\n\nFigure 5: Detailed comparison of OLTP, OLAP and HTAP\n\nAnalogy: Choosing the right database is like picking the perfect ride. Need pay-as-you-go flexibility? Grab a taxi. Tackling heavy-duty tasks? 🚜 Bring in the bulldozer. For everyday use, 🚗 a Toyota fits. Bringing a war tank to a community center is overkill. Sometimes, you need a fleet—a car for daily use, and a truck for heavy loads.\n\n☢️ InfluxDB: Stagnant contribution graph, Flux deprecation, but new benchmarks!\n\n\n\n\n4.2. Partition and Indexes\n\nWithout getting into too much detail, it's crucial to choose the right partitioning strategy (Ex: Range, List, Hash) to ensure partitions don't bloat and effectively support primary read patterns (in this context, example: client_id + region + 1st Day of Month).\n\n\nFigure 6: Types of Indexes and Materialized view\n\nFollowing this, clustering columns and indexes help organize data within partitions to optimize range queries and sorting. Secondary indexes (within the partition/local or across partitions/global) are valuable for query patterns where partition or primary keys don't apply. Materialized views for precomputing and storing complex query results, speeding up read operations for frequently accessed data.\n\n\nFigure 7: Partition Key, Clustering Keys, Local/Global Secondary Indexes and Materialized views\n\nMulti-dimensional Index (Spatial/Spatio-temporal): Indexes such as B+ trees and LSM trees are not designed to directly store higher-dimensional data. Spatial indexing uses structures like R-trees and Quad-trees and techniques like geohash. Space-filling curves like Z-order (Morton) and Hilbert curves interleave spatial and temporal dimensions, preserving locality and enabling efficient queries.\n\n \nFigure 8: Commonly Used: Types of Spatial Indexes\n\n🌵 GeoMesa: spatio-temporal indexing on top of the Accumulo, HBase, Redis, Kafka, PostGIS and Cassandra. XZ-Ordering: Customizing Index Creation.\n\n Next blog post is all about spatial indexes!\n\n\n\n\n\n4.3. Analytics and Alerts\n\nTypically, analytics are performed as batch queries on bounded datasets of recorded events, requiring reruns to incorporate new data.\n\n\nFigure 9: Analytics on Static, Relative and In-Motion Data\n\nIn contrast, streaming queries ingest real-time event streams, continuously updating results as events are consumed, with outputs either written to an external database or maintained as internal state.\n\n\nFigure 10: Batch Analytics vs Stream Analytics\n\n\n    \n        Feature\n        Batch Analytics\n        Stream Analytics\n    \n    \n        Data Processing\n        Processes large volumes of stored data\n        Processes data in real-time as it arrives\n    \n    \n        Result Latency\n        Produces results with some delay; near real-time results with frequent query runs\n        Provides immediate insights and actions\n    \n    \n        Resource Efficiency\n        Requires querying the database often for necessary data\n        Continuously updates results in transient data stores without re-querying the database\n    \n    \n        Typical Use\n        Ideal for historical analysis and periodic reporting\n        Best for real-time monitoring, alerting, and dynamic applications\n    \n    \n        Complexity Handling\n        Can handle complex queries and computations\n        Less effective for highly complex queries\n    \n    \n        Backfill\n        Easy to backfill historical data and re-run queries\n        Backfill can potentially introduce complexity\n    \n\n\n\n🌵 Anomaly Detection and Remediation\n\n🌵 MindsDB: Connect Data Source, Configure AI Engine, Create AI Tables, Query for predictions and Automate workflows.\n\n\n\n\n\n\n5. References\n\n1. Wikipedia, \"Telemetry,\" available: https://en.wikipedia.org/wiki/Telemetry. [Accessed: June 5, 2024].\n2. Apache Cassandra, \"Cassandra,\" available: https://cassandra.apache.org. [Accessed: June 5, 2024].\n3. VictoriaMetrics, \"VictoriaMetrics,\" available: https://victoriametrics.com. [Accessed: June 6, 2024].\n4. Fluentd, \"Fluentd,\" available: https://www.fluentd.org. [Accessed: June 5, 2024].\n5. Elasticsearch, \"Elasticsearch,\" available: https://www.elastic.co. [Accessed: June 5, 2024].\n6. InfluxData, \"Telegraf,\" available: https://www.influxdata.com. [Accessed: June 5, 2024].\n7. InfluxData, \"Telegraf Plugins,\" available: https://docs.influxdata.com. [Accessed: June 5, 2024].\n8. GitHub, \"osx-cpu-temp,\" available: https://github.com/lavoiesl/osx-cpu-temp. [Accessed: June 5, 2024].\n9. GitHub, \"Inlets,\" available: https://github.com/inlets/inlets. [Accessed: June 5, 2024].\n10. InfluxData, \"Telegraf Installation,\" available: https://docs.influxdata.com/telegraf/v1. [Accessed: June 5, 2024].\n11. InfluxData, \"InfluxDB Line Protocol,\" available: https://docs.influxdata.com/influxdb/v1.8/write_protocols/line_protocol. [Accessed: June 5, 2024].\n12. GitHub, \"Telegraf Exec Plugin,\" available: https://github.com/influxdata/telegraf/tree/master/plugins/inputs/exec. [Accessed: June 5, 2024].\n13. GitHub, \"Telegraf Output Plugins,\" available: https://github.com/influxdata/telegraf/tree/master/plugins/outputs. [Accessed: June 5, 2024].\n14. Pallets Projects, \"Flask,\" available: https://flask.palletsprojects.com. [Accessed: June 5, 2024].\n15. Apache Kafka, \"Kafka,\" available: https://kafka.apache.org. [Accessed: June 5, 2024].\n16. Confluent, \"Kafka Partitions,\" available: https://www.confluent.io. [Accessed: June 5, 2024].\n17. AWS, \"Amazon Kinesis,\" available: https://aws.amazon.com/kinesis. [Accessed: June 5, 2024].\n18. Redpanda, \"Redpanda,\" available: https://redpanda.com. [Accessed: June 5, 2024].\n19. Apache, \"Apache Flink,\" available: https://flink.apache.org. [Accessed: June 6, 2024].\n20. GitHub, \"flink-python/pyflink/examples,\" available: https://github.com/apache/flink/tree/master/flink-python/pyflink/examples. [Accessed: June 6, 2024].\n21. Apache, \"Flink Download,\" available: https://www.apache.org/dyn/closer.lua/flink. [Accessed: June 6, 2024].\n22. Apache, \"Flink Kafka Connector,\" available: https://www.apache.org/dyn/closer.lua/flink/flink-connector-kafka-3.1.0. [Accessed: June 6, 2024].\n23. Docker, \"Docker Installation,\" available: https://docs.docker.com. [Accessed: June 6, 2024].\n24. Apache Kafka, \"Kafka CLI,\" available: https://kafka.apache.org/quickstart. [Accessed: June 6, 2024].\n25. Homebrew, \"Kafka Installation,\" available: https://formulae.brew.sh/formula/kafka. [Accessed: June 6, 2024].\n26. Apache, \"Apache Storm,\" available: https://storm.apache.org. [Accessed: June 6, 2024].\n27. Apache, \"Apache Samza,\" available: https://samza.apache.org. [Accessed: June 6, 2024].\n28. ClickHouse, \"ClickHouse,\" available: https://clickhouse.com. [Accessed: June 6, 2024].\n29. InfluxData, \"InfluxDB Benchmarks,\" available: https://www.influxdata.com/benchmarks. [Accessed: June 6, 2024].\n30. TiDB, \"TiDB,\" available: https://github.com/pingcap/tidb. [Accessed: June 6, 2024].\n31. Timescale, \"TimescaleDB,\" available: https://www.timescale.com. [Accessed: June 6, 2024].\n32. MindsDB, \"MindsDB,\" available: https://docs.mindsdb.com. [Accessed: June 6, 2024].\n33. Wikipedia, \"Write Amplification,\" available: https://en.wikipedia.org/wiki/Write_amplification. [Accessed: June 6, 2024].\n34. GitHub, \"LSM-Tree,\" available: https://tikv.github.io/deep-dive/introduction/theory/lsm-tree.html. [Accessed: June 6, 2024]."

    },
  
  
  
  
    {

      "title"    : "B Trees and B+ Trees",
      "url"      : "/b-tree",
      "index"    : "grapes",
      "content"  : "This post aims to correlate the use of B+ Trees as indexes in DBMS, rather than delving deeply into the specifics of B+ Trees. And is primarily derived from Abdul Bari's explanation on B Trees.\n\n\n0. Foundation\n\nDisk Structure\nLet's start with disk structure. It's a platter with concentric circles, which are logical not physical. These circles of different radii are called tracks, and the vertical sections are called sectors. The intersection of a track and a sector is called a block. Hence, any location on the disk, i.e., block address, can be identified by the track number and sector number.\n\n\n\nHow is Data Stored on Disk\nLet's consider a block size of 512 bytes (hypothetical), meaning each block is 512 bytes. All read and write operations are in terms of blocks. Considering one block of 512 bytes, we have a beginning address of 0 up to 511. Each byte can have its own address and is called an offset. Now, we can refer to any one byte on the disk in terms of block address and offset.\n\nThe disk is mounted on a spindle and has a head post. By spinning, we can change the sectors, and with the arm movement of the head (for reading and writing), the tracks are changed. This allows us to access any block or byte of data.\n\n\nFigure 1: Main Memory for Processing\n\nAnother type of memory is main memory (RAM). We run programs in main memory, and these programs access the data on the disk. To do so, the data has to be brought into the main memory to be accessed, and any updated results are written back to the disk. Thus, the data cannot be directly processed on the disk and must be brought into the main memory.\n\nOrganizing the data inside the main memory as used by the program involves data structures, while organizing the data on the disk so that it can be accessed efficiently is managed by a DBMS (Database Management System).\n\nMoving on to understanding how the data is organized on the disk, let's consider an employee table with columns such as ID, name, department, and several others, containing 100 records/rows. Each record is 128 bytes, and each block on the disk is 512 bytes. Hence, the number of records that can be stored in each block is 4.\n\nFigure 2: Disk Organization\n\nNumber of blocks required = Total Number of Records / Records per block = 100/4 = 25 blocks are required for storing 100 records.\n\n\nWhat is Indexing\nNow, let's consider a query to search for a particular record. Because of the sequential access, this would require reading up to 25 blocks on the disk. To reduce this time and the number of blocks to scan, we prepare an index table. Each record in the index has a pointer to the table inside the disk/block, called the record pointer.\n\n\nFigure 3: Table Index\n\nTypically, for a dense index, each record in the table has an entry in the index. The index is also stored on the disk. Given that the index is stored on the disk, the space it takes can be calculated as follows: We know that the ID takes 10 bytes, and assume the pointer takes 6 bytes. So, each entry in the index takes 16 bytes.\n\nFor 100 records, the size of the index on the disk is: 100 × 16 bytes = 1600 bytes. And the number of blocks required is 1600 bytes/512 bytes per block = 3.125. Approximately, we need 4 blocks to store the index records on the disk.\n\nThis significantly reduces the number of blocks to be read on the disk. By finding the ID in the index (accessing up to 4 blocks), followed by using the pointer address to find the record (accessing 1 block), the maximum number of blocks to access a record is reduced from 25 to 5 blocks.\n\n\nWhat is Multi-Level Indexing\n\nBut as the number of records grows, the size of the index also grows. For instance, if we have 1,000 records in the employee table, this would take up 250 blocks on the disk. The index would now need 32 blocks for the 1,000 entries, leading to the index itself becoming too large or requiring too many blocks to scan.\n\n\nFigure 4: Multi Level Index\n\nThis can be optimized by using multi-level indexing. The level-2 index points to the first record of each level-1 index block. Each level-1 index block contains 32 index records (ID + Pointer). Thus, each entry in the level-2 index holds the first ID and a pointer to a level-1 index block. This sparse index reduces the number of blocks needed to search.\n\nBy adding another level of indexing, the number of blocks to scan for a given ID is reduced from 33 blocks (32 level-1 Index + 1 table block) to 3 blocks (1 level-2 index + 1 level-1 index + 1 table block).\n\n\nFigure 4: Two Level Sparse Index\n\nThe multi-level index forms a tree structure. However, manually adding indexes every time the number of records increases significantly is not feasible. What we want is a system that can automatically add and delete higher-level indices, resulting in a self-managed multi-level indexing system.\n\nThis concludes the foundation and introduces the basic concept behind B-trees, B+ trees, and more generally, M-way search trees.\n\n\n\n\n\n1. M-way Search Trees\nStarting with a BST (Binary Search Tree), each node has one key/value and at most two children: the left child contains values less than the parent node, and the right child contains values greater than the parent node.\n\n\nFigure 5: BST vs M-Way Search Tree\n\nExtending this to a similar search tree structure with multiple keys per node, consider an example with 2 keys: [20, 50]. Here, the keys are arranged in ascending order, and the node can have 3 children: one for values less than 20, one for values between 20 and 50, and one for values greater than 50.\n\nThese are called M-Way Search Trees. The above example is a 3-way search tree, where the node has 2 keys and at most 3 children. In general, an M-Way Search Tree can have at most M children and (M - 1) keys. Thus, M is based on the number of children, representing the degree of a node.\n\n\nFigure 6: BST and 4-Way Search Tree Node\n\nSimilar to a BST, where a node has a key/data and left and right child pointers, in an M-Way Search Tree, a node can have up to M children pointers and (M - 1) keys. For example, in a 4-way search tree, a node can have up to 4 children pointers and 3 keys.\n\n\nFigure 7: M-Way Search Tree (Multi Index) Node\n\nFor multi-level indexes, we can use an M-Way Search Tree, where each node contains keys, child pointers, and record pointers (to the database record).\n\nHowever, let's take an example of using a 10-Way Search Tree for Multi-Level Indexing to see the issue with M-Way Trees. Where each node can have at most 10 children (M - 1) and 9 keys. Insert the keys: 10, 20, 30\n\n\nFigure 8: M-Way Search Tree - Out of Control\n\nWhile it may seem obvious to first fill up the node keys before branching to a new child, M-Way Search Trees have no strict rules enforced on branching (inserts and deletes). In the worst case, an M-Way Search Tree could have a length of N for N number of keys, which is as bad as a linear search.\n\n\n\n\n\n\n\n2. B Trees\nIn short, B-trees are M-Way trees with rules. The 4 rules:\n\nA node should have at least ⌈M/2⌉ children before creating a new child node to control the height of the M-Way Search Tree.\nThe root node can have a minimum of 2 children without the restriction of Point 1.\nAll leaf nodes should be at the same level.\nThe creation process is bottom-up.\n\n\nTaking an example, M = 4 (4 children, 3 keys), starting with keys: [10, 20, 40, 50]:\nInitial Insertion:\n\nInsert 10: The tree is empty, so 10 becomes the first key in the root node.\nInsert 20: 20 is greater than 10, so it is placed in the same node, resulting in [10, 20].\nInsert 40: 40 is greater than both 10 and 20, so it is placed in the same node, resulting in [10, 20, 40].\n\n\n\nFigure 9: B Tree, Insertion and Overflow (1)\n\nHandling Overflow:\n\nInsert 50: The node now has 4 keys, exceeding the limit of 3 keys per node. This requires splitting the node.\nThe middle key, 40, will be promoted to a new root node.\nThe keys are split into two nodes: [10, 20] and [50], with 40 as the root.\n\n\n\nAdding more keys [60, 70, 70]:\n\nInsert 60: Goes to the right node [50], resulting in [50, 60].\nInsert 70: Goes to the right node [50, 60], resulting in [50, 60, 70].\n\n\n\nFigure 10: B Tree, Insertion and Overflow (2)\n\nHandling Overflow in Right Node:\n\nInsert 80: The right node now has 4 keys, exceeding the limit. The middle key, 70, is promoted to the root.\nThe keys are split into two nodes: [50, 60] and [80].\n\n\n\nAdding more keys [30, 35]:\n\n\nInsert 30: Goes to the left node [10, 20], resulting in [10, 20, 30].\nInsert 35: The left node [10, 20, 30] now has 4 keys, exceeding the limit. The middle key, 30, will be promoted to the root.\nThe keys are split into two nodes: [10, 20] and [35].\n\n\n\nFigure 11: B Tree, Insertion and Overflow (3)\n\n\nThe root now has [30, 40, 70].\nChildren: [10, 20], [35], [50, 60], [80].\n\n\nAdding more keys [5, 15]:\n\nInsert 5: Goes to the leftmost node [10, 20], resulting in [5, 10, 20].\nInsert 15: The leftmost node [5, 10, 20] now has 4 keys, exceeding the limit. The middle key, 15, will be promoted to the parent node.\n\n\n\nFigure 12: B Tree, Insertion and Overflow (3)\n\n\nThe node [30, 40, 70] will now have [15, 30, 40, 70], and needs to split because it has exceeded the limit.\nThe middle key, 40, will be promoted to a new root.\nThe keys are split into three nodes: [15, 30] and [70].\nRe-arrange the links/connections.\n\n\n\nFigure 13: B Tree, Final Form\n\nNote: Splitting the node in the above example is ⌈M/2⌉ (Ciel), which makes it left-biased, whereas choosing ⌊M/2⌋ (Floor) is still valid and would be right-biased.\n\n\n\n\n\n3. B+ Trees\n\nB+ Tree is a variant of a B Tree. In a B Tree, every node has keys with record pointers, as shown in Figure 7. In contrast, a B+ Tree does not have record pointers in every node. Instead, only the leaf nodes have record pointers.\n\n\nFigure 13: B+ Tree\n\nEvery key in the B+ Tree has its copy in the leaf node along with its record pointer. Additionally, the leaf nodes are connected, forming a linked list, making the leaf-node level a dense index, which matches the format of the multi-level index we wanted (Figure 4).\n\n\n\n\n4. References\n\"Platter,\" Wikipedia. [Online]. Available: https://en.wikipedia.org/wiki/Platter.\n\"Sector,\" Wikipedia. [Online]. Available: https://en.wikipedia.org/wiki/Sector.\n\"Random-access memory,\" Wikipedia. [Online]. Available: https://en.wikipedia.org/wiki/Random-access_memory.\n\"Database index,\" Wikipedia. [Online]. Available: https://en.wikipedia.org/wiki/Database_index.\n\"Dense Index,\" MLWiki. [Online]. Available: https://mlwiki.org/index.php/Dense_Index.\n\"Sparse Index,\" MLWiki. [Online]. Available: https://mlwiki.org/index.php/Sparse_Index.\n\"Binary search tree,\" Wikipedia. [Online]. Available: https://en.wikipedia.org/wiki/Binary_search_tree.\n\"B-tree,\" Wikipedia. [Online]. Available: https://en.wikipedia.org/wiki/B-tree.\n\"B+ tree,\" Wikipedia. [Online]. Available: https://en.wikipedia.org/wiki/B%2B_tree.\n\"M-ary tree,\" Wikipedia. [Online]. Available: https://en.wikipedia.org/wiki/M-ary_tree.\n\"B-Trees and B+ Trees,\" YouTube. [Online]. Available: https://www.youtube.com/watch?v=aZjYr87r1b8."

    },
  
  
  
  
    {

      "title"    : "Debezium: PostgreSQL Change Data Capture (CDC)",
      "url"      : "/debezium-postgres-cdc",
      "index"    : "tangerine",
      "content"  : "Figure 1: Debezium Postgres Connector\n\n1. Goal\nSet up Debezium to capture row-level changes in the schemas of a PostgreSQL database and publish to Kafka topic(s).\n\nThe high-level architecture is unquestionably explained in the above diagram 😎. Pikachu, aka Debezium PostgreSQL Connector, detects and carries/publishes row-level change events to Kafka topic(s) for configured Postgres tables.\n\n\n\n\n2. Definitions\n\n2.1. Change Data Capture (CDC)\nIn databases, change data capture (CDC) is a set of software design patterns used to determine and track the data that has changed (the \"deltas\") so that action can be taken using the changed data [1].\n\n\n\n2.2. Debezium\nDebezium is a set of distributed services to capture changes in your databases so that your applications can see those changes and respond to them. Debezium records all row-level changes within each database table in a change event stream, and applications simply read these streams to see the change events in the same order in which they occurred [2].\n\n\n\n2.3. Debezium Connectors\nA library of connectors that capture changes from a variety of database management systems and produce events with very similar structures, making it far easier for your applications to consume and respond to the events regardless of where the changes originated [3].\n\n\n\n2.4. Debezium connector for PostgreSQL\nThe Debezium PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database [4].\n\n\n\n2.5. Kafka\nApache Kafka is a distributed data store optimized for ingesting and processing streaming data in real-time. Streaming data is data that is continuously generated by thousands of data sources, which typically send the data records in simultaneously [5].\n\n\n\n2.6. Kafka Connect\nKafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka [6].\n\n\n\n\n\n3. Define Services (Docker-Compose)\nAs a generate note, If you use Mac M1/M2, ensure the docker image has linux/arm64 OS/ARCH.\n \n\nSection 3.x covers the breakdown of each service/docker image used in docker-compose.yaml file, if you have worked with docker before, skip the section and pick up the entire file from section 4 instead.\n\nBreak down of services in docker-compose.yaml\n\n\nPostgres: The database containing the table(s) for which CDC is tracked.\n\nKafka and Zookeeper: The event broker where CDC events are stored.\n\nSchema Registry: To serialize/deserialize CDC message(s) using Avro schema.\n\nDebezium: Responsible for capturing the row-level changes made to Postgres table(s) and streaming them to a Kafka topic.\n\n\n3.1. PostgreSQL\ndebezium/postgres: PostgreSQL for use with Debezium change data capture. This image is based upon postgres along with logical decoding plugin from Debezium\n\ndpage/pgadmin4 (Optional): Web browser version of pgAdmin 4 for the ease of running DML and DDL operations on PostgreSQL.\n\n\npostgres:\n  image: debezium/postgres:13-alpine\n  ports:\n    - 5432:5432\n  environment:\n    - POSTGRES_USER=admin\n    - POSTGRES_PASSWORD=root\n    - POSTGRES_DB=pyblog\n\npgadmin:\n  image: dpage/pgadmin4\n  environment:\n    - PGADMIN_DEFAULT_EMAIL=admin@admin.com\n    - PGADMIN_DEFAULT_PASSWORD=root\n  ports:\n    - '5050:80'\n  restart: always\n\n\n\n\n\n3.2. Kafka and Zookeeper\n\nConfluent Platform Docker images for Kafka: confluentinc/cp-enterprise-kafka/postgres and Zookeeper: confluentinc/cp-zookeeper. The below example is for version 7.3, a more recent version, i.e., 7.5 onwards, Confluent recommends KRaft mode for new deployments, and Zookeeper is deprecated.\n\n\nzookeeper:\n  image: confluentinc/cp-zookeeper:7.3.5\n  environment:\n    ZOOKEEPER_CLIENT_PORT: 2181\n\nkafka:\n  image: confluentinc/cp-enterprise-kafka:7.3.5\n  depends_on: [zookeeper]\n  environment:\n    KAFKA_BROKER_ID: 1\n    KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n    KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092\n    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n    KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n    KAFKA_JMX_PORT: 9991\n  ports:\n    - 9092:9092\n\n\n\n\n\n\n3.3. Debezium and Schema Registry\n\ndebezium/connect image defines a runnable Kafka Connect service preconfigured with all Debezium connectors; it monitors database management system(s) for changing data and then forwards those changes directly into Kafka topics organized by server, database, and table.\n\n\nconfluentinc/cp-schema-registry enables client applications to read and write Avro data, in this case, to serialize and deserialize CDC messages.\n\n\n\ndebezium:\n  image: debezium/connect:2.4\n  environment:\n    BOOTSTRAP_SERVERS: kafka:9092\n    GROUP_ID: 1\n    CONFIG_STORAGE_TOPIC: connect_configs\n    OFFSET_STORAGE_TOPIC: connect_offsets\n    STATUS_STORAGE_TOPIC: my_status_topic\n    CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8085\n    CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8085\n  depends_on: [kafka]\n  ports:\n    - 8083:8083\n\nschema-registry:\n  image: confluentinc/cp-schema-registry:7.3.5\n  environment:\n    - SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL=zookeeper:2181\n    - SCHEMA_REGISTRY_HOST_NAME=schema-registry\n    - SCHEMA_REGISTRY_LISTENERS=http://schema-registry:8085,http://localhost:8085\n  ports:\n    - 8085:8085\n  depends_on: [zookeeper, kafka]\n\n\n\n\n\n\n\n\n4. Start Services (Docker-Compose)\n\nThe complete docker-compose.yaml to set up Postgres with debezium and publish data change events to Kafka:\n\nNote: At the time of writing this post, the services use the current stable version(s); visit the docker hub page for the latest stable version(s).\n\nversion: \"3.7\"\nservices:\n  postgres:\n    image: debezium/postgres:13-alpine\n    ports:\n      - 5432:5432\n    environment:\n      - POSTGRES_USER=admin\n      - POSTGRES_PASSWORD=root\n      - POSTGRES_DB=pyblog\n\n  pgadmin:\n    image: dpage/pgadmin4\n    environment:\n      - PGADMIN_DEFAULT_EMAIL=admin@admin.com\n      - PGADMIN_DEFAULT_PASSWORD=root\n    ports:\n      - '5050:80'\n    restart: always\n\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.3.5\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n\n  kafka:\n    image: confluentinc/cp-enterprise-kafka:7.3.5\n    depends_on: [zookeeper]\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      KAFKA_JMX_PORT: 9991\n    ports:\n      - 9092:9092\n\n  debezium:\n    image: debezium/connect:2.4\n    environment:\n      BOOTSTRAP_SERVERS: kafka:9092\n      GROUP_ID: 1\n      CONFIG_STORAGE_TOPIC: connect_configs\n      OFFSET_STORAGE_TOPIC: connect_offsets\n      STATUS_STORAGE_TOPIC: my_status_topic\n      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8085\n      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8085\n    depends_on: [kafka]\n    ports:\n      - 8083:8083\n\n  schema-registry:\n    image: confluentinc/cp-schema-registry:7.3.5\n    environment:\n      - SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL=zookeeper:2181\n      - SCHEMA_REGISTRY_HOST_NAME=schema-registry\n      - SCHEMA_REGISTRY_LISTENERS=http://schema-registry:8085,http://localhost:8085\n    ports:\n      - 8085:8085\n    depends_on: [zookeeper, kafka]\n\n\n\n\n4.1. Run all containers\n\nClean-up (Optional) and Run (Create and Start) containers:\ndocker rm -f $(docker ps -a -q)\ndocker-compose up -d\n\n\n \nMake a note of the assigned network name; from the above output, the network name is: enricher_default. To create a custom network, refer Networking in Compose\n\n\n\n\n\n5. Configure Services\n\nEnd-to-end configuration for all the services to create the CDC pipeline:\n\n5.1. Create Postgres Tables\n\na. Login to pgAdmin localhost:5050 with email/password (admin@admin.com/root) configured in pgadmin container (refer: docker-compose.yaml)\n \n\n\n\nb. Register database server with username/password (admin/root) and hostname (postgres) configured in postgres container (refer: docker-compose.yaml)\n \n\n\n\nc. Create and Alter table queries:\nExample: Create a table user-profile from the query tool to track data change events in this table. Skip this; if you already have your own schema for Postgres database tables for which CDC has to be configured.\n \n\nCREATE TABLE user_profile (\n  user_id INT NOT NULL,\n  full_name VARCHAR(64) NOT NULL,\n  email VARCHAR(255) NOT NULL,\n  PRIMARY KEY (user_id),\n  UNIQUE (email)\n);\n\nALTER TABLE user_profile REPLICA IDENTITY FULL;\n\n\nSetting the table's replication identity to full infers that the entire row is used as the identifier for change-tracking.\n\n\n\n\n\n5.2. Set up Debezium Postgres Connector (Kafka Connect)\n\na. Check the status of the Kafka Connect service:\ncurl -H \"Accept:application/json\" localhost:8083/\n\n \n\n\n\nb. Register the Debezium Postgres connector:\nCreate a file debezium.json, the Debezium Postgres connector configuration, where user_profile is the table being tracked \n\n{\n    \"name\": \"postgresql-connector\",\n    \"config\": {\n        \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n        \"plugin.name\": \"pgoutput\",\n        \"database.hostname\": \"postgres\",\n        \"database.port\": \"5432\",\n        \"database.user\": \"admin\",\n        \"database.password\": \"root\",\n        \"database.dbname\": \"pyblog\",\n        \"database.server.name\": \"postgres\",\n        \"table.include.list\": \"public.user_profile\",\n        \"table.whitelist\": \"public.user_profile\",\n        \"database.tcpKeepAlive\": true,\n        \"topic.prefix\": \"topic_user_profile\"\n    }\n}\n\n\nThis command uses the Kafka Connect service’s API to submit a POST request against the /connectors resource with a JSON document that describes the new connector (called postgresql-connector).\n\ncurl -i -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" localhost:8083/connectors/ --data \"@debezium.json\"\n\n \n\n\n\nc. Check the list of connectors registered with Kafka Connect:\n\ncurl -H \"Accept:application/json\" localhost:8083/connectors/\n\n \n\n\n\n\n\n5.3. View Kafka Messages\n\na. Pull kafkacat docker image:\n\ndocker pull confluentinc/cp-kafkacat:7.1.9\n\nkafkacat is a commandline tool for interacting with Kafka brokers. It can be used to produce and consume messages, as well as query metadata.\n\n\n\nb. Listing topics on a broker:\nFor the Kafka broker is accessible as kafka:9092 on the Docker network enricher_default, list topics by running:\n\ndocker run --tty \\\n--network enricher_default \\\nconfluentinc/cp-kafkacat:7.1.9 \\\nkafkacat -b kafka:9092 \\\n-L\n\n\n\n\nc. Consuming messages from a topic:\n\nFor the Kafka broker is accessible as kafka:9092 on the Docker network enricher_default, print messages and their associated metadata from topic topic_user_profile.public.user_profile:\n\ndocker run --tty \\\n--network enricher_default \\\nconfluentinc/cp-kafkacat:7.1.9 \\\nkafkacat -b kafka:9092 -C \\\n-t topic_user_profile.public.user_profile\n\n\n \n\nIf you get the error % ERROR: Topic topic_user_profile.public.user_profile error: Broker: Leader not available, run the same command again!\n\n\n\n\n\n\n\n6. Moment of Truth 🚀\n\na. Insert/Update a row in Postgres table:\nFor the table, Debezium CDC is configured; Following the example, creating a row in user_profile\n\nINSERT INTO user_profile\n (user_id, full_name, email) \nVALUES\n (1,'John Ross', 'john.ross@pyblog.xyz');\n\n\nb. Validate messages in Kafka topic:\nConsuming the Kafka messages, as mentioned in 3.2.4, section c, the output for inserting a new row:\n\n \n\n\n\nc. Stop services and delete Docker Containers:\n\nTo stop all the services and delete the docker containers, run:\n\ndocker-compose down\ndocker rm -f $(docker ps -a -q)\n\n \n\n\n\n\n\n7. Conclusion\nThe post demonstrated how to capture data change events with Debezium by streaming data from a PostgreSQL database to Kafka.\n\nChange Data Capture (CDC) has a lot of use cases, some of the top uses:\nUpdating/Invalidating Cache, Enriching Data/Logs from Entity Identifiers, Real-time data loading into Data Warehouse(s) and search engine(s), Synchronize data (on-premises to cloud), Microservices Data exchange with the Outbox Pattern and many more.\n\n\nWhats' next: In the next post, we see how to process the CDC events with stream processing engines such as Apache Flink, cache the transformed data (RockDB), and enrich/cleanse other events with more meaningful information than their raw versions without having to query the source database.\n\n\n\n\n8. References\n\n\n[1] Wikipedia Contributors, “Change data capture,” Wikipedia, Feb. 04, 2019. https://en.wikipedia.org/wiki/Change_data_capture\n\n[2] “Debezium Documentation :: Debezium Documentation,” debezium.io. https://debezium.io/documentation/reference/stable/index.html\n\n[3] “Connectors :: Debezium Documentation,” debezium.io. https://debezium.io/documentation/reference/stable/connectors/index.html\n\n[4] “Debezium connector for PostgreSQL :: Debezium Documentation,” debezium.io. https://debezium.io/documentation/reference/stable/connectors/postgresql.html (accessed Oct. 21, 2023).\n\n[5] “What is Apache Kafka? | AWS,” Amazon Web Services, Inc. https://aws.amazon.com/msk/what-is-kafka/\n\n[6] “Kafka Connect | Confluent Documentation,” docs.confluent.io. https://docs.confluent.io/platform/current/connect/index.html\n‌\n‌[7] J. P. Alvim, “Streaming data from PostgreSQL to s3 using Debezium, Kafka and Python,” Medium, Feb. 11, 2023. https://medium.com/@joaopaulonobregaalvim/streaming-data-from-postgresql-to-s3-using-debezium-kafka-and-python-16c6cdd6dc1e (accessed Oct. 21, 2023).\n\n[8] D. Danushka, “Configuring Debezium to Capture PostgreSQL Changes with Docker Compose,” Tributary Data, Aug. 16, 2021. https://medium.com/event-driven-utopia/configuring-debezium-to-capture-postgresql-changes-with-docker-compose-224742ca5372 (accessed Oct. 21, 2023)."

    },
  
  
  
  
    {

      "title"    : "Postgres as a Graph Database",
      "url"      : "/postgres-as-graph",
      "index"    : "kiwi",
      "content"  : "Neo4j vs PostgreSQL\n\nHave you ever come across the need to store a graph in a relational database because using/onboarding a graph database for a small use-case is overkill?\n\nBefore jumping into using a relational database like MySQL or PostgreSQL as a graph database, let’s lay down the fundamentals:\n\nWhat is a Graph?\nA graph is a set of vertices/nodes interconnected by edges/links. The edges can be directed (unidirectional or bidirectional) or undirected (no orientation, only infers a connection between nodes).\n\n\nFigure 1: (Left to Right) Undirected, Unidirectional and Bidirectional\n\nGraph Data Structure in Java\nA vertex represents the entity, and an edge represents the relationship between entities:\nclass Vertex {\n    Integer label;\n    // standard constructor, getters, setters\n}\n\nThe graph would be a collection of vertices and edges:\nclass Graph {\n    private Map&lt;Vertex, List&lt;Vertex&gt;&gt; adjVertices;\n    // standard constructor, getters, setters\n\n    void addVertex(Integer label) {\n        adjVertices.putIfAbsent(new Vertex(label), new ArrayList&lt;&gt;());\n    }\n\n    void addEdge(Integer label1, Integer label2) {\n        Vertex v1 = new Vertex(label1);\n        Vertex v2 = new Vertex(label2);\n        adjVertices.get(v1).add(v2);\n        adjVertices.get(v2).add(v1);\n    }\n}\n\n\nGraph createGraph() {\n    Graph graph = new Graph();\n    graph.addVertex(1);\n    graph.addVertex(2);\n    graph.addVertex(3);\n    graph.addVertex(4);\n    graph.addVertex(5);\n    graph.addVertex(6);\n    \n    graph.addEdge(1, 2);\n    graph.addEdge(1, 3);\n    graph.addEdge(2, 1);\n    graph.addEdge(2, 4);\n    graph.addEdge(3, 5);\n    graph.addEdge(3, 6);\n    \n    return graph;\n}\n\n\nFigure 2: Graph visual and map data-structure representation\n\nGraph Data Structures in Postgres\n\nStoring the graph Map&lt;Vertex, List&lt;Vertex&gt;&gt; in a relational database such as Postgres as-is would mean creating two tables: Vertex and Graph:\nCREATE TABLE vertex (\n    vertex_id INT,\n    PRIMARY KEY(vertex_id)\n    // Other columns\n);\n\nCREATE TABLE graph (\n    graph_id INT,\n    vertex_id INT,\n    vertices INTEGER[]\n\n    PRIMARY KEY(graph_id),\n    CONSTRAINT fk_vertex_id\n    FOREIGN KEY(vertex_id)\n    REFERENCES vertex(vertex_id)\n    ON DELETE NO ACTION\n);\n\nIdeally, each element in the graph -&gt; vertices array should represent foreign keys to the vertex table.\n\nRelational databases operate most efficiently on properly normalized data models. Arrays are not relational data structures, by definition they are sets; while the SQL standard supports defining foreign keys on array elements, PostgreSQL currently does not support it. However, there is an ongoing effort to implement this.\n\nA better way to store a graph in Postgres is by creating two tables: vertex and edge\n\nFigure 3: Graph representation - List of Edges\n\nCREATE TABLE vertex (\n    vertex_id INT,\n    PRIMARY KEY(vertex_id)\n    // Other columns\n);\n\nCREATE TABLE edge (\n    source_vertex INT REFERENCES vertex(vertex_id),\n    target_vertex INT REFERENCES vertex(vertex_id),\n    PRIMARY KEY (source_vertex, target_vertex)\n);\n\nThe table edge represents the relationship between two vertices; the composite primary key (source_vertex, target_vertex) ensures that each edge is unique.\n\nReal World Use-case\nZigbee protocol [2] and its mesh topology is a perfect example of a tree data structure. Zigbee has been around for a long time, initially conceived in the early 1990s, and is a widely wireless technology designed to facilitate low-cost, low-power wireless Internet of Things (IoT) networks.\n\nMoving on to the technical details, Zigbee Devices Types:\n\n  Zigbee Coordinator (ZC)\n  Zigbee Router (ZR)\n  Zigbee Endpoint Device (ZED)\n\n\n\nFigure 4: Zigbee Mesh Topology\n\nThe Zigbee network has exactly one Zigbee Coordinator (ZC) responsible for forming and coordinating the network. The Zigbee Router (ZR) represents intermediate nodes to assist in relaying data between nodes in the network and is instrumental in building the Zigbee network. The Zigbee Endpoint Device (ZED) are nodes that are logically attached to a Zigbee Router (ZR) and are typically devices such as lights, sensors, switches, etc., and communicates only with the Zigbee Router (parent).\n\nCreate Queries\nTables to store ZC/ZR and their relationships in Postgres:\n CREATE TABLE router (\n    id SERIAL PRIMARY KEY,\n    serial_number VARCHAR(64),\n    role VARCHAR(32)\n);\n\nCREATE TABLE neighbor (\n    PRIMARY KEY (source_router, target_router),\n    source_router INTEGER REFERENCES router(id),\n    target_router INTEGER REFERENCES router(id)\n);\n\nThe router table has ZC and ZR, distinguished by the role column; in a Zigbee network, ZC and ZR are essentially routers, where ZC is often called Leader Router/Co-ordinator. The relationship between ZC and ZR(s) and ZR and ZR(s) is stored in the neighbor table.\n\nInsert Queries\nThe data inserts for the mesh topology as shown in Figure 4 (without leaf nodes/end devices):\nINSERT INTO router (serial_number, role) VALUES ('ACX7100-67600', 'ZC');\nINSERT INTO router (serial_number, role) VALUES ('ACX7100-67601', 'ZR');\nINSERT INTO router (serial_number, role) VALUES ('ACX7100-67602', 'ZR');\nINSERT INTO router (serial_number, role) VALUES ('ACX7100-67603', 'ZR');\nINSERT INTO router (serial_number, role) VALUES ('ACX7100-67604', 'ZR');\nINSERT INTO router (serial_number, role) VALUES ('ACX7100-67605', 'ZR');\nINSERT INTO router (serial_number, role) VALUES ('ACX7100-67606', 'ZR');\n\n\n\nFigure 5: Entries in Router\n\nINSERT INTO neighbor (source_router, target_router) VALUES (1, 2);\nINSERT INTO neighbor (source_router, target_router) VALUES (1, 3);\nINSERT INTO neighbor (source_router, target_router) VALUES (1, 6);\nINSERT INTO neighbor (source_router, target_router) VALUES (3, 4);\nINSERT INTO neighbor (source_router, target_router) VALUES (3, 5);\nINSERT INTO neighbor (source_router, target_router) VALUES (6, 7);\n\n\n\nFigure 6: Entries in Neighbor\n\nSelect Queries\nA router (ZC or ZR) has neighbors, and a neighbor router (ZR) also has neighbors (ZR), and this goes on until we have a leaf node (ZED). Querying for all the neighbors of a router = traversing the graph.\n\nPostgres offers built-in recursive queries, typically used for hierarchical or tree-structured data, i.e., to find all the direct and indirect relations to an entity.\n\nGet all neighbors (neighbors of neighbors) for a given router id:\nWITH RECURSIVE all_neighbors AS (\n\tSELECT neighbor.target_router\n\tFROM neighbor\n\tWHERE neighbor.source_router = 1\n\t\n\tUNION\n\t\n\tSELECT neighbor.target_router\n\tFROM neighbor\n\tJOIN all_neighbors ON neighbor.source_router = all_neighbors.target_router\n)\nSELECT router.id, router.serial_number\nFROM router\nJOIN all_neighbors ON router.id = all_neighbors.target_router;\n\n\nFigure 7: All interconnected neighbors\n\nGet all neighbors (neighbors of neighbors - relationships) for a given router id\n\nWITH RECURSIVE all_neighbors AS (\n\tSELECT neighbor.source_router, neighbor.target_router\n\tFROM neighbor\n\tWHERE neighbor.source_router = 1\n\t\n\tUNION\n\t\n\tSELECT neighbor.source_router, neighbor.target_router\n\tFROM neighbor\n\tJOIN all_neighbors ON neighbor.source_router = all_neighbors.target_router\n)\nSELECT all_neighbors.source_router, all_neighbors.target_router FROM all_neighbors;\n\n\n\nFigure 8: Neighbors of neighbors\n\nNote: Postgres recursive queries work with circular graphs and will not lead to an infinite loop.\n\n\n\nReferences\n\n[1] “7.8. WITH Queries (Common Table Expressions),” PostgreSQL Documentation, May 11, 2023. https://www.postgresql.org/docs/current/queries-with.html#QUERIES-WITH-RECURSIVE\n\n[2] BHIS, “Understanding Zigbee and Wireless Mesh Networking,” Black Hills Information Security, Aug. 27, 2021. https://www.blackhillsinfosec.com/understanding-zigbee-and-wireless-mesh-networking/‌\n\n[3]“Apache AGE,” age.apache.org. https://age.apache.org/\n\n[4] D. Paulus, “Postgres: The Graph Database You Didn’t Know You Had,” dylanpaulus.com. https://www.dylanpaulus.com/posts/postgres-is-a-graph-database"

    },
  
  
  
  
    {

      "title"    : "Documentation: Conceptualization of a Cartogram",
      "url"      : "/cartograms-documentation",
      "index"    : "pear",
      "content"  : "Explanation\n\n 1. What is a cartogram?\n\nSimply put, a cartogram is a map. But a cartogram is a unique type of map that combines statistical information such as population with geographic location. Typically, physical or topographical maps show relative area and distance, but they do not provide any data about the inhabitants or the population of a place. For example, a quick and intuitive view of the world map in relation to population makes it easy for viewers to co-relate the effect and the relative measure's gravity. \n\nThe basic idea is distorting the map by resizing the regions by population (or any other metric) since the population is among the most important aspects to consider; for example, if malnutrition is high in a vast country, then the severity is much worse than if malnutrition is high in a tiny country.\n\n\n\n\n\n 2. How are grids related to cartograms?\n\nWith an objective to plot a visually conclusive map by illustrating territories using a method for trading off shape and area.\n\nIt’s vital to ensure the shape or the outline of a region (Example: Country and Province) is preserved, i.e., visualization steps have to be in place so that the resulting cartograms appear similar to the original world cartograms, such that the area is easily recognizable only by its appearance without the explicit need for labels and quickly understand the displayed data.\n\nWhile generating a cartogram algorithmically yields good results, the best cartograms out there are the ones that as designed artistically/manually. This boils down to finding a balance between using algorithms to generate cartograms and manually nitpicking fine details - that's where the grids come into the picture.\n\n \n\nFigure 1: Hex grid cartogram. \n\n\n\n\n\n 3. Choosing the right grid\n\nGrids are built from a repetition of simple shapes such as squares and hexagons. Grids have three types of parts: faces (tiles), edges, and vertices.\n\n\nEach face is a two-dimensional surface enclosed by edges. \n\nEach edge is a one-dimensional line segment ending at two vertices. \n\nEach vertex is a zero-dimensional point\n\n\nSquare\n\nOne of the most commonly used grids is a square grid. It's simple, easy to work with, and maps nicely onto a computer screen. The location uses cartesian coordinates (x, y), and the axes are orthogonal. Not to mention, the coordinate system is the same even if the squares are angled in an isometric or axonometric projection.\n\n\nSquares are 4-sided polygons. \n\nSquares have all the sides the same length. \n\nThey have 4 sides and 4 corners.\n\nEach side is shared by 2 squares. \n\nEach corner is shared by 4 squares.\n\n\nHexagon\n\nHexagonal grids are the next commonly used grids, as they offer less distortion of distances than square grids because each hexagon has more non-diagonal neighbors than a square (diagonals distort grid distances). Moreover, hexagons have a pleasing appearance (the honeycomb is a good example). As for the grids, the position is either pointy tops and flat sides or flat tops and pointy sides.\n\n \n\nFigure 2: Modified from original Image source: @redblobgames\n\n\nHexagons are 6-sided polygons. \n\nRegular hexagons have all the sides the same length. \n\nThey have 6 sides and 6 corners.\n\nEach side is shared by 2 hexagons. \n\nEach corner is shared by 3 hexagons.\n\nTypically, the orientations for hex grids are vertical columns (flat-topped) and horizontal rows (pointy-topped).\n\n\n\n\n\n\n 4. Hexagons vs Squares\n\nSquare grids\n\n\nSquare grids are universally used in Raster datasets in GIS. \n\nEase of definition and storage: the only explicit geographical information necessary to define a raster grid are the coordinates of the origin, cell size, and grid dimensions, i.e., the number of cells in each direction. The attribute data can be stored as an aspatial matrix, and the geographical location of any cell can be derived from the cell’s position relative to the origin - this makes data storage and retrieval easier since the coordinates of the vertices of each grid cell are not explicitly stored.\n\nEase of resampling to different spatial scales: increasing the spatial resolution of a square grid is just a matter of dividing each grid cell into four. Similarly, decreasing the spatial resolution only requires combining groups of four cells into one.\n\n\nHexagonal grids\n\n\nReduced edge effects: a hexagonal grid gives the lowest perimeter to area ratio of any regular tessellation of the plane - this means that edge effects are minimized when working with hexagonal grids.\n\nAll neighbours are identical: square grids have two classes of neighbours, those in the cardinal directions that share an edge and those in diagonal directions that share a vertex. In contrast, a hexagonal grid cell has six identical neighboring cells, each sharing one of the six equal-length sides. Furthermore, the distance between centroids is the same for all neighbors.\n\nBetter fit to curved surfaces: when dealing with large areas, where the curvature of the earth becomes important, hexagons are better able to fit this curvature than squares (this is why soccer balls are constructed of hexagonal panels).\n\n\n \n\nFigure 3: Tessellation of the plane (Square and Hexagon). \n\nHexagonal grid for Cartograms\n\nFor a cartogram, the reasons to choose hexagons over squares are as follows:\n\n\nIt's a better fit for curved surfaces, thereby supporting most geographic projections.\n\nRepresenting a complex-shaped polygon by hexagons offers a lower error factor (tessellation of the plane), i.e., (the actual area of the polygon - Area formed by tiny tiles/hexagons) is lower as compared to that formed by squares.\n\nThey look badass! Without a doubt, hexagonal grids look way more impressive than square grids.\n\n\n\n\n\n\n 5. Building a shape preserved hexagonal grid cartogram\n\nSince the primary dependency is D3 - a Javascript library extensively used for drawing geographic visualizations and uses GeoJSON/TopoJSON for representing shapes on maps by converting them to rendered SVG element(s); explanations are supported by implementation details in D3.\n\n 5.1. Projection\n\n \n\nFigure 4: Mercator projection. \n\nEarth is round or more accurately, an ellipsoid. To show its features on a flat surface, it's not possible to accurately translate a sphere onto a plane, hence the need for projections. For instance, the Mercator projection is famously known to over-exaggerate the size of landmasses near the poles (No wonder Greenland looks massive). \n\nD3 offers a range of built-in projections; however, no projection accurately depicts all points in the globe, so it's important to choose the appropriate projection for the use case. The purpose is simple: translate the latitude and longitude pair to a pair of X and Y coordinates on SVG. Lastly, to fit the coordinates to the SVG element, the fitExtent and rotate are handly, as the projection has no knowledge of the size or extent of the SVG element.\n\n \n\nFigure 5: Projection function to map coordinate. \n\n\n\n\n\n 5.2. Geopath\n\nThe projection function works well for converting points into X and Y coordinates but not lines. A typical map has regions represented by lines and not individual points. Hence to render the map, irregular lines are represented using the path element.\nThe d attribute in &lt;path&gt;&lt;/path&gt; defines the shape of the line.\n\n  &lt;path\n    d=\"M732.4944016581658,608.9022205707863L727.1354648887938,\n    610.9411167803873L706.8155159265721,604.6447353677604L703.587646715891,\n    610.7806528270128L688.0319490712842,611.8868016539795L688.8280117925813, \n    ......\n    ......\n    ......\n    600.4706783128443L788.2046582778905,605.2215195516151L781.7980088643487,\n    600.5439608373076L772.9856281618564,600.8681045994042L760.5726799028025,\n    607.2632255686299L744.3618779892297,607.9935254189165L742.5384536592165,\n    615.3237961667451Z\"\n    stroke=\"white\"\n    fill=\"rgb(211, 211, 211)\"\n  &lt;/path&gt;\n\n\nUssage in D3: const path = d3.geoPath().projection(projection), the path functions takes GeoJSON polygons, and returns a string which can directly be used as the d attribute of an SVG path.\n\nTo render the map, the plan is to:\n\n\nLoop through each country’s GeoJSON polygon\n\nCreate the d attribute string using the d3.geopath function\n\nCreate and append an SVG path element with the above d attribute\n\n\n\n\n\n\n 5.3. Tessellation\n\nA tessellation or tiling is a process of covering a surface or a plane, using one or more geometric shapes, called tiles, with no overlaps or gaps. Furthermore, a variant of symmetric tessellation has a fixed tile size and geometric shape.\n\nFigure 3 shows the tessellation of Sri Lanka using a Hexagon and Square as the tile/cell. However, with the tessellation of a polygon, only the tiles within the polygon are arranged in the same order. Whereas, when dealing with multiple polygons in the same grid, the arrangement of tiles has to be based on the nearest tile that fits in the grid - implying the need for a point grid.\n\n \n\nFigure 6: Consistent Tessellation in a Grid. \n\n\n\n\n\n 5.4. Tessellation of n polygons\n\nPutting it all together, \n\n\nthe first step is forming a grid of points, where each point represents the center of the tile (hexagon/square). Figure 7 shows the point grid for hexagon tiles.\n\nThe next step is to draw the tile relative to each point in the grid (tessellate points), forming the base playground - Then, superimpose the set of polygons (Features in TopoJSON) on the grid playground. \n\nFinally, tessellate each of the polygons by ensuring the tiles chosen are from the previously formed grid of tiles.\n\n\n \n\nFigure 7: Point grid of (Width x Height). \n\n \n\n \n\nFigure 8: Tessellate points with hexagons \n\n \n\n \n\nFigure 9: Draw the TopoJSON on Canvas (the above TopoJSON is the world map scaled by population of 2018). \n\n \n\n \n\nFigure 10: Regularly tessellate each country/polygon in the world-map with hexagons. \n\n\n\n\n\n 5.5. Plotting a cartogram\n\nThis section is a word in progress, stay tuned! 🤓\n\nThe algorithm for generating a cartogram is a variant of continuous area cartograms by James A. Dougenik, Nicholas R. Chrisman, and Duane R. Niemeyer. \n\nThe research paper: An Algorithm to Construct Continous Area Cartograms. Without getting into the exact details, line-by-line, the procedure to produce cartograms is as follows: \n\n \n\nFigure 11: Centroid of all polygons/countries. \n\n\n\n\n\n 5.6. Fixed vs Fluid mode\n\nFixed: The cell size is fixed across years. The cell size is the population count of each cell (a country with a population of 10 million has 20 cells when the cell size is 0.5 million). Irrespective of the year/total population, the cell size remains the same in the Fixed mode.\n\n \n\nFigure 13: Cartogram scaled from 1950 to 1990 in Fixed mode \n\nFluid: On the other hand, in the fluid mode, as the year/total population changes, the cell size is adjusted accordingly to best utilize the entire screen/container to display the cartogram. For example: A region with a total population of 20 million and a cell size of 0.5 million would have the same view when the total population is 40 million, and the cell size is 1 million.\n\n \n\nFigure 14: Cartogram scaled from 1950 to 1990 in Fluid mode \n\n\n\n\n\n\n\nImplementation\n\n 6. Dependencies\n\n\"d3\": \"^7.4.3\",\n\"d3-array\": \"^3.1.6\",\n\"d3-geo\": \"^3.0.1\",\n\"d3-hexbin\": \"^0.2.2\",\n\"topojson-client\": \"^3.1.0\",\n\"topojson-server\": \"^3.0.1\",\n\"topojson-simplify\": \"^3.0.3\"\n\n\n\n\n\n\n 7. Project Structure\n\nThe core module:\n\n\nindex.html: HTML page of the main screen containing the root container and input form fields such as year, radius, scale mode, cell size, export format, and cell color selector.\n\ncartogram.js: Implementation of the algorithm to construct continuous area cartograms.\n\nplot.js: The logic for rendering the tessellated point-grid and plotting the cartogram based on the selected input fields.\n\nshaper.js: Functions dependent on the cell shape; the common pattern followed is to take decisions based on the cell shape using a switch case.\n\nevents.js: All the mouse events in the application, such as single/double click, hover, and drag/drop.\n\n\n\n\n\n\n8. Project Files\n\n\n\n8.1. index.html\n\nCreate a HTML div with a unique id\n\nTo append SVG, i.e., the hexagonal grid and polygons/regions of the cartogram (derived from the topojson).\n\n&lt;div class=\"container-fluid\"&gt;\n    &lt;div id=\"container\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n\n\n\n\n\n\n8.2. cartogram.js\n\n Without getting into the exact details, line-by-line, the procedure to produce cartograms is as follows: \n\nCalculate Force Reduction Factor\n\nThe \"force reduction factor\" is a number less than 1, used to reduce the impact of cartogram forces in the early iterations of the procedure. The force reduction factor is the reciprocal of one plus the mean of the size error. The size error is calculated by the ratio of area over the desired area (if area is larger) or desired area over area in the other case.\n\nFor each polygon\n  Read and store PolygonValue (negative value illegal)\n  Sum PolygonValue into TotalValue\n\n\nFor each iteration (user controls when done)\n  For each polygon\n      Calculate area and centroid (using current boundaries)\n  Sum areas into TotalArea\n  For each polygon\n      Desired = (TotalArea * (PolygonValuelTotaIValue))\n      Radius = SquareRoot (Area / π)\n      Mass = SquareRoot (Desired / π) - SquareRoot (Area / π)\n      SizeError = Max(Area, Desired) / Min(Area, Desired)\n\n\nMove boundary co-ordinates\n\nThe brute force method (fixed small number of polygons): the forces of all polygons/countries act upon every boundary coordinate. As long as the number of polygons is relatively small (under 500), distortions can be computed for a rather complex line work (thousands of points). Furthermore, the computation of force effects could be restricted by implementing a search limitation to exclude infinitesimal forces from far-away polygons.\n\n  ForceReductionFactor = 1 / (1 + Mean (SizeError))\n  For each boundary line; Read coordinate chain\n      For each coordinate pair\n          For each polygon centroid\n              Find angle, Distance from centroid to coordinate\n                If (Distance &gt; Radius of polygon): Fij = Mass * (Radius / Distance)\n                Else: Fij = Mass * (Distance^2 / Radius^2) * (4 - 3 * (Distance / Radius))\n          Using Fij and angles, calculate vector sum\n          Multiply by ForceReductionFactor\n          Move coordinate accordingly\n      Write distorted line to output and plot result\n\n\n\n\n\n\n8.3. plot.js\n\nCreate a point grid\n\nA point grid is a matrix containing the centers of all the cells in the grid.\n\n  let cellRadius = cellDetails.radius;\n  let cellShape = cellDetails.shape;\n\n  let shapeDistance = getRadius(cellRadius, cellShape);\n  let cols = width / shapeDistance;\n  let rows = height / shapeDistance;\n  let pointGrid = d3.range(rows * cols).map(function (el, i) {\n    return {\n      x: Math.floor(i % cols) * shapeDistance,\n      y: Math.floor(i / cols) * shapeDistance,\n      datapoint: 0,\n    };\n  });\n\n\nThe shapeDistance is different for different cell-shapes. For example:\n\n  switch (cellShape) {\n      case cellPolygon.Hexagon:\n        shapeDistance = radius * 1.5;\n      case cellPolygon.Square:\n        shapeDistance = radius * 2;\n    }\n\n\n \n\nPlot the hexagonal grid playground\n\nThe playground of cells is as shown in Figure 8, where each point in the grid is tesselated with the respective cell shape. The playground also serves as the never-ending sea/ocean on the world map.\n\n  d3.select(\"#container\").selectAll(\"*\").remove();\n    const svg = d3\n      .select(\"#container\")\n      .append(\"svg\")\n      .attr(\"width\", width + margin.left + margin.top)\n      .attr(\"height\", height + margin.top + margin.bottom)\n      .append(\"g\")\n      .attr(\"transform\", `translate(${margin.left} ${margin.top})`);\n\n  svg\n    .append(\"g\")\n    .attr(\"id\", \"hexes\")\n    .selectAll(\".hex\")\n    .data(getGridData(cellShape, newHexbin, pointGrid))\n    .enter()\n    .append(\"path\")\n    .attr(\"class\", \"hex\")\n    .attr(\"transform\", getTransformation(cellShape))\n    .attr(\"d\", getPath(cellShape, newHexbin, shapeDistance))\n    .style(\"fill\", \"#fff\")\n    .style(\"stroke\", \"#e0e0e0\")\n    .style(\"stroke-width\", strokeWidth)\n    .on(\"click\", mclickBase);\n\n\n\n\n\n\n8.4. shaper.js\n\nThe shaper.js has all the code snippets that depend on the cells shape. \n\nOnce again, the transformation, SVG path, and binned data points (grid) are dependent on the cell-shape.\nFor hexagons, the library used: d3-hexbin\n\n  function getGridData(cellShape, bin, grid) {\n    switch (cellShape) {\n      case cellPolygon.Hexagon:\n        return bin(grid);\n      case cellPolygon.Square:\n        return grid;\n    }\n  }\n\n\nTranslate is one of the support transformations (Translate, Rotate, Scale, and Skew). It moves the SVG elements inside the webpage and takes two values, x and y. The x value translates the SVG element along the x-axis, while y translates the SVG element along the y-axis. \nFor example: A single point in a point-grid represents the top-right corner of a square, which is moved by length of the side/2 on the x and y-axis using transform.translate(x, y)\n\n  function getTransformation(cellShape) {\n    switch (cellShape) {\n      case cellPolygon.Hexagon:\n        return function (d) {\n          return \"translate(\" + d.x + \", \" + d.y + \")\";\n        };\n      case cellPolygon.Square:\n        return function (d) {\n          return \"translate(\" + d.x / 2 + \", \" + d.y / 2 + \")\";\n        };\n    }\n  }\n\n\nTo emphasize the ease of extending the solution for other cell shapes, notice the rightRoundedRect that takes borderRadius (zero for a square/rectangle); however, setting it to 50% would result in circular cells.\n\n  function getPath(cellShape, bin, distance) {\n    switch (cellShape) {\n      case cellPolygon.Hexagon:\n        return bin.hexagon();\n      case cellPolygon.Square:\n        return function (d) {\n          return rightRoundedRect(d.x / 2, d.y / 2, distance, distance, 0);\n        };\n    }\n  }\n\n\n \n\nCreate the base cartogram\n\nThe expectation of Cartogram() is to take the current topo-features of the map projection along with the source population count and target population count to return new topo-features (arcs for every polygon/country).\n\nIn this example, the base cartogram is a population-scaled world map for the year 2018.\n\n  var topoCartogram = cartogram()\n    .projection(null)\n    .properties(function (d) {\n      return d.properties;\n    })\n    .value(function (d) {\n      var currentValue = d.properties.count;\n      return +currentValue;\n    });\n  topoCartogram.features(topo, topo.objects.tiles.geometries);\n  topoCartogram.value(function (d) {\n    var currentValue = populationJson[d.properties.id][year];\n    return +currentValue;\n  });\n\n\nAs for the presentation, there are two types: Fixed and Fluid, as shown in Figures 13 and 14.\n\nPopulation Factor: The populationFactor is \"1\" in FLUID mode and depends on the source and target population ratio in FIXED mode, calculated using back-propagation, where the default populationFactor is 1.6 (mean of expected values across years) and increased/decreased in steps of 0.1 to reach the desired cell-size.\n\n  var topoFeatures = topoCartogram(\n    topo,\n    topo.objects.tiles.geometries,\n    cellDetails,\n    populationData, year,\n    populationFactor\n  ).features;\n\n\n  populationFactor(selectedScale, populationData, year) {\n    switch (selectedScale) {\n      case cellScale.Fixed:\n        var factor =\n          getTotalPopulation(populationData, 2018) /\n          getTotalPopulation(populationData, year) /\n          1.6;\n        return factor;\n      case cellScale.Fluid:\n        return 1;\n    }\n  }\n\n\n \n\nFlatten the features of the cartogram/topojson.\n\nA quick transformation to form a list of polygons irrespective of whether the feature is a MultiPolygon or a MultiPolygon.\n\nfunction flattenFeatures(topoFeatures) {\n  let features = [];\n  for (let i = 0; i &lt; topoFeatures.length; i++) {\n    var tempFeatures = [];\n    if (topoFeatures[i].geometry.type == \"MultiPolygon\") {\n      for (let j = 0; j &lt; topoFeatures[i].geometry.coordinates.length; j++) {\n        tempFeatures[j] = topoFeatures[i].geometry.coordinates[j][0];\n      }\n    } else if (topoFeatures[i].geometry.type == \"Polygon\") {\n      tempFeatures[0] = topoFeatures[i].geometry.coordinates[0];\n    }\n    features[i] = {\n      coordinates: tempFeatures,\n      properties: topoFeatures[i].properties,\n    };\n  }\n  return features;\n}\n\n\n \n\nFill the polygons/regions of the base cartogram with hexagons (tessellation)\n\nThis is the step where the polygons are tesselated, and the d3.polygonContains function checks for points in the point-grid within the polygon as shown in figures 9 and 10. \n\n  let features = flattenFeatures(topoFeatures);\n  let cellCount = 0;\n  for (let i = 0; i &lt; features.length; i++) {\n    for (let j = 0; j &lt; features[i].coordinates.length; j++) {\n      var polygonPoints = features[i].coordinates[j];\n\n      let tessellatedPoints = pointGrid.reduce(function (arr, el) {\n        if (d3.polygonContains(polygonPoints, [el.x, el.y])) arr.push(el);\n        return arr;\n      }, []);\n      cellCount = cellCount + tessellatedPoints.length;\n\n      svg\n        .append(\"g\")\n        .attr(\"id\", \"hexes\")\n        .selectAll(\".hex\")\n        .data(getGridData(cellShape, newHexbin, tessellatedPoints))\n        .append(\"path\")\n        .attr(\"class\", \"hex\" + features[i].properties.id)\n        .attr(\"transform\", getTransformation(cellShape))\n        .attr(\"x\", function (d) {\n          return d.x;\n        })\n        .attr(\"y\", function (d) {\n          return d.y;\n        })\n        .attr(\"d\", getPath(cellShape, newHexbin, shapeDistance))\n        ... // same as above\n        .style(\"stroke-width\", strokeWidth);\n    }\n  }\n\n\n\n\n\n\n8.5. events.js\n\nDrag and drop hexagons in the hex-grid\n\nImplementation of start, drag, and end - representing the states when the drag has started, in-flight, and dropped to a cell-slot.\n\n  function dragstarted(event, d) {\n    d.fixed = false;\n    d3.select(this).raise().style(\"stroke-width\", 1).style(\"stroke\", \"#000\");\n  }\n\n  function dragged(event, d) {\n    let cellShape = document.querySelector(\"#cell-shape-option\").value;\n    let hexRadius = document.querySelector(\"input#radius\").value;\n    var x = event.x;\n    var y = event.y;\n    var grids = getNearestSlot(x, y, hexRadius, cellShape);\n    d3.select(this)\n      .attr(\"x\", (d.x = grids[0]))\n      .attr(\"y\", (d.y = grids[1]))\n      .attr(\"transform\", getTransformation(cellShape));\n  }\n\n  function dragended(event, d) {\n    d.fixed = true;\n    d3.select(this).style(\"stroke-width\", strokeWidth).style(\"stroke\", \"#000\");\n  }\n\n\nFinding the nearest cell-slot: It's vital to ensure that a cell can only be dragged to another cell-slot. From the x and y co-ordinate, calculate the nearest available slot. For example, a square of length 5 units at x co-ordinate of 102, 102 - (102 % 5) = 100 would be the position of the nearest slot on the x-axis, similarly on the y-axis. On the other hand, hexagons are a bit tricky, where the lengths of the hexagon are radius * 2 and apothem * 2. Recommended read on hexagons and hex-grid: https://www.redblobgames.com/grids/hexagons\n\n  function getNearestSlot(x, y, n, cellShape) {\n    switch (cellShape) {\n      case cellPolygon.Hexagon:\n        var gridx;\n        var gridy;\n        var factor = Math.sqrt(3) / 2;\n        var d = n * 2;\n        var sx = d * factor;\n        var sy = n * 3;\n        if (y % sy &lt; n) {\n          gridy = y - (y % sy);\n          gridx = x - (x % sx);\n        } else {\n          gridy = y + (d - (n * factor) / 2) - (y % sy);\n          gridx = x + n * factor - (x % sx);\n        }\n        return [gridx, gridy];\n      case cellPolygon.Square:\n        var gridx;\n        var gridy;\n        var sx = n * 2;\n        var sy = n * 2;\n        gridy = y - (y % sy);\n        gridx = x - (x % sx);\n        return [gridx, gridy];\n    }\n  }\n\n\nMouse over and out in the hex-grid\n\nSimilarly, a few other events include mouse over, mouse out, and mouse click.\n\n  svg.append('g')\n  ... // same as above\n  .on(\"mouseover\", mover)\n  .on(\"mouseout\", mout)\n  .call(d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended));\n\n\n  function mover(d) {\n    d3.selectAll(\".\" + this.getAttribute(\"class\"))\n      .transition()\n      .duration(10)\n      .style(\"fill-opacity\", 0.9);\n  }\n\n  function mout(d) {\n    d3.selectAll(\".\" + this.getAttribute(\"class\"))\n      .transition()\n      .duration(10)\n      .style(\"fill-opacity\", 1);\n  }\n\n\n\n\n\n\n\n\nConclusion\n\n 9. Pending items\n\nA complete implementation of the above (with additional features):\n\n\nPrototype: https://owid.github.io/cartograms\n\nGithub Repository: https://github.com/owid/cartograms\n\n\nHowever, this does not conclude meeting the expected requirement(s). The last pending piece is to generate a new cartogram/topojson after moving the cells. That's a work in progress; stay tuned! Subscribe maybe?\n\n\n\n\n\n 10. References\n[1] “Amit’s Thoughts on Grids,” www-cs-students.stanford.edu. http://www-cs-students.stanford.edu/~amitp/game-programming/grids\n\n[2] M. Strimas-Mackey, “Fishnets and Honeycomb: Square vs. Hexagonal Spatial Grids,” Matt Strimas-Mackey, Jan. 14, 2016. https://strimas.com/post/hexagonal-grids\n\n[3] S. Kamani, “D3 Geo Projections Explained” www.sohamkamani.com. https://www.sohamkamani.com/blog/javascript/2019-02-18-d3-geo-projections-explained (accessed Jun. 14, 2022).\n\n[4] “Markdown to HTML Converter - Markdown Editor - Online - Browserling Web Developer Tools,” www.browserling.com. https://www.browserling.com/tools/markdown-to-html (accessed Jul. 10, 2022)."

    },
  
  
  
  
    {

      "title"    : "Why You Should Reinvent the Wheel",
      "url"      : "/reinvent-the-wheel",
      "index"    : "banana",
      "content"  : "Reinvent the Wheel! \n\nAs I get slightly older and slightly wiser - to me, it’s more and more apparent that we humans live a life full of contradiction, are rarely self-satisfied, and thrive on learning and discovering new things. It’s as if a duty to contribute with all the might to a better future, not for oneself, but for the future, that’s way beyond imagination and self-existence.\n\nOn these lines, it almost always gets to my core when someone overuses “Don’t reinvent the wheel”—implying that a solution to the problem already exists. In most cases, to underplay the problem or assert an opinion before understanding the crux of the problem.\n\nStarting with the obvious, the wheel would still look like a big chunk of stone that rolls if we didn’t reinvent the wheel. While some may say that it’s an upgrade rather than reinvention, going by the definition “change (something) so much that it appears to be entirely new,” reinvention is more than fair in this context.\n\n\nEvolution of Wheel! \n\nThe primitive wheel did the job thousands of years ago but completely unprepared for today’s world. If not for the natural innovators around the world who insisted and invested in reinventing the wheel from time to time.\n\nOnce again, it’s not just about reinventing anything and everything—using the phrase so much that it has become so common that we often don’t challenge the existing solutions is something to ponder on. But this isn’t always true; products that are deeply engraved in our daily lives are eagerly awaited. For instance, the communication medium, specifically mobile devices, has drastically changed (reinvented) in a short span of 2 decades.\n\nWhile I agree that all of these are subjective and among the common examples that have conflicting reactions. What if the repercussions are profound and have a severe impact on the future of the world. At first thought, the education system has been about the same for the past 100 years or earlier; despite the efforts of amazing educators to adapt to the modern world, the changes aren’t drastic.\n\nEnglish, Physics, Chemistry, and Mathematics - no doubt these are essential, but it seems unfair to ask students to pause their passions while they put all of their time and work into these “core” subjects. \nImagine a world where students go to school and teachers help them explore their passion and are more concerned about equipping students with the skills and opportunities rather than teaching the same generic content over and over again. No doubt the education system is stuck in a timeline, and it’s about time to reinvent the wheel of education suited to the road of 2022 and beyond.\n\n\n**End of philosophical thinking**\n\n\n\nI’m out of count - the number of times “don’t reinvent the wheel” is overused in software engineering.\n\nAs a norm, avoiding reinventing the wheel goes unsaid. If the functionality exists in the standard built-in library of a language.\nHowever, if it comes down to using third-party libraries, it better be a hard judgment call, considering how widely the library is used and maintained.\n\nEven if the library has an active community and is commonly used, it’s still a third-party dependency created for a larger use case. Software engineers emphasizing the virtues of code reuse while glossing over the danger of dependencies are nothing but trouble. A project with too many third-party dependencies will likely fall apart in the long run and devolves into a maintenance and refactoring nightmare.\n\nMoreover, the existing wheel(s) either does way more than what’s necessary and suffers from the inner platform effect and are unnecessarily complex, or they are missing some key feature and would be difficult to implement on top of what’s already there.\n\nFurthermore, using existing wheels often adds constraints to the project:\n\n  The existing wheel requires a different language and/or programming style.\n  The existing wheel only works with the legacy language version (Example: Java 8 instead of Java 11).\n  Efficiency, flexibility, and simplicity: the existing wheel makes suboptimal choices.\n  The existing wheel has tons of legacy cruft that make life difficult. For example, A Java library forcing to use container classes because it was written before generics, etc.).\n  The way the existing wheel models the problem is entirely different than what’s convenient for the use case.\n  The library adds a massive, brittle dependency that would be a major hassle when all that’s needed is a small subset of its features.\n\n\nSo, leveraging existing code is good - but dependencies are bad. While the two may seem to contradict each other, the key here is to find the right balance:\n\n  Preference for standard built-in libraries,\n  Choose acceptable libraries that have an active community and user base\n  Choose an external library where at least half the features the library has to offer are useful to the project.\n  Lastly, a relatively unknown library only if you are familiar with the source code to maintain it yourself.\n\n\nIt’s all a balancing act. But the point is that just blindly saying, “Code reuse good! Reinventing wheel bad” is dangerous and dogmatic. The benefits of leveraging third-party code must be weighed against the disadvantages of introducing dependencies.\n\nReferences\n\n[1] K. Smithers, “Why we Need to Reinvent the Wheel,” www.ted.com, 1549. https://www.ted.com/talks/katie_smithers_why_we_need_to_reinvent_the_wheel (accessed Jun. 23, 2022).\n\n[2] “Is reinventing the wheel really all that bad?,” Software Engineering Stack Exchange. https://softwareengineering.stackexchange.com/questions/29513/is-reinventing-the-wheel-really-all-that-bad (accessed Jun. 24, 2022)."

    },
  
  
  
  
    {

      "title"    : "Distributed Model Training",
      "url"      : "/distributed-model-training",
      "index"    : "watermelon",
      "content"  : "Porco Rosso. \n\nDistributed Training\nDeep learning is a subset of machine learning, a branch of artificial intelligence to perform tasks through experience. Deep learning algorithms are well suited and perform the best with large datasets, not to mention the need for high computation power. With the pay-per-use serverless service model, such as the google collab, training large neural networks on the cloud is easier than ever.\nWhile it’s possible to train huge models in a single multi-core GPU machine, it could take days and even weeks. Hence, this leads to the fundamental problem of reducing the training time.\n\nTypically, any scaling problem is broadly addressed by scaling-up or scaling-out, i.e., horizontal and vertical scaling. Depending on the use case, vertical scaling has the limitation of maxing out at a point and often tends to be a lot more expensive in the long run, both in price and technical backlog.\n\nOne-liner: Distributed training distributes training workloads across multiple computation processors. Where a cluster of worker nodes works in parallel to accelerate the training process, parallelism is achieved by data parallelism or model parallelism.\n\n\n\nTypes of Distributed Training\nData Parallelism\nAs the name suggests, the dataset is horizontally/vertically sharded and processed parallelly. Each worker node in the cluster trains a copy of the model on a different batch of training data, communicating the computation results to keep the model parameters and gradients in sync across all nodes. The computation results can be shared synchronously, i.e., at the end of each batch computation or asynchronously.\n\n\nFigure 1: Data-Parallel training. \n\nOne-liner: The entire model is deployed to multiple nodes of the cluster, and each node represents the horizontal/vertical split of the sharded dataset and the model.\n\nModel Parallelism\nOn the contrary, in model parallelism, the model itself is divided into parts/layers in situations where the model size is too large for a single worker; hence a set of layers are trained simultaneously across different worker nodes. The entire dataset is copied/available to all worker nodes, and they only share the global model parameters with other workers—typically just before forward or backward propagation. Furthermore, the layers can be partitioned vertically or horizontally.\n\n\nFigure 2: Model-Parallel training. \n\nOne-liner: A layer or a group of layers of the model is deployed to multiple nodes of the cluster, and the entire dataset is copied to every node.\n\n\nFigure 3: Model-Partitioning horizontally or\nvertically. \n\nAmong the two, data parallelism is commonly used and easier to implement. The ability to train a model in batches of data (non-sequential) and contribute to the overall performance of the model is the crux of the solution. In other words, the model parameters and gradients are calculated for every small batch of data in the worker node, and at the end of it → updated weights are sent back to the initiating node → the weighted average/mean of the weights from each worker node is applied to the model parameters → updated model parameters are sent back all worker nodes for the next iteration; this leads to questions about how and when model parameters are stored and updated.\n\n\n\nDistributed Training Loops\nThe two ways of carrying out distributed training loops are as follows:\n\nSynchronous training\nOnce again, taking the example of data parallelism, where we divide the data into partitions/batches for each worker node to process. Every worker node has a full replica of the model and the batch of data.\n\n\n  The forward pass starts at the same time for all workers, and each worker node computes the gradients (Output).\n  Workers wait until all the other workers have completed the training loop. Then, once all the workers have computed the gradients, they start communicating with each other to aggregate the gradients.\n  After all the gradients are combined, a copy of the updated gradients is sent to all the workers.\n  Then, each worker continues with the backward pass and updates the local copy of the weights.\n  Until all the workers have updated their weights, the next forward pass does not start; hence the name “synchronous”.\n\n\nNote: All the workers produce different gradients as they are trained on different subsets of data, and eventually, all workers have the same weight.\n\nReduce Algorithm\nTypically, a single node is used to complete aggregation. For instance, in the case shown in Figure 3, the bandwidth for Machine A increases as the number of machines/parameters increases.\n\n\nFigure 4: Single node aggregator.\n\nFollowing up on the reduce-algorithm mentioned in synchronous training, the idea behind the all-reduce algorithm is to share the load of storing and maintaining the global parameters to overcome the limitation of using the parameter server method. There are serval all-reduce algorithms that dictate how parameters are calculated and shared:\n\n\nFigure 5: All Reduce: Aggregation task distributed to all nodes instead of a single node.\n\nLike AllReduce, each node performs the aggregation task on a subset of parameters: machine A – parameter 1, machine B – parameter 2, etc. Instead of sending its version of parameters to all other nodes, each worker node sends its version to the next one.\n\n\nFigure 6: Ring All Reduce.\n\nSimilarly, in tree-all-reduce, parameters are shared via a tree structure. Irrespective of the topology, all-reduce algorithms reduce synchronization overhead and make it easier to scale horizontally.\n\n\nFigure 7: Tree All Reduce.\n\nEach worker node holds a subset of data and computes the gradient(s); those values are passed up the tree and aggregated until a global aggregate value is calculated in the root node. Then, the global value is passed down to all other nodes.\n\nAsynchronous training\nThe evident problem with the synchronous approach is the lack of efficient resource usage since a worker must wait for all the other workers in the cluster to move forward in the pipeline. Furthermore, the problem amplifies when the computation time for workers is significantly different, which could be because of dataset or computation power variations - because of which the whole process is only as fast as the slowest worker in the cluster. Hence in asynchronous training, the workers work independently in such a way that a worker need not wait for any other worker in the cluster. One way to achieve this is by using a parameter server.\n\n\n\nCommunication Approaches\nThe two communication approaches, centralized and de-centralized patterns, apply to both data-parallel and model-parallel training. The key here is the communication between the worker nodes, how the parameters are initiated, and how the weights/biases are updated.\n\nCentralized Training\nIn distributed training, the cluster of workers performs just one task: training. However, in the centralized communication pattern, we assign a different role to each worker, where some workers act as parameter servers and the rest as training workers.\n\nThe parameter servers are responsible for holding the parameters of the model and are responsible for updating the global state of our model. At the same time, the training workers run the actual training loop and produce the gradients from the batch of data assigned to them.\n\n\nFigure 8: Centralized training. \n\nHence the entire process for Centralized data-parallel training is as follows:\n\n  Replicate the model across the training worker nodes; each worker node uses a subset of the data.\n  Each training worker fetches the parameters from the parameter server(s).\n  Each training worker node performs a training loop and sends back the gradients to all parameter servers.\n  Parameter servers update the model parameters and ensures all the worker models are in sync.\n\n\nSome known disadvantages are:\n\n  At a given point in time, only one of the workers may be using the updated version of the model, while the rest use a stale version.\n  Using only one worker as a parameter server can become a bottleneck and lead to a single point of failure.\n\n\nDe-centralized Training\nOn the flip side, In a de-centralized communication pattern, each worker node communicates with every other node to update the model parameters. The advantage of this approach is that peer-peer updates are faster, and there is no single point of failure.\n\n\nFigure 9: De-centralized training. \n\n\n\nConclusion\nDeep learning models become more ambitious by the day, and their supporting infrastructures struggle to keep up. Employing distributed model training techniques is only a matter of time to solve the problem of training a complex machine learning model on huge datasets. Moreover, the advantages supersede the development time/bandwidth with better Fault tolerance and reliability, higher Efficiency,  horizontally scalable to handle massive scale, and cost-effective in the long run.\n\n\n\nReferences\n\n[1] “Distributed Training: Guide for Data Scientists,” neptune.ai, Jan. 19, 2022. https://neptune.ai/blog/distributed-training (accessed Jun. 23, 2022).\n\n[2] “Distributed Training,” www.run.ai. https://www.run.ai/guides/gpu-deep-learning/distributed-training (accessed Jun. 24, 2022).\n\n[3] “Distributed Training for Machine Learning – Amazon Web Services,” Amazon Web Services, Inc. https://aws.amazon.com/sagemaker/distributed-training/ (accessed Jun. 26, 2022).\n\n[4] “Distributed model training II: Parameter Server and AllReduce – Ju Yang.” http://www.juyang.co/distributed-model-training-ii-parameter-server-and-allreduce/ (accessed Jun. 26, 2022)."

    },
  
  
  
  
    {

      "title"    : "Refactoring Nightmares",
      "url"      : "/refactoring-nightmares",
      "index"    : "peach",
      "content"  : "Get Well Soon. \n\nYou are working on a fast-growing, never-stopping product; 35 sprints and 600 coffees later, you finally got to move on, and a new engineer steps in, or you realize sooner or later that you cannot keep up with the feature requests while ensuring code quality. Then, after raising the need for refactoring as blockers for a couple of features, finally! It’s the day when your engineering team decides to dedicate bandwidth for refactoring - Sounds familiar? You are not alone.\n\n\n\nHow to prevent the need for a major refactor\n\nDon’t Rebuild; Build the Right Things.\n(1) Striking the right balance between business needs and development time is beyond important, especially in an early-stage start-up, quantifying the need to follow best practices to write clean code with meaningful test cases can be harder than it looks. One of the go-to approaches is to retrospect the hotfixes/number of bugs/blockers to the original story. Have a process around merging code to pre-production/production:\n\n\n  Set a code coverage threshold,\n  automatically tag pull/merge requests without review comments, or the classic “LGTM” (Looks Good To Me),\n  link hotfixes to production to the original story/PR,\n  “code owners” for certain critical classes/modules, not everyone has access to merge changes to any file/module (important for a monolith codebase).\n\n\n(2) Almost never start working on a feature directly on your IDE,\n\n  gather the requirements,\n  start a rough solutioning document,\n  list down the different components,\n  direct how each of these components talks to each other,\n  think of the extent of abstraction and probable future use cases,\n  decide on the contracts and re-iterate\n\n\nFinally, it’s time to get on to that IDE and start off with the low-level design: interfaces, entities, DTOs, request/response, etc, followed by the implementation backed with test cases (Test Driven Development).\n\nIf it’s a Stable Product, Don’t Touch it\nDo the very opposite! Just because the application is stable and doesn’t have more features coming up anytime soon does not necessarily mean it goes untouched. Of course, it may not be the highest priority, but make sure to maintain the project and keep it up to date before it bites you back at a later stage:\n\n\n  Upgrade dependencies, at least the next major version, if not minor ones.\n  Keep it on par with the rest of the codebase/services, be it the architecture, conventions, or contracts.\n  Stable services are often forgotten or taken for granted; write detailed documentation.\n\n\nAgain, from the business point of view, making any changes to a stable product has no ROI, at least in the short time, but it’s services such as these that expose security vulnerabilities and have scalability issues in the longer run.\n\nRefactoring is NOT a One-time Gig\nDespite extensive solutioning and following all the best practices, the product requirements, customer needs, and business goals aren’t as predictable as we think and are likely to change drastically over time. Fortunately, the changes aren’t overnight either;\n\n\n\n\n  The best way to avoid heavy refactoring is to do your refactoring a lot more often.\n  Revalidate the existing low-level design when you touch older sections of the codebase.\n  Temporary hacks aren’t too bad, but they eventually become the norm; these hacks should top the list of items to take up next.\n  Don’t worry about “Ohh! That’s too much code for a small use-case”. For example, consider a finite state-machine of 5 states of 4 transitions, resulting in a state-machine orchestrator/engine class, four transitions classes, four listener classes (pub-sub), and classes/functions for API resource(s), transformations, DTOs, etc. For all you know, the same could have been written with a bunch of if-else conditions on states.\n  Don’t be shy to use design patterns; don’t go overboard with it either. The best site for design patterns https://refactoring.guru/design-patterns\n\n\nAlso, if you have a story for refactoring under technical debt, it’s a clear indicator that something isn’t right.\n\nFind ways to correlate the importance of refactoring to ROI.\nAlrighty, this is the last one. As mentioned earlier, it’s not always easy to emphasize the importance of regular refactoring, which may seem like it’s slowing you down.\n\n“Think of it this way: You’re a code politician. Your stakeholders are your citizens, and they will expect you to not only make new laws but also to repeal laws that no one agrees with anymore. Refactoring is just as necessary as feature development, and developers tend to focus too much on adding new things. We can spend our time filling our rooms with new toys, clothes, and furniture, but we also need to clean our rooms and throw out old items from time to time.”\n\nHead back to “Don’t Rebuild; Build the Right Things” and come up with metrics to better justify the need for refactoring.\n\n\n\nHow to refactor\n\nPause new development while refactoring\nImagine refactoring, and the other developers are also making changes or even refactoring the same class you are - leading to a merge-conflicts nightmare. So, communicate beforehand about the changes you plan on doing, and make sure to refactor in stages so that it doesn’t end up being a blocker for too long.\n\n\n\nDefine the scope of your refactoring\nRefactoring is not different from working on a feature; start with\n\n  gathering context about the existing piece of code you are about to refactor,\n  scope the refactoring plan and decide on the classes/functions you’d refactor,\n  as always, don’t start coding right away; start with the low-level design and come up with a rough sketch of what the results look like - more importantly, take solutioning-time into consideration while estimating the approximate time,\n  finally, prioritize and break down the refactoring task into smaller deployable chucks instead of raising that one mega-sized pull request expecting your peer to review it.\n\n\nTest cases, Test cases, Test cases\nTo start with, one shouldn’t even refactor if there are minimal or no prior test cases. Always remember, if anything can go wrong, it will (murphy’s law). I mean it! Maybe refactoring isn’t the right choice at this moment. Instead, improve the code coverage, document the expected behavior, and visit back to refactor. Without a quantifiable metric to measure the correctness of the refactor, the repercussions can be brutal. If there are test cases with good code coverage already, follow TDD, add new test cases, and fix the older ones.\n\n\n\nAll said, how deep is the mess you’re in? Coming out of ugly legacy code is hard, and not many signup for it. The same old “prevention is better than cure” is the way to avoid ending up with the Refactoring Nightmare."

    },
  
  
  
  
    {

      "title"    : "Why Every Engineer Should Contribute More to Open Source",
      "url"      : "/free-and-open-source",
      "index"    : "redapple",
      "content"  : "FOSS Developers 💪🏻 \n\nFree and Open Source Software\n“Free and open-source software (FOSS) is software that is both free software and open-source software where anyone is freely licensed to use, copy, study, and change the software in any way, and the source code is openly shared so that people are encouraged to voluntarily improve the design of the software” - Wikipedia\n\nMost of the applications, softwares, and tools we use day-to-day are built by thousands and millions of people all over the globe. So to the extent that Free and Open Source Softwares define who we are as Engineers today. Be it a full-fledged product or technologies used to build one - languages, frameworks, databases, queues/buses, caching mechanisms, deployment tools, visualization dashboards, the list goes on; revolves around the OSS community. It’s imperative to keep this community active and ever-growing.\n\n\n\nHow to Contribute\nBroadly, one can contribute in two ways (ideally both): (1) Sponsor FOSS project(s), developer(s), organization(s). It can be &lt;$5 to start with and consider being a monthly sponsorer. (2) Do what you do best. Build features, fix bugs, enhance the design, improve the documentation, test the software to find bugs and vulnerabilities.\n\n\n\nKey Motivators\n\nImprove Skills and Gain Experience\nThe open-source community is inclusive; it’s not limited to a region, nor does it matter who you are or where you are from. Early on in the career as a Software Engineer, working on open source projects is probably the best way to work on an actual project and improve programming skills. But again, the complexities of every project differ; there’s always room for anyone to learn more and contribute towards enhancing a OSS.\n\nDepending on the project, the code quality of most OSS tends to have a high standard; after all, it’s public and used by many, ranging from hundreds to millions. Lastly, contributing to OSS gives the experience of understanding the existing code base, fixing bugs, implementing a feature, debugging issues, and deploying the application, which is a game-changer for a new graduate.\n\nMeet Top Talent/Find a Mentor\nThe maintainers and active contributors of good OSS projects are among the best developers and do what they do out of passion; this is an opportunity to “learn from the best.”\n\nBetter Job Prospects\nA lot of the companies out there are on a constant lookout to hire engineers, and there’s no better than the OSS community. Toptal helps companies connect with open source developers. Moreover, it’s almost a norm for companies to ask for GitHub username in the job application, and contributions to open source projects stand out because it’s easier to assess the skills of a developer directly based on the code written.\n\nSide Hustle\nA lot of creators out there have sponsors as their primary source of income. While that’s not remotely easy to achieve, this could be your chance to have a side hustle or even consider starting a project of your own; it doesn’t have to be a business plan and can be a simple library.\n\n\n\nMoreover, in a way, it’s the right thing to do to give back to the community. About 3 years into my career as a Backend Engineer, an advice I would give to myself is to be a lot more involved with and contribute to open-source projects. I’m glad to be selected as a contributor for Google Summer of Code 2022 - working with Our World in Data is without a doubt a stepping stone towards the right direction :)"

    },
  
  
  
  
    {

      "title"    : "Anatomy of a System Design Interview: The example",
      "url"      : "/tackle-system-design",
      "index"    : "avocado",
      "content"  : "Complex is Simple. \n\nWhat’s better than going over a real system design interview question (Interviewed in June 2022).\n\n1. Requirements\nReview the end-to-end design below. You need to design Your System. Your system is responsible for\nlinking a customer patient to an internal system, maintaining that mapping, and letting the customer\nsystem know whether your system was able to establish the link.\n\n\n  Communication is Asynchronous.\n  Orders must be acknowledged as soon as possible.\n  System must be fault-tolerant.\n  System must be reliable, available and scalable.\n  System needs to persist the patient’s ID from both the customer and the internal system.\n\n\n\nExpected Design\n\n\n\n2. Overview\n2.1. Architecture Diagram\n\n\nArchitecture Diagram \n\nLink: https://drive.google.com/file/d/1xCP9EpqqrDzCT2jd47ke6WlX69TD4NSk/view?usp=sharing\n\n\n\n2.2. Components\nThe overview of the different components of the merchant service is as follows:\n\n  \n    Merchant Service: A proxy-link service (like a merchant who buys and sells) links the customer’s patient-id to internal system’s patient-id.\n  \n  \n    LB (Load Balancer): Receives public traffic and distributes the load across the merchant-service producer cluster (compute instances).\n  \n  \n    P Cluster: The merchant-service application producer cluster that scales horizontally (Example: EC2 cluster with auto-scaling and target group attached to the load balancer), receives the requests from the load-balancer and push messages to the queue.\n  \n  \n    Queue: A messaging broker to handle high traffic to process long-running/asynchronous tasks in streams/batches.\n  \n  \n    C Cluster: The merchant-service application consumer cluster processes the messages in the queue and returns the appropriate response.\n  \n  \n    Cron Scheduler: Schedule cron jobs to reconcile the workflows stuck in pending state(s).\n  \n  \n    Relational DB: The database primarily has two tables: “workflows” to track and manage the state of the incoming requests until completion and patient-mapping to store the mapping between customer and internal patient-id.\n  \n  \n    Key-value Store: For storing the incoming request data (customer and patient details) and idempotency-key of requests.\n  \n\n\n\n\n2.3. Datastores\n2.3.1 Relational Database (SQL)\n\n  \n    Workflow: A workflows table to keep track of the request received from the customer and manage the state until the state transitions to an end-state.\n\n    \n      \n        Table workflows:\n\n        \n          \n            \n              workflow-id\n              request-id\n              customer-id\n              patient-id\n              status\n              request-type\n            \n          \n        \n      \n      \n        Where, status: RECEIVED, IN_PROGRESS, LINKED, NO_RESULTS, COMPLETED, FAILED and request-type: LINK\n      \n    \n  \n  \n    Linkage: A patient-mapping table links the customer patient-id to the internal patient-id.\n\n    \n      \n        Table patient-mapping:\n\n        \n          \n            \n              customer-id\n              customer-patient-id\n              internal-patient-id\n              status\n              workflow-id\n            \n          \n        \n      \n      \n        Where, status: LINKED, DE-LINKED\n      \n    \n  \n\n\n2.3.2. Key-value Store (NoSQL)\n\n  \n    Idempotency: Key: (idempotency-key + customer-id), Value: idempotency key from client/customer.\n  \n  \n    Request Data: Key: (Unique request-id), Value: Request data (customer and patient details).\n  \n\n\n\n\n3. Workflow\n\n3.1. Request from Customer System\nAPI call from the Customer System to the load-balancer of Merchant Service.\n\nMethod: POST, Path: /link\nHeader:\n\"idempotency-key\": \"String\"\n\"customer-token\": \"String\"\n\n\nRequest Body:\n{\n\t\"patient\": {\n\t\t\"id\": \"UUID\",\n\t\t\"full_name\": \"String\",\n\t\t\"dob\": \"DateTime\",\n\t\t...\n\t\t\"contact\": {\n\t\t\t\"address\": \"String\",\n\t\t\t\"phone\": \"String\",\n\t\t\t\"email\": \"String\"\n\t\t}\n\t\t\"insurance\": {\n\t\t\t...\n\t\t}\n\t},\n\t\"webhook\": \"url\"\n}\n\n\nTo perform an idempotent request, the customer/client must add an idempotency-key header to the request. Idempotency works by saving the resulting status code, response body and body of the first request made for the given idempotency key; subsequent requests with the same key return the same result (or until the state changes) until the pre-defined TTL (typically, 24 hours).\n\nThe customer-token is for access control and decrypted on the backend to retrieve the customer details (Example: unique customer-id). Similar to the JSON Web Token model.\n\n\n\n3.2. Acknowledgement: Fail first and Fail Fast\n\n  \n    Validations: Regex matching on expected input pattern, idempotency check, already linked or in progress, etc.\n  \n  \n    Persistence:\n\n    \n      \n        Request Data: Store the request data in the key-value store (key as unique request-id and value as request data - Customer and Patient details).\n      \n      \n        Workflow details: in the “workflows” table: workflow-id, request-id, customer-id, patient-id, status (RECEIVED), request-type (LINK), and timestamp.\n      \n    \n  \n  \n    Queue: Push the message to the queue (section 3.3) - silent failure with retries\n  \n  Response:\n    \n      Return error response/exception(s) before or during persistence - if any (fail fast),\n      Else\n        \n          Store the idempotency-key with expected response\n          Return the response with a status code 202 (Request accepted) and a workflow-id. The client/customer can check the request’s status using the workflow-id. \nThe response body:\n            {\n\"workflow-id\": \"String\"\n}\n            \n          \n        \n      \n    \n  \n  Status API:\n    \n      The merchant service guarantees a valid response once a 202 is returned with a workflow-id. The customer/client can check the status of the request from the workflow-id or get a callback to the webhook.\n      Method: GET , Path: /link/status/&lt;workflow-id&gt;\nResponse:\n        {\n  \"status\": \"workflows.status\"\n}\n        \n      \n    \n  \n\n\nNote:\n\n  \n    The request data can be archived if necessary after linking customer-internal patient-id.\n  \n  \n    It is not recommended to poll the status API; caching may be required if done.\n  \n\n\n\n\n3.3. Process the request\nThe merchant service producer pushes the message to the queue in the forward flow (section 3.2) or reconciliation (section 4). The message body is as follows:\n\n{\n\t\"workflow-id\": \"String\",\n\t\"request-id\": \"String\",\n\t\"customer-id\": \"String\",\n\t\"patient-id\": \"String\",\n\t\"request-type\": \"LINK\",\n}\n\n\nThe merchant service consumer picks the message(s) from the queue:\n\n\n  \n    Workflow Details: Get details from the workflows table by workflow-id and update the workflow status to IN_PROGRESS.\n  \n  \n    Patient and Customer Details: Get the customer and patient details from request-id from the key-value store.\n  \n  \n    Search: Make an API call to the internal service to search for the patient:\n\n    \n      \n        Empty Search result: Mark the status NO_RESULTS in the workflow table.\n      \n      \n        Non-empty Search result: Create an entry in the patient-mapping table (customer-id, customer-patient-id, internal-patient-id, status, workflow-id), mark the status LINKED.\n      \n    \n  \n  \n    Response: Update response in NoSQL store for idempotency-key, return the success/error response (webhook), and mark the status COMPLETED in the workflows table.\n  \n\n\nNote:\n\n  \n    Failures are marked as FAILED in the workflows table to retry/debug (Wildcard to stop a workflow).\n  \n  \n    Use a finite state machine with predefined from-to states and transitions.\n  \n  \n    The webhook URL is usually not a part of the request but rather configured beforehand as a part of merchant-customer onboarding and available as a part of customer details.\n  \n\n\n\n\n4. Fault tolerance and Reconciliation\n4.1. Application Failure\nReconciliation is necessary to ensure that a request is processed after responding to the customer system’s request with a 202 response and workflow-id. At this stage, we have an entry in the workflows table and a key-value pair of the request data containing the customer and patient details.\n\nThe scheduler such as Airflow is configured to check for workflows in pending state(s) (RECEIVED, IN_PROGRESS, LINKED, NO_RESULTS) beyond an expected time interval. For instance, if the expected processing time is at most 7 minutes, the workflows with (current timestamp - updated_at) &gt; 7 minutes are put back into the queue until it reaches an end state (limit on the number of retries). Followed by an hourly or daily report for workflows stuck in a pending state for further debugging.\n\nHence a cron job/scheduler runs at a frequency (say, every 5 minutes) to check for workflows in pending state(s).\n\n\n\n4.2. Resource Failure\nFault tolerance and high availability go hand-in-hand. While HA ensures minimal service interruption, fault tolerance aims for no service interruption or zero downtime, resulting in a higher cost.\nTypically satisfied with standby servers and storage systems to switch to in the case of failovers. For example:\n\n\n  \n    MySQL/RDS: Automatically switch to a standby replica or secondary instance in another Site/Availability Zone (Multi-AZ DB instance).\n  \n  \n    Compute/EC2: Stand-by consumer/producer cluster spread across sites/AZs to handle failovers.\n  \n  \n    Aerospike/DynamoDB: Nodes in a multi-site/AZ cluster are identically sized and evenly distributed across all sites to handle failovers and enable symmetry in performance and recovery.\n  \n  \n    Message Bus/Queue: In Kafka - done by copying the partition data to other brokers (replicas) by setting a suitable replication factor.\n  \n\n\nLastly, a time-series data store (InfluxDB) for application events and\nAnomaly Detection and Remediation: https://www.pyblog.xyz/anomaly-detection-and-remediation\n\n\n\n5. Scalability, Availability, and Reliability\nThe scalability, availability, reliability, and fault tolerance of the merchant service are dependent on the components mentioned in section 2.2.\n\n\n  \n    The use of load balancers allows us to horizontally scale the application to handle traffic by increasing/decreasing the number of instances in the producer/consumer cluster. Further, health checks ensure high cluster availability and reliability by deregistering unhealthy instances and spinning up new healthy instances.\n  \n  \n    Messaging brokers/bus such as RabbitMQ/Kafka have built clustering abilities, scales horizontally, and is HA (Highly Available). Moreover, even if the messages in the queue are lost, we can always reproduce the messages by reconciliation (section 4).\n  \n  \n    The NoSQL databases and relational databases (ACID vs BASE) are scalable when sharded horizontally. Furthermore, using the master-slave configuration combined with frequent snapshots offers availability and reliability.\n  \n  \n    Workflow orchestrator/Scheduler such as Airflow is a highly available service and plays a prominent role in timely reconciliation. That said, the scheduler’s unavailability may delay the response time for failed processes until recovered.\n  \n\n\n\n\n6. Assumptions\n6.1. Unique Search Result\nThe search API returns a single patient entry; however, to handle a list of patients in the search result, one possible approach is to pick the best result, and link automatically or let the customer/client decide the best link (compliance and patient data privacy?).\n\n\n\n6.2. Sync Search\nThe API call to the internal system to search for the patient is synchronous. However, the async API call working is as follows:\n\n\n  \n    The search API request to the internal system returns a 202 status code with search-id, and the callback URL is pre-configured (webhook).\n  \n  \n    The search-id is now a column in the workflows table.\n  \n  \n    The search search-id is saved in the workflows table with the status REQUESTED, which falls under pending state(s) to perform reconciliation beyond a time interval.\n  \n  \n    Note: Search call to the internal system is made after receiving the message from the consumer and not before pushing the message to the queue in the producer; hence search-id is initially NULL.\n  \n\n\n\n\n6.3. Patient Details\nThe /link API is meant for acknowledging the link status, and another API\n /patient/&lt;patient-id&gt;  gives the patient details after a successful link.\n\n\n\n7. Questions\n\n\n  \n    \n      Question\n      Sections\n    \n  \n  \n    \n      How should the link request work?\n      3.1, 3.2, 3.3\n    \n    \n      What does your system do when it receives the request?\n      3.1, 3.2\n    \n    \n      How and when does it respond?\n      3.2, 3.3\n    \n    \n      What data stores does your system use?\n      2.3\n    \n    \n      What are all the things you need to consider in terms of reliability, availability, scalability?\n      4, 5\n    \n    \n      How is your system fault tolerant?\n      4, 5\n    \n    \n      What does your overall solution look like and what are the components inside your system?\n      2, 3\n    \n    \n      Describe the APIs, responses, any behaviors you have designed within your system\n      3.1, 3.2, 3.3"

    },
  
  
  
  
    {

      "title"    : "Google Summer of Code - Cartograms in Grapher",
      "url"      : "/gsoc-2022",
      "index"    : "strawberry",
      "content"  : "Wish me Luck :) \n\nUpdate: I got the gig 😋\n\nA proposal I submitted for Google Summer of Code, Summer 2022 to Our World in Data (OWID) to implement a population-scaled, shape preserved world cartogram.\n\n1. Overview\n\nOur World in Data (OWID) strives to make knowledge of global problems accessible and understandable through interactive geographical data visualizations to see how the world has changed over the course of time.\n\nA quick and intuitive view of the world map in relation to population makes it easy for viewers to co-relate the effect and the relative measure’s gravity. OWID uses a range of choropleth maps, but size-scaled maps have downsides, primarily because human visual perception associates areas with importance. Furthermore, readily available alternatives use mosaic cartograms, which distort the shape of countries.\n\nThe basic idea is distorting the map by resizing the regions by population since the population is among the most important aspects to consider; for example, if malnutrition is high in a vast country, then the severity is much worse than if malnutrition is high in a tiny country.\n\n\nFigure 1, a population-based cartogram, shows how a cartogram can give a different impression of the world population compared with the original map.\n\n2. Problem description\nIn a sentence, the primary objective is to plot a visually conclusive world map by illustrating territories using a method for trading off shape and area.\n\nFurthermore, it’s vital to ensure the shape or the outline of a region (Example: Country and Province) is preserved, i.e., visualization steps have to be in place so that the resulting cartograms appear similar to the original world cartograms, such that the area is easily recognizable only by its appearance without the explicit need for labels and quickly understand the displayed data.\n\nTherefore, the problem definition comes down to [2]:\n\n\n  Shape Preservation: Rescaling the territories by a factor without changing the shape is nearly impossible; hence the solution bridges the gap between an ideal solution and the expected outcome.\n  Topology Preservation: Emphasizing the importance for the algorithm to provide a mapping to the original polygon set, without which solving for graph isomorphism is challenging.\n\n\n3. Implementation\nThe project primarily consists of 3 components:\n\n  Data Retrieval and Transformation\n  Core Service\n  Output and Visualization\n\n\nBefore working on the individual components, the first and foremost step is the contract definition (Low-Level Design), considering the most prominent use-cases; the first step is to finalize the inputs, structure of the dataset, interfaces, DTOs, and DAOs.\n\n3.1. Data Retrieval and Transformation\n\nData source:\n\n  File system: CSV or TSV.\n  Database: MySQL.\n  API: Pythonic API for working with OWID’s data catalog; Reference.\n\n\nIrrespective of the data source, the resulting in-memory list or an iterator (for large datasets) is the input for the transformation layer, responsible for standardizing the input to the core service; this ensures that changes in the input are ideally independent of the core functionalities.\n\n3.2. Core Service\n\n3.2.1. Input/Output\n\n\n  Abstract: takes population dataset after data transformation as the input and generates an output to plot a world map in the relative scale.\n  Input Parameters:\n    \n      The population dataset.\n      Number of cells (squares or Hexagons) in the cartogram (Default: 20000, an approximate number for a visually acute world map for a cell density of 0.5 million).\n      Initial border of the world map (Default: Size/Area scaled cartogram).\n    \n  \n  Output:\n    \n      Final border of the world map.\n      Mapping of cells within the above border for each territory/country.\n      Output format: CSV or Json.\n    \n  \n\n\n3.2.2. Gist of the Algorithm\n\n\n  Taking an example, let’s say there are 20000 square (1x1) cells in a size-scaled world map, where each country has a specific number of cells representing the total area.\n  Now, taking population as the measure instead of size changes the number of cells for every country (total number of cells remains to be 20000).\n  To transform a size-scaled world map into a population-scaled map, we move cells across countries while retaining the shape—also known as Diffusion Method (DIF), where cells move from one country to another until a balanced distribution is reached [3]\n\n\n\n  \n    \n       \n      Figure 2 and 3\n    \n    \n      For simplicity, let’s take Australia and India. India is approximately 3,287,263 sq km, while Australia is approximately 7,741,220 sq km, making Australia nearly 2.4 times bigger than India. On the other hand, Australia and India’s populations are 26,068,792 and 1,406,631,776, respectively, making India 54 times the population of Australia.\n      \n    \n    \n      Figure 2 shows the size scaled map of Australia and India; in order to scale the map to population, we move the cells from Australia to India; figure 3 shows the population-scaled map of the two countries. Notice that the map is scaled while retaining the shape of the country. However, the extent of visual identification of a country by its shape after scaling is subjective but measured by computing the level of distortion between the area and population-scaled world map.\n      \n    \n  \n\n\nNote: Figures 2 and 3 are not perfectly scaled and hand-drawn to show the correlation.\n\nPoints to note:\n\n  The cells can be thought of as the pixels, and the border is a perimeter around the pixels.\n  While the algorithm is not limited to the population dataset and is generic, the input and output may adhere to population-specific transformation and standards such as country-code and geojson specifications.\n  It is important to note that the value of a cell is also changed, i.e., in figure 2, each cell represents a 25,000 sq km area, whereas, in figure 2, each cell represents a 0.5 million population count.\n\n\n3.3. Output and Visualization\nAs mentioned above, the output primarily consists of two sections, the cells, and the border. Each cell is associated with a territory/country-code, XY coordinates (lower-left), and metadata.\nThe output of the core service is fed to the transformation layer to produce meaningful illustrations; one such example is creating an SVG of the world map. Furthermore, the transformed output is stored appropriately in a data or a file store. Other transformations include formats that facilitate easy integration with visualization tool(s).\n\n4. Deliverables and milestones\n\n4.1. Deliverables\n\n\n  Input and output standardization (independent of the dataset but flexible enough to support a variety of population datasets) and transformation (mappers).\n  Cells (X, Y, country) and Borders (X, Y, country-code, border-type) generator from Topo-json.\n  Cell density (the value of each cell, for example, a country of population 5 million would have ten cells if each cell translates to 0.5 million) calculator depending on the input; necessary to determine a suitable number of cells in the world map.\n  Measure distortion or topology error: To find the extent of country shape distortion, the relative change in position or orientation across countries, and adjacency distortion between neighboring countries [3].\n  Algorithm for Cartogram:\n    \n      Implementation of Diffusion Method (Section 3.2.2).\n      Abstraction to experiment/use other algorithms.\n    \n  \n  Output transformation:\n    \n      Data frame (cells and border) to CSV/JSON convertor.\n      Persistence of the output (MySQL/file-store).\n      Visual representation of the persisted output in the form of SVG.\n      Integration with graph visualization tool(s) (may need additional output data transformation).\n    \n  \n\n\n4.2. Timeline\n\nWork estimation: 300 hours\n\nIn the summer term, I have opted to enroll in an additional course, “Process of Data Science” and am likely to volunteer to cook food for an NGO. I intend to work 20 hours a week until the project’s completion without a break.\n\nAgile methodology 15-week timeline with 7 x 2-week sprint(s):\n\n  Sprint 1: June 13 - June 20\n    \n      Pre-requisites: Requirement closure and repository set-up.\n      Data: Format and unit(s) finalization of Input and Output (flexible for extension).\n      Cartogram Algorithm: Implementation details (Pseudo Code).\n    \n  \n  Sprint 2: June 20 - July 4\n    \n      Data (Transformation): Raster (.asc) to list of cell coordinates and computation of boundaries for cell groups/countries.\n      Cartogram Algorithm: Low-level design with test cases.\n      Visualization: Explore suitable libraries for visualizing the output.\n    \n  \n  Sprint 3: June 4 - July 18\n    \n      Cartogram Algorithm: First working model of moving cells across cell groups/countries.\n      Validation: Calculate the degree of distortion of the population-scaled map compared to the size-scaled map (Ideally, find a measure for shape preservation).\n      Verify test cases (Test Driven Development).\n    \n  \n  Sprint 4: July 18 - August 1\n    \n      Cartogram Algorithm: Shape preservation implementation details, low-level design, and test cases.\n      Visualization: Visualize population scaled world map to better understand shape preservation.\n    \n  \n  Sprint 5: August 1 - August 15\n    \n      Diffusion Algorithm: Shape preservation implementation and re-iteration.\n      Verify test cases (Test Driven Development).\n    \n  \n  Sprint 6: August 15 - August 29\n    \n      Cartogram Algorithm: Implementation closure to piece it together (Result: Move cells across cell groups/countries while retaining the shape of the country and measure the degree of distortion).\n      Visualization: Explore developing an easy-to-use visualizer to delete and insert cells in a grid (world-map) for fine-tuning the output map.\n    \n  \n  Sprint 7: August 29 - September 12\n    \n      Visualization: Implement a visualization tool to fine-tune the output world map and re-generate the output CSV.\n    \n  \n  Closure: September 12 - September 19\n    \n      Buffer for completion for leftover(s).\n      Documentation: Installation, Usage, and example.\n    \n  \n\n\n4.3. Prototype\n\nEarly requirement gathering and building a prototype gives a head start and clarity in work involved, translating to nearly accurate project planning.\n\n4.3.1. Contiguous Cartogram\n\nI initially built a prototype doing the following:\n\n\n  As mentioned in section 3.2.2, a size-scaled world map with 20000 cells is derived from a raster world map dataset of 60 minute/1 degree (~110km) resolution.\n  Next, the borders are computed from cells for all unique country codes.\n  Lastly, compare with the population-scaled world map manually derived by moving cells across countries [1].\n\n\n\nFigure 4: Size-scaled world map\n\nFigure 4 shows the size-scaled world map derived from the raster dataset [5], whereas Figure 5 shows the expected population-scaled world map output. By the end of this project, the goal is to generate Figure 5 algorithmically.\n\n\nFigure 5: Population-scaled world map (2018)\n\n4.3.2. Non-contiguous Cartogram\n\nWhile the high-level design of generating a population-scaled world cartogram remains conceptually the same, one other approach to distort a polygon (with increased complexity to rearrange):\n\n\n  Import the size-scaled/previous year population-scaled world map (geoJson).\n  Fill the entire region with points representing the center of cells.\n  Filter the cells outside the polygon (country); while doing so, the width of the stroke around the border for selecting edge nodes is responsible for increasing/decreasing the number of cells (while retaining the shape).\n  Repeat the steps for all polygons/countries.\n\n\n\nFigures 6.a and 6.b: US country is shrunk in size. The points in dark blue represent the cells to be removed.\n\n4.3.3. Working Prototype\n\nLastly, using the previous year’s cartogram to generate the cartogram of the successive year yields a better result than scaling a size-scaled world map. For example, figure 7 shows the 2018 population-scaled cartogram (manually generated). Figure 8 shows the map of 2050 generated algorithmically with figure 7 as the base reference.\n\nAfter exploring existing tools such as Tilegrams (not maintained anymore), I built a lightweight prototype to handle higher resolution.\n\nPrototype: https://www.pyblog.xyz/population-cartogram\n\nOther Scripts: https://github.com/addu390/population-cartogram\n\nThe prototype allows changing the radius of the hexagon and visualizing the population-scaled world map from 1950 to 2100.\n\n\nFigures 7: World Population Cartogram from 1960 to 2060\n\n4.3.4. Mentorship\nI have been brainstorming with industry experts and received tremendous feedback for the project. Most importantly, OWID GSoC mentor Daniel Bachler constantly helped me gather the requirements, work towards a possible solution, and improve the proposal. Secondly, the discussion with Matt Dzugan on the World population cartogram laid a foundation for the prototype.\n\n5. About me\n\nMy name is Adesh Nalpet Adimurthy, and I’m a computer science graduate student at Dalhousie University, a budding illustrator, and previously a backend engineer at PhonePe and YC ClearTax. I’m proficient in Java and Python but versatile to adopt any programming language. From GSoC, I hope to get a head start as an open-source contributor and give back to the community.\n\nContact:\n\n  Email: 390.adesh@gmail.com\n  Institution: Dalhousie University\n  Program: Master of Applied Computer Science\n  Timezone: Atlantic Standard Time\n\n\nLinks:\n\n  Github: https://github.com/addu390\n  Author at: https://pyblog.xyz/\n  Bio: https://www.pyblog.xyz/about\n\n\nRelevant projects and discussions:\n\n  hybrid spatial indexes and a prototype for secure delivery.\n  Project Idea and discussion\n\n\n6) References\n\n[1] M. Dzugan, “World Population Cartogram,” GitHub, Mar. 25, 2022. https://github.com/mattdzugan/World-Population-Cartogram (accessed Mar. 25, 2022).\n\n[2] Keim, Daniel &amp; North, Stephen &amp; Panse, Christian. (2004). CartoDraw: A Fast Algorithm for Generating Contiguous Cartograms. IEEE transactions on visualization and computer graphics. 10. 95-110. 10.1109/TVCG.2004.1260761. \n\n[3] M. Alam, S. Kobourov, M. Schneider, and S. Veeramoni, \"An Experimental Study of Algorithms for Cartogram Generation.\" Accessed: Mar. 25, 2022. [Online]. Available: https://www2.cs.arizona.edu/~mjalam/cartogram/main.pdf.\n\n[4] \"Proposal Example 2, Google Summer of Code Guides,\" google.github.io. https://google.github.io/gsocguides/student/proposal-example-2.html (accessed Mar. 25, 2022).\n\n[5] \"National Identifier Grid, v4.11: Gridded Population of the World (GPW), v4, SEDAC,\" sedac.ciesin.columbia.edu. https://sedac.ciesin.columbia.edu/data/set/gpw-v4-national-identifier-grid-rev11 (accessed Mar. 28, 2022)."

    },
  
  
  
  
    {

      "title"    : "Anomaly Detection and Remediation",
      "url"      : "/anomaly-detection-and-remediation",
      "index"    : "grapes",
      "content"  : "Expect the Unexpected \n\n1. Introduction\nThe overview of the post is on building a system to detect potential anomalies and take immediate action(s). Hence primarily has two phases: Detect and Contain.\nHowever, the post discusses the use case to detect and contain anomalies as a generic implementation detail.\n\n1.1. Two main components:\n\n  \n    Anomaly Detection Service (Detect): Define rules to detect abnormal incidents.\n  \n  \n    Kill Switch Service (Contain): Prioritize and evaluate rules to switch the application behavior.\n  \n\n\n2. Anomaly Detection Service\nAnomaly Detection service is an alerting framework for configuring alerts on top of any time series datastore.\n\n2.1. Working Overview\nEvents from applications(s) are published and stored in a time-series data store. Predefined rules (absolute and relative) are evaluated at a fixed time interval and frequency to alert if any anomalies are detected.\n\n2.2. Anomalies can be broadly categorized into two types:\n\n  \n    Absolute Anomaly: is based on absolute data values; for example, an application receives a minimum of 400 payments every 30 minutes from 10 AM to 5 PM; anything below the minimum is considered an anomaly.\n  \n  \n    Relative Anomaly: is based on data values relative to the previous day/week/month; for example, a 25% or more decrease in the number of payments in a 30-minute time window(s) from 10 AM to 3 PM relative to the previous day(s) (on the same day and 30-minute window); i.e., if there were 200 payments from 3:00 PM to 3:30 PM yesterday and it’s less than 140 today (more than 25% decrease), it’s an anomaly.\n  \n\n\n2.3. Model\nPresuming the data is being published to a time-series store, the different fields of a rule are as follows:\n\n  ID: A Unique Identifier.\n  Query: To get the data for evaluation from the time-series data store.\n  Anomaly rule-type: Absolute/Relative.\n  Time window: Sliding window interval.\n  Time Interval: From and to interval.\n  Frequency: Frequency of rule evaluation (Every x seconds).\n  Expiry/Validity: To decide whether to evaluate the rule or not.\n  Content: Alert content (JSON or String).\n  Receivers: Webhook, Email, etc.\n  Status: Active/Inactive.\n\n\n2.4. System Design\nAt this point, it’s clear that queries have to be executed within a defined time interval (from-to) and time window; furthermore, in the case of relative rules, current query results have to compare the results with historical data.\n\nHowever, historical data is often archived and is not scalable to run a query every time the rule is evaluated, as certain critical systems may have frequent executions.\n\nOne possible solution is to use lazy loading: Store the results in ElasticSearch (Query result, time window/interval, rule ID, etc.) after evaluating the rule; thereby, for relative rules, the historical data values can be retrieved from ElasticSearch instead of querying the time-series datastore every time. Furthermore, having a pipeline to prefill historical data results into ElasticSearch further reduces the rule evaluation time.\n\nWhen a rule is registered, for the time interval and widow, a scheduler is responsible for evaluating the rule at a predefined frequency. For instance, a rule has a 30-minute time window and a 10 AM to 3 PM time interval with a frequency of 10 minutes: in this case, the rule evaluations are 10-10:30, 10:10-10:40, 10:20-10:50, and so on.\n\nLastly, when the rule has to be evaluated, the scheduler publishes a message to the queue. Once the rule evaluation results in an alert, the alert message is pushed to another queue.\n\nHence, the different components involved are as follows:\n\n\nAnomaly Detection Service System Design \n\n\n  Key-value store (MongoDB):\n    \n      A catalog to store the rule(s) and alert(s) data required for rule evaluation and alerts (A better option: use ElasticSearch to search for rules from time interval and window - necessary for the reconciliation of rule registration).\n      For idempotency - to ensure the same rule is not evaluated more than once under the same time window and keep track of previous evaluation(s) status.\n    \n  \n  \n    Queue (RabbitMQ/Kafka): To queue rule evaluation and alert requests.\n  \n  Time-series data store (InfluxDB): Datastore for application events. Rules are evaluated by querying the data.\n\n\n2.5. Conclusion\nThe applications of the Anomaly detector are endless as long as data is published to a time-series data store and have use cases across domains: abnormal queries on the database, frequency of APIs, usage of tools (Cloud Trail), change in user privileges, resource utilization spikes, and many more. The action items after detecting anomalies are open-ended, and lastly, the rule evaluation is not limited to a query on the time-series data store. The extension could be having an ML decision layer for the query data.\n\n3. Kill Switch Service\n\n2.1. Working Overview\nThe basic idea of the “kill-switch” is to take a JSON input and evaluate a set of rules on the input JSON and return the match result. \nAfter that, it’s up to the receiver to decide the action item; one go-to approach is to throw an exception with a custom status code and configure the client-side pages to display a relevant message such as “Temporarily blocked.”\n\nFor example: Let’s say there’s a workflow in the application where the purchase of an order is fulfilled, and HSBC bank is temporarily down, or there’s a bug identified; hence, to block such workflows, one would create a kill-switch rule which matches to True for PURCHASE workflow of HSBC bank.\n\n\n  Predefined Rule: WORKFLOW == \"PURCHASE\" and BANK == \"HSBC\"\n  JSON input: {\"WORKFLOW\": \"PURCHASE\", \"BANK\": \"HSBC\"}\n  Output: True\n\n\n3.2. Model\nThe different fields of a rule are as follows:\n\n  ID: A Unique Identifier.\n  Conditions: To evaluate the input JSON.\n  Time Interval: From and to interval (Validity/Expiry of the rule).\n  Frequency: Frequency of rule evaluation (Defaults to 30 seconds).\n  Status: Active/Inactive.\n\n\n3.3. System Design\nWhile implementing a simple rule engine for the given JSON is relatively easy, the expectation is that most APIs in the backend application would need to call the Kill-switch service to evaluate. Hence, ensuring low latency is of a higher priority.\n\nThe kill-switch service can store the ruleset in a key-value cache-store such as Redis with a longer TTL and update the cache when the rules are expired or modified.\n\nThe application calling the kill-switch service still has to bear the network latency to make the API call, possibly for most backend APIs. Using another layer of Redis is a bad idea, given the TTL may have to be seconds. A better approach is to load the kill-switch rules relevant to the integrated application at run-time for a predefined frequency, using a scheduler + queue combination.\n\nThe barebone implementation of KS: https://github.com/addu390/kill-switch\n\nThe different components involved are as follows:\n\n\nKill Switch Service System Design \n\n\n  \n    Data Store (MySQL): To validate and store the rule(s).\n  \n  \n    Queue (RabbitMQ/Kafka): (for Client application) Scheduler callback pushed to a queue to import the kill-switch rules (in-memory).\n  \n  \n    Key-value Cache (Redis): To cache the rules to facilitate low latency API calls.\n  \n\n\n3.4. Conclusion\nThe use-case(s) of the kill-switch service spans across domains. All it takes is a set of key-value pairs and rule(s) to validate the key-value pairs, followed by an action item in complete control of the client. Be it temporarily blocking a workflow, a set of users, a tool, or even resources. However, it’s important to use KS for its prime purpose and not force-fit to other use-cases like A/B testing.\n\nBoth the services (AD and KS) work independently and are stand-alone applications. But go hand-in-hand. For instance, activating a kill switch can be an action item of the anomaly detector."

    },
  
  
  
  
    {

      "title"    : "Hybrid Spatial Data Structures: Quad-KD and R-KD trees",
      "url"      : "/hybrid-spatial-index-conclusion",
      "index"    : "tangerine",
      "content"  : "Quad-KD vs R-KD trees\n\n1. Abstract\n\nA hybrid spatial index is a data structure that combines two or more data structures suitable for effectively storing spatial objects to improve search performance. The project report compares tree data structures kd-tree, quad-tree, and r-tree and then introduces hybrid tree structures quad-kd and r-kd trees to reduce the access time of spatial objects, specifically when the dataset has both points and polygons. Finally, the report concludes how and when hybrid spatial indexes have better search performance than other commonly used spatial indexes in a 2D space.\n\nIndex Terms—Spatial data structure, kd-Tree, quad-tree, r-tree, hybrid tree, quad-kd tree, r-kd tree, geographic information system.\n\n2. Introduction and Survey\n\n2.1. R-tree\n\nR-tree is a common data-driven data structure used in GIS databases, including MySQL and PostGIS (Postgres). Still, r-trees do not guarantee good worst-case performance (primarily because of overlapping bounding rectangles) but generally perform well with real-world data. Interestingly, r-Tree’s worst-case performance for a search is “undefined”, and the average case is O(logMn), where M is the maximum number of entries in a page; Guttman says, “more than one subtree under a node visited may need to be searched, hence it is not possible to guarantee good worst-case performance.” But I would justify it by saying that the worst-case is O(n + logMn), for example, considering a lot of overlapping rectangles in the r-Tree and storing a small rectangle located in the area where all the other rectangles overlap. A search query for that rectangle is to traverse all subtrees, i.e., O(logMn) nodes and O(n) entries [8].\n\n2.2. Quad-tree\n\nQuad-trees are one of the space-driven data structures used to store raster datasets. But the search performance decreases for high dimensional spatial data and when the density of the points increases.\n\n\nFigure 1: Quadtree\n\nBecause when the points are densely populated in certain areas, the tree is not balanced and may have a worst-case time complexity of nearly O(n), with a best-case of O(log4n) [6].\n\n2.3. KD-tree\n\nKD-trees are space partitioning data structures; despite the varying density of spatial points, the height of the tree is O(log n) with batch insertion; thereby, the average case time complexity of search operation is O(log n).\n\n\nFigure 2: KD-tree\n\nFurthermore, the kd-tree is a good choice for points compared to polygons (as the overlap is needed for polygons), and the performance degrades for higher dimensions (curse of dimensionality). Lastly, the worst-case time complexity to construct a static kd-tree is O(n log n), assuming an O(n) median-of-medians algorithm (to select the median at each level) [7].\n\n2.4. Hybrid-tree\n\nA hybrid tree is a combination of spatial data structures that combines the features of two or more spatial structures for the best interest of improving the performance.\n\nIn the real world, the spatial objects are not evenly scattered; instead, the spatial objects are a combination of complex polygons and points concentrated in some areas (varying density). Hence, one possible approach to improve the search performance is using a combination of spatial data structures (hybrid trees) that perform well with polygons and points. So far, we know that quad-trees and r-trees perform well for polygons, and kd-trees perform the best with points. Hence, using quad/r-tree for polygons and kd-tree for points is a viable option to explore with search time complexity, a combination of r-tree/quad-tree (from 2.1 and 2.2) to search for a polygon/bounding rectangle and an additional search time of kd-tree (from 2.3) for points.\n\nThe rest of the paper compares the search and construction performance of the r-tree, quad-tree, and kd-tree, then introduces and discusses the hybrid spatial data structures, quad-kd tree, and r-kd tree.\n\n\nFigure 3. A representation of a Quad-KD hybrid tree structure.\n\n3. Known results\n\n\n  Sections 2.1 to 2.4 discusses the properties of the quad, kd, and r trees, which form the base for the project survey.\n  The experimental results comparing quad, kd, and quad-kd trees for ten points and ten polygons in different areas in the GIS map:\n    \n      For points: The search time in the quad-kd tree is reduced by 56.6% compared to kd-tree and 45% compared to quad-tree.\n      For polygons: The search time in the quad-kd tree is reduced by 71.4% compared to quad-tree and 55.6% compared to kd-tree.\n      For points and polygons from a GIS map: The search time in the quad-kd tree is reduced by 67.47% compared to kd-tree and 51.3% compared to quad-tree.\n    \n  \n  Quad-tree, r-tree and qr-tree are found to deplete search performance when the overlapping of the data increases.\n  k-d-trees are elegant when bulk-loaded, and modifying or rebalancing a kd-tree is non-trivial; however, r and quad trees do not suffer from this.\n\n\nWhile the research papers [2] [3] show that hybrid trees drastically improve the search performance of points and polygons in a 2D space. The results are either based on theoretical analysis (priori analysis) or experiments (posteriori testing) for a narrow use case considering ten points and ten polygons scattered across the map. The results do not clearly signify that the hybrid trees improve performance in all circumstances. For instance, the density, size, and quantity of the spatial data directly correlate with the search performance. Furthermore, the research paper(s) does not indicate any reference to the implementation details of the different spatial tree structures required to better understand the gravity of any optimizations.\n\nThe survey shows the need to test quad-kd trees for different input datasets (varying the densities and quantity of spatial data) and further compare with other hybrid tree(s). Not to mention, the overhead of constructing a hybrid-tree data structure is not taken into consideration.\n\n4. Problem statement\nWith the ever-growing market share of the supply chain industry, spatial indexing techniques play an important role in time-critical applications and managing spatial data. For example, a common use case of in-memory spatial tree structures in geographic information systems is indexing the spatial objects on the client devices, which often have low CPU. The GIS imports a section of the spatial data and indexes them on the client device for a better user experience to access spatial information.\n\nWe know that spatial objects consist of points and polygons, and the same spatial structure does not yield the best search performance for both points and polygons. On the other hand, the current results do not depict the real-world arrangement of spatial data. Therefore, the problem statement is to take the theoretical analysis into consideration to choose a set of trees for comparison, depict a real-world spatial dataset, and validate with another hybrid tree to conclude the effect of hybrid trees to improve search performance.\n\n5. Experiment\n\n5.1. Assumptions\n\nTo ensure that the experiments conducted are within the timeframe. The following assumptions define the scope of the project\n\n  Spatial data are in a 2-dimensional space.\n  Spatial objects are points and rectangles (as complex polygons are represented by minimum bounding rectangles - MBRs).\n  Performance comparison of search and construction/insert operations.\n\n\n5.2. Data structures\n\nThe following are the set of spatial data structures compared against each other:\n\n  R-tree\n  KD-tree\n  Quad-tree\n  Quad-KD tree\n  R-KD tree\n\n\n5.3. Input dataset\n\nAs mentioned earlier, an important aspect of the experiment is depicting the real-world spatial data arrangement.\n\n  Space of 100,000 x 100,000 units.\n  A range of 1 to 80,000 points and rectangles.\n  Varying densities of points and rectangles by placing a group of points and rectangles closer.\n  Choosing different bounding boxes for kd-trees within the quad-tree/r-tree.\n\n\n5.3. Results\n\nImportant and interesting results:\n\n  For 2D points, r-kd hybrid trees have the best search performance, with nearly 49% and 58% reduced search time compared to quad and r-trees, respectively, and slightly better than kd-trees. R-kd trees search for points within a bounding rectangle (by grouping nearby points), unlike the kd-trees, which partition the entire area. Similarly, quad-kd trees search for the points within a bounding rectangle in a nested quadrant.\n  As the number of 2D rectangles increases, the search performance of quad-tree is ~26% better than r-tree, which correlates to saying quad-kd trees are better than r-kd trees for rectangles. Furthermore, the construction/bulk-load time of the r-tree is significantly greater than the quad-tree.\n  The density of points does not drastically affect the search performance when there are multiple high-density groups of points across the map.\n  When the spatial data has more than 25,000 points and rectangles, quad-kd trees outperform all the other trees. In general, hybrid trees have better search performance, for instance, the quad-kd tree has nearly 68% reduced search time than the r-tree, and the r-kd tree has almost 54% reduced search time than the quad-tree. However, the construction time of the hybrid trees is higher because of the “search and insert” mechanism while inserting points within the bounding rectangle.\n  Furthermore, when the dataset has less than 25,000 points and rectangles, r-kd trees have better search performance than quad-kd trees, averaging a 17% improvement.\n\n\nNote: The above improvement or reduction percentages depend on the input dataset and has a precision of +/- 7%.\n\nAfter conducting the experiments for several scenarios for varying densities and different quantities of points and rectangles, some of the noticeable results are as follows:\n\n5.3.1 Points\n\n\nFigure 4: Comparison of search and construct/insert performance of quad, kd, r, quad-kd, and r-kd trees containing 100 to 20,000 points.\n\n\nFigure 5: Comparison of search and construct/insert performance of quad, kd, r, quad-kd, and r-kd trees containing 100 to 80,000 points.\n\n\nFigure 6: Comparison of search and construct/insert performance of quad, kd, r, quad-kd, and r-kd trees containing 25,000 points with densities from 0 to 10.\n\n5.3.2 Rectangles\n\nWhen the spatial data has only rectangles, r-kd and quad-kd trees are a mere representation of r-tree and quad-tree, respectively. Hence, for rectangles, the comparison is between quad and r-tree without hybrid trees.\n\n\nFigure 7: Comparison of search and construct/insert performance of quad-tree and r-tree containing 100 to 80,000 rectangles.\n\n5.3.3 Points and Rectangles\n\nBecause of the known complexities of implementing kd-trees for polygons, the kd-tree is omitted from the comparison when the spatial data contains rectangles.\n\nNote: For hybrid trees (quad-kd and r-kd), the points are within ~32 bounding rectangles spread across the area.\n\n\nFigure 8: Comparison of search and construct/insert performance of quad, r, quad-kd, and r-kd trees containing 100 to 20,000 points and rectangles.\n\n\nFigure 9: Comparison of search and construct/insert performance of quad, r, quad-kd, and r-kd trees containing 100 to 80,000 points and rectangles.\n\n\nFigure 10: Comparison of search and construct/insert performance of quad, r, quad-kd, and r-kd trees containing 50,000 points with densities from 0 to 10.\n\nNote: After trial and error, the pre-defined maximum capacity for bounding rectangles for quad and r trees is ~450, and a depth of 150 levels for quad-trees.\n\n6. Implementation\n\nAs mentioned in section 3, the need for experimenting with different datasets depicting real-world use cases is clear. Furthermore, implementing the data structures from scratch was needed for better flexibility in testing by tweaking various input parameters. Lastly, to abide by the problem statement, exploring other data structures to form another hybrid variant to compare, sets the path to better understanding the hybrid structures altogether (concluded and explained in detail in section 5).\n\nA high-level overview of a series of tasks completed for the project:\n\n  Starting with the low-level design to be able to quickly onboard and compare various spatial index structures.\n  Implementing tree data structures: quad-tree, r-tree, and kd-tree and the hybrid trees: quad-kd and r-kd trees.\n  Generating the input dataset to compare all the five tree structures for different scenarios, some of the prominent input parameters are:\n    \n      Quantity of spatial data (Number of points and rectangles).\n      Density of spatial data (how closely the data points are to each other).\n      Number of bounding rectangles within the given area in hybrid trees.\n      Maximum capacity of bounding rectangle/quadrant (branching factor) to decide when to split.\n    \n  \n  Integrating with a lightweight plotting library to visualize the results.\n  Creating a framework to develop and test spatial data structures, thereby making it easier for further research.\n\n\nCode base: https://github.com/addu390/hybrid-spatial-index\n\n7. Conclusion\n\nSection 5 has the details of the various experiments conducted for all 5 data structures, which are summarized/concluded in section 5.4. However, a short conclusion specifically addressing the problem statement (section 4) is as follows:\n\n\n  Hybrid data structures, the quad-kd tree, and the r-kd tree outperform quad-tree, kd-tree, and r-tree for a spatial dataset containing approximately the same number of points and polygons.\n  Furthermore, the r-kd tree is a good choice if the frequency of accessing the data is higher to compensate for the construction cost of the spatial index. However, the quad-kd tree is better when dealing with larger datasets (spatial data &gt;25,000) within the upper limits of in-memory storage constraints.\n  Lastly, the future work may include exploring alternative representations of hybrid trees (example: succinct), caching models, and analysis of other fundamental operations such as nearest neighbor search, update and delete.\n\n\n8. References\n\n[1] Zhang, X and Du, Z. (2017). Spatial Indexing. The Geographic Information Science &amp; Technology Body of Knowledge (4th Quarter 2017 Edition), John P. Wilson (ed). DOI: 10.22224/gistbok/2017.4.12\n\n[2] Mahmood, Mahmood. (2019). A Proposed Hybrid Spatial Data Structure based on KD Tree and Quad Tree. Jokull. 69. 2-6. \n\n[3] Bereczky, Nikolett &amp; Duch, Amalia &amp; Németh, Krisztián &amp; Roura, Salvador. (2015). Quad-kd trees: A general framework for kd trees and quad trees. Theoretical Computer Science. 616. 10.1016/j.tcs.2015.12.030. \n\n[4] Wang, W.; Zhang, Y.; Ge, G.; Jiang, Q.; Wang, Y.; Hu, L. (2021). A Hybrid Spatial Indexing Structure of Massive Point Cloud Based on Octree and 3D R*-Tree. DOI: 10.3390/app11209581\n\n[5] Jayanaranjan Dash et al. (2015). International Journal of Computer Science and Information Technologies (IJCSIT). Vol 6 (2).\n\n[6] Wikipedia, “Quadtree,” Nov. 06, 2019. Available: https://en.wikipedia.org/wiki/Quadtree. [Accessed Feb. 12, 2022].\n\n[7] Wikipedia, “k-d tree,” Apr. 02, 2022. Available: https://en.wikipedia.org/wiki/K-d_tree. [Accessed Feb. 12, 2022].\n\n[8] Wikipedia, “R-tree,” Aug. 18, 2021. Available: https://en.wikipedia.org/wiki/R-tree. [Accessed Feb. 12, 2022]."

    },
  
  
  
  
    {

      "title"    : "Sketches of the past in the present",
      "url"      : "/gallery-2016",
      "index"    : "kiwi",
      "content"  : "Nothing beats Popeye with double shades. \n\nIn recent years, the convenience of digital art has probably overpowered the traditional forms, which I can’t arguably say is good or bad. After all, it’s the change in tools and not the process.\n\nFrom time to time, going back to just paper and pencil is bliss, the texture, the impressions, the overlapping shades, the fear of mistakes, and the eye for perfection and precision.\n\n\n\nHere’s an archive of sketches dating back to 2016 or earlier:\n\n\n  \n    \n      \n      \n       \n    \n    \n      \n      \n       \n    \n    \n      \n      \n       \n    \n    \n      \n      \n       \n    \n    \n      \n      \n       \n    \n    \n      \n      \n       \n    \n    \n      \n      \n       \n    \n    \n      \n      Many more to come!\n       "

    },
  
  
  
  
    {

      "title"    : "Popeye the Mightiest",
      "url"      : "/popeye-the-mightiest",
      "index"    : "pear",
      "content"  : "Luffy vs Popeye.\n\n\nWreck it Ralph vs Popeye.\n\n\nBen Grimm vs Popeye.\n\n\nSaitama vs Popeye.\n\n\nGoku vs Popeye.\n\n\nHulk vs Popeye.\n\n\nSuperman vs Popeye.\n\n\nAsterix vs Popeye.\n\n\nBluto vs Popeye.\n\nWork In Progress.\nUntil then, who do you think is going to win? 🤔"

    },
  
  
  
  
    {

      "title"    : "Location-based application using Django, PostGIS and Leaflet",
      "url"      : "/django-postgis",
      "index"    : "banana",
      "content"  : "One spatial point to an other — Popeye the Sailor.\n\nIn a prior post, the experiment on hybrid spatial-index to find spatial data points is worth exploring, but the in-memory implementations and the lack of ability to scale make it almost pointless to use it in real-world applications.\n\nIn a web application that requires storage and accessing spatial data (co-ordinates), a combination of Django, PostgreSQL (PostGIS), GeoDjango, and Leaflet (or Google Maps) solves most of the preliminary use cases.\n\nThe Local Set-up\n\n\n  Install Dependencies\n  Start PostgreSQL Server\n  Configure Django Application\n\n\nInstall virtualenv\n\nAs always, when working on python projects, always use virtualenv or conda env. If you don’t have virtualenv already installed, install it (Reference):\n\npython3 -m pip install --user --upgrade pip\npython3 -m pip --version\npython3 -m pip install --user virtualenv\n\n\nInstall Dependencies\n\nCreate an environment for the project, replace env with the appropriate name:\n\npython3 -m venv env\nsource env/bin/activate\npython3 -m pip install -r requirements.txt\n\n\nThe contents of requirements.txt:\n\nDjango==4.0.2\ndjango-leaflet==0.28.2\npsycopg2-binary==2.9.3\ngunicorn==20.1.0\n\n\nOf course, you don’t need gunicorn here, just a practice to remember to not use the development server in production (which I often see in other blog posts).\n\nInstall Prerequisites\n\nPostGIS is a spatial database extender for PostgreSQL object-relational database. It supports geographic objects allowing location queries to be run in SQL.\n\nbrew install postgresql\nbrew install postgis\nbrew install gdal\nbrew install libgeoip\n\n\nMake sure psycopg2-binary is already installed as mentioned in requirements.txt\n\nStart PostgreSQL server\n\nWhile you can install PostgreSQL server locally, using docker makes it a lot easier without any hassle apart from ensuring docker is installed.\n\nStart docker in your local machine and run:\n\ndocker run --name=postgis -d -e POSTGRES_USER=&lt;database-username&gt; -e POSTGRES_PASS=&lt;database-password&gt; -e POSTGRES_DBNAME=&lt;database-name&gt; -p 5432:5432 kartoza/postgis:14-3.2\n\n\nand replace &lt;database-username&gt; and &lt;database-password&gt;, &lt;database-name&gt; as per your needs.\n\nDjango Project Set-up\n\nConfigure PostGIS database, INSTALLED_APPS, and Leaflet in settings.py\n\nDATABASES\n\nAssuming that you already have a Django project handy, in settings.py, the database:\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.contrib.gis.db.backends.postgis',\n        'NAME': config('DATABASE_NAME'),\n        'USER': config('DATABASE_USER'),\n        'PASSWORD': config('DATABASE_PASSWORD'),\n        'HOST': config('HOST_ENDPOINT'),\n        'PORT': '5432',\n    }\n}\n\n\nIf you are using the Mac M1, you’ll need to add the full path to gdal, for local set-up, simply add the below two files in settings.py\n\nGDAL_LIBRARY_PATH = '/opt/homebrew/opt/gdal/lib/libgdal.dylib'\nGEOS_LIBRARY_PATH = '/opt/homebrew/opt/geos/lib/libgeos_c.dylib'\n\n\nINSTALLED_APPS\n\nAdd django.contrib.gis to INSTALLED_APPS in settings.py.\nIf you are using Leaflet, install django-leaflet and add leaflet to INSTALLED_APPS in settings.py.\n\nLEAFLET_CONFIG\n\nAdd LEAFLET_CONFIG configuration in settings.py\n\nLEAFLET_CONFIG = {\n    'DEFAULT_CENTER': (44.638569, -63.586262),\n    'DEFAULT_ZOOM': 18,\n    'MAX_ZOOM': 20,\n    'MIN_ZOOM': 3,\n    'SCALE': 'both',\n    'ATTRIBUTION_PREFIX': 'Location Tracker'\n}\n\n\nLastly, Leaflet might need static; make sure to add the path in settings.py:\n\nSTATIC\n\nstatic file(s) relative path from the root directory:\n\nSTATIC_URL = '/static/'\nSTATIC_ROOT = os.path.join(BASE_DIR, 'static')\n\n\nDjango Models\n\nLet’s say you have a model called Trip, which has the source and destination address (co-ordinates: Latitude and Longitude).\n\nclass Trip(models.Model):\n    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)\n    source_location = models.PointField(null=True)\n    destination_location = models.PointField(null=True)\n\n\nin admin.py:\n\nfrom leaflet.admin import LeafletGeoAdmin\nfrom .models import Trip\n\nclass TripAdmin(LeafletGeoAdmin):\n    list_display = ('source_location', 'destination_location')\n\nadmin.site.register(Trip, TripAdmin)\n\n\nRun migrations, Create a superuser and Start the development server\n\npython3 manage.py makemigrations\npython3 manage.py migrate\n\npython3 manage.py createsuperuser\n\npython3 manage.py runserver\n\n\nFinally, head to http://localhost:8000/admin, enter the username and password, and navigate to the model (Trip), which has the location fields, to create an entry.\n\nCode Snippet\n\nTo search for trips for source_location within a radius\n\nfrom django.contrib.gis.geos import Point\nfrom django.contrib.gis.measure import Distance\nfrom .models import Trip\n\ndef get_trips(latitude, longitude, radius):\n    point = Point(latitude, longitude)\n    trips = Trip.objects.filter(source_location__distance_lt=(point, Distance(km=radius)))\n    return trips\n\n\nThat’s about it! While this is not a complete list of features of GeoDjango, refer to the documentation here: https://docs.djangoproject.com/en/4.0/ref/contrib/gis/tutorial/# for more details.\n\nAs always, the reference code: https://github.com/addu390/dedo"

    },
  
  
  
  
    {

      "title"    : "Floating Point Round Off Errors in Geometric Algorithms",
      "url"      : "/floating-point-convex-hull",
      "index"    : "watermelon",
      "content"  : "Eat Spinach and Build your Convex Hull — Popeye the Sailor.\n\nFloating Point Precision in Geometric Algorithms\n\nMost geometric algorithms are designed for exact real arithmetic; replacing them with floating-point arithmetic may cause those implementations to fail. Unfortunately, there are no go-to documents on what can go wrong and why across geometric algorithms. The rest of the post discusses what can go wrong when geometric algorithms are executed with floating-point arithmetic.\n\nTo make the explanation simple, consider the convex hull algorithms for a given set of points S, the smallest polygon that can contain all the points of S; the extreme points of S would be the vertices of the convex hull polygon; all the points inside the convex hull are simply discarded.\n\nTypically, convex hulls computed by the floating-point implementation may leave out some of the extreme points, compute non-convex polygons, or the algorithm may run forever.\n\nIncremental Planar Convex Hull Algorithm\n\nAn Convex Hull incremental algorithm maintains the current convex hull (CH) of the points seen so far. Initially, a convex hull is formed by choosing three non-collinear points in S. It then considers the remaining points one-by-one. When considering a point r, it first determines whether r is outside the current convex hull polygon. If not, r is discarded. Otherwise, the convex hull is updated by forming the tangents from r to CH. The algorithm maintains the current hull as a circular list L = (v0, v1,. . .,vk−1) of its extreme points in counter-clockwise order, where line segments (vi, vi+1) are the edges of the current hull.\n\nSingle-Step Failures\n\nAn instance of violating the correctness properties of the algorithm. Consider a sequence: p1, p2, p3, . . . of points such that the first three points form a counter-clockwise triangle, and the insertion of some later point leads to a violation of a correctness property (because of floating-point). The examples that resulted in a violation always involve nearly or truly collinear points; to be clear, sufficiently non-collinear points did not cause any problems. Although this may seem that the examples are unrealistic, it mostly depends on the requirement of tolerable rounding errors as point sets may contain nearly collinear points or truly collinear points, which become nearly collinear by conversion to floating-point representation.\n\nGeometry of Float-Orientation\n\nThree points p = (px, py), q = (qx, qy), and r = (rx, ry) in the plane lie on the same line or form a left or right turn.\n\nThe orientation of the triple (p, q, r) is defined by:\n\norientation(p, q, r) = sign((qx − px)(ry − py)−(qy − py)(rx − px)).\n\n\nHowever, for floating-point arithmetic, because of the possible round-off errors, there are three ways in which the result of float_orient could differ from the correct orientation:\n\n\n  Rounding to zero: Misclassify + or − as 0\n  Perturbed zero: Misclassify 0 as + or −\n  Sign inversion: Misclassify + as − or vice-versa.\n\n\nfloat_orient, in other words, triple points are classified as left-turns, right-turns, or collinear.\n\nFailure 1: A point outside the current hull sees no edge of the current convex hull\n\nConsider the set of points:\n\np1 = (7.3000000000000194, 7.3000000000000167) \np2 = (24.000000000000068, 24.000000000000071) \np3 = (24.00000000000005, 24.000000000000053) \np4 = (0.50000000000001621, 0.5000000000)\np5 = (8, 4) \np6 = (4, 9) \np7 = (15, 27) \np8 = (26, 25) \np9 = (19, 11)\n\n\nOrientation:\n\nfloat_orient(p1, p2, p3) &gt; 0 \nfloat_orient(p1, p2, p4) &gt; 0 \nfloat_orient(p2, p3, p4) &gt; 0 \nfloat_orient(p3, p1, p4) &gt; 0 (??)\n\n\n\nFigure 1: Computer Convex Hull\n\nThe above figure shows the computed convex hull, where a point that is clearly extreme is left out of the convex hull; p1 ≈ (17, 17), p2 ≈ (24, 24) ≈ p3\n\nThe first four points lie almost on the line: y = x, and float_orient shows the above results. As indicated by (??), the last evaluation is wrong. Geometrically, these evaluations mean that p4 sees no edge of the triangle (p1, p2, p3). The points p5, . . . , p9 are then correctly identified as extreme points and are added to the hull. \nHowever, the algorithm never recovers from the error made when considering p4 and the result of the computation differs drastically from the correct hull.\n\nFailure 2: A point inside the current hull sees an edge of the current hull\n\nTake a counterclockwise triangle (Initially) and choose the fourth point inside the triangle but very close to one of the edges. There is the chance of sign reversal. For example, consider the set of points:\n\np1 = (27.643564356435643, −21.881188118811881 ) \np2 = (83.366336633663366, 15.544554455445542 ) \np3 = ( 4.0, 4.0 ) \np4 = (73.415841584158414, 8.8613861386138595)\n\n\nOrientation:\n\nfloat orient(p1, p2, p3) &gt; 0 \nfloat orient(p1, p2, p4) &lt; 0 (??) \nfloat orient(p2, p3, p4) &gt; 0 \nfloat orient(p3, p1, p4) &gt; 0 \n\n\nThe convex hull is correctly initialized to (p1, p2, p3). The point p4 is inside the current convex hull, but the algorithm incorrectly believes that p4 can see the edge (p1, p2) and hence changes the hull to (p1, p4, p2, p3), a slightly non-convex polygon.\n\nFailure 3: A point outside the current hull sees all edges of the convex hull\n\nConsider the set of points:\n\np1 = ( 200.0, 49.200000000000003) \np2 = ( 100.0, 49.600000000000001) \np3 = (−233.33333333333334, 50.93333333333333 ) \np4 = ( 166.66666666666669, 49.333333333333336) \n\n\nOrientation:\n\nfloat orient(p1, p2, p3) &gt; 0 \nfloat orient(p1, p2, p4) &lt; 0 \nfloat orient(p2, p3, p4) &lt; 0 \nfloat orient(p3, p1, p4) &lt; 0 (??) \n\n\n\nFigure 2: Schematic view of Failure 3: The point p4 sees all edges of the triangle (p1, p2, p3)\n\nThe first three points form a counterclockwise oriented triangle, and according to float_orient, the algorithm believes that p4 can see all edges of the triangle.\n\nFailure 4: A point outside the current hull sees a non-contiguous set of edges\n\nConsider the set of points:\n\np1 = (0.50000000000001243, 0.50000000000000189) \np2 = (0.50000000000001243, 0.50000000000000333) \np3 = (24.00000000000005, 24.000000000000053) \np4 = (24.000000000000068, 24.000000000000071) \np5 = (17.300000000000001, 17.300000000000001)\n\n\nOrientation:\n\nfloat_orient(p1, p4, p5) &lt; 0 (??) \nfloat_orient(p4, p3, p5) &gt; 0 \nfloat_orient(p3, p2, p5) &lt; 0 \nfloat_orient(p2, p1, p5) &gt; 0\n\n\nInserting the first three points, followed by the fourth point p4 results in the convex quadrilateral (p1, p4, p3, p2), which is correct. The last point, p5, sees only the edge (p3, p2) and none of the other three. However, float_orient makes p5 see also the edge (p1, p4). The subsequences of visible and invisible edges are not contiguous. Since the falsely classified edge (p1, p4) comes first, the algorithm inserts p5 at this edge, removes no other vertex, and returns a polygon that has self-intersections.\n\nThe Takeaway\n\nAs illustrated above, algorithms can fail because of rounding errors, taking an example of a convex hull algorithm implemented with floating-point arithmetic naively.\n\nAs always, the reference to the C++ code, with variants of convex algorithms that fail because of rounding errors and how to prevent it can be found here: https://github.com/addu390/rounding-errors\n\nAs an extension, consider trying the CGAL library by using the exact float (MP_Float).\n\nLastly, the post summarizes [1] and [2]. Which I came across while developing CSCI 6105 lab instructions as a Teaching Assistant.\n\n\n\nReferences:\n\n[1] “Robustness Problems in Convex Hull Computation,” wwwisg.cs.uni-magdeburg.de. http://wwwisg.cs.uni-magdeburg.de/ag/ClassroomExample/#Kettner2008 (accessed Mar. 11, 2022).\n\n[2] L. Kettner, K. Mehlhorn, S. Pion, S. Schirra, and C. Yap, “Classroom examples of robustness problems in geometric computations,” Computational Geometry, vol. 40, no. 1, pp. 61–78, May 2008, doi: 10.1016/j.comgeo.2007.06.003.\n\n[3] “The Floating-Point Guide - Exact Types,” floating-point-gui.de. https://floating-point-gui.de/formats/exact/ (accessed Mar. 11, 2022)."

    },
  
  
  
  
    {

      "title"    : "Custom domain for GitHub pages",
      "url"      : "/gh-page-custom-domain",
      "index"    : "peach",
      "content"  : "Let's get the website up! — Drama llama.\n\nConfiguration\n\n\n  Domain (example.com).\n  Subdomain (www.example.com).\n  HTTPS (Optional but strongly recommended).\n\n\nAt the end of the tutorial, you’ll have a set-up, where all requests to example.com will be redirected to https://www.example.com\n\nEnable GitHub pages in GitHub settings\n\n\n  Go to the repository → Settings ⚙️ → Pages\n  Select Source; Choosing master/main branch will treat README.md as web index.html and choosing /docs will treat /docs/README.md as web index.html\n\n\n\nFigure 1: Enable Gh Pages in GitHub Settings\n\n\n  Theme Choose → Choose theme; Choose one among the default themes or clone your favorite from: jamstackthemes.dev\n  Wait until GitHub publishes the website. Confirmation message: Your site is ready to be published at example.com\n\n\nSpecify custom domain in GitHub settings\n\n\n  Enter Custom domain: www.example.com\n\n\n\nFigure 2: Set Custom Domain\n\n\n  Note (recommended to use www.example.com):\n    \n      If the custom domain is example.com, then www.example.com will redirect to example.com\n      If the custom domain is www.example.com, then example.com will redirect to www.example.com.\n    \n  \n\n\nManage DNS\n\n\n  In the DNS provider’s console (GoDaddy in my case), create four A records and one CNAME.\n    \n      In GoDaddy and a few other DNS providers, you will have to specify @ in the name (Leave in black in AWS Route 53).\n    \n  \n  IP addresses for four A records:\n\n\n185.199.108.153\n185.199.109.153\n185.199.110.153\n185.199.111.153\n\n\nNote: These can change over time, refer to the documentation\n\n\n  Create a CNAME record to point www.example.com to &lt;GITHUB-USERNAME&gt;.github.io\n\n\n\nFigure 3: Set A and CNAME Record(s)\n\nConfirm DNS entries\n\nConfirm CNAME and A records by running dig www.example.com +nostats +nocomments +nocmd; It should return the four 185.x.x.x IP addresses and one CNAME with &lt;GITHUB-USERNAME&gt;.github.io\n\nNote: This can take between an hour and 3 hours for the DNS entries to resolve/propagate. To verify, on the browser: https://&lt;GITHUB-USERNAME&gt;.github.io redirects to http://www.example.com\n\nEnable HTTPS\n\nThe Enable HTTPS checkbox is clickable if everything goes as expected.\n\n\nFigure 4: Enable HTTPS\n\nNote:\n\n  The checkbox takes time and is not clickable, and it can take as long as a day at times.\n  After you Enable HTTPS, once again, it can take between 1 hour to a day.\n\n\nGithub Support is amazing; in case it takes longer than expected, create a ticket: Github Support\n\nThat’s about it! 🚀"

    },
  
  
  
  
    {

      "title"    : "Hybrid Spatial Data Structures: Introduction",
      "url"      : "/hybrid-spatial-index",
      "index"    : "redapple",
      "content"  : "Let’s grow the trees! — Totoro.\n\nWhat’s a Spatial Index?\n\nA spatial index is a data structure that allows for accessing a spatial object efficiently, which is a commonly used technique in spatial databases. Without indexing, searching would require a “sequential scan” of all records in the database, thereby increasing the processing time. The Minimum Bounding Rectangle (MBR), often termed the Bounding Box (BBox), serves as an object approximation in a spatial index construction process.\n\nDifferent types of Spatial Index\n\nVarious spatial indices have been developed that yield measurable performance differences. Broadly, there are two types of structures for spatial index:\nSpace Driven Structures are based on partitioning of the embedding 2D space into grids/cells, mapping MBRs to the cells according to some spatial relationship (overlap or intersect)\nData-driven Structures are organized by a partition of the collection of spatial objects, where Spatial Data Objects are grouped using MBRs adapting to their distribution in the embedding space.\n\nPopular Spatial Index:\n\n\n  Space Driven Structures: Quadtree and KD-tree\n  Data-driven Structures: R-tree and variants\n\n\nQuadtree\nA quadtree is a specialized form of the fixed grid index in which the resolution of the grid is varied according to the density of the spatial objects to be fitted.\n\n\nFigure 1: Quadtree\n\nIn a Quadtree, each node represents a bounding box covering some part of the space being indexed, with the root node covering the entire area. Each node is either a leaf node that contains one or more indexed points and no children, or it is an internal node with exactly four children, one for each quadrant obtained by dividing the area covered in half along both axes. Hence the name “quadtree.”\n\nKD-tree\n\nA KD-tree is a binary tree where each node represents an axis-aligned hyper-rectangle. Each node specifies an axis and splits the set of points based on whether their coordinate along that axis is greater than or less than a particular value, such as the coordinate median.\n\n\nFigure 2: KD-tree\n\nThe KD-tree can be used to index a set of k-dimensional points. Every non-leaf node divides the space into two parts by a hyper-plane in the specific dimension. Points in the left half-space are represented by the left subtree of that node, and points falling to the right half-space are represented by the right subtree [2].\n\nHybrid algorithm\n\nK-D tree has a lot of benefits; however, it is non-trivial to implement efficiently and has issues with high-dimensional data such as line and polygon. Also, Quadtree has a significant disadvantage: an object intersecting the boundaries at level zero will be placed automatically into the root node irrespective of its size. The biggest challenge using quadtrees is to find a good factor for k. The root node can be overfilled with small, poorly partitioned objects if the factor is too small. On the other hand, using a factor too large can lead to excessively loose bounding lengths and, therefore, have too much overlapping between nodes on one level.\n\n\nFigure 3: Hybrid Tree (Quad-KD Tree)\n\nA hybrid algorithm combines two or more algorithms that solve a similar problem. For example, a hybrid tree data structure that combines the features of Quadtree and the KD-tree. The above figure (3) shows a hybrid structure with seven points inserted in a map, so the map is divided into four parts according to the quadtree structure, and each part is subdivided into four parts. If one part has a line or polygon stored in a quadtree structure, but other parts contain points, we use a KD-tree structure inside this part to store the location of each point [2].\nSimilarly, we can think of a hybrid tree data structure that combines the features of R-tree and the KD-tree, where we use a KD-tree structure inside a smaller bound in the R-tree to store the location of each point.\n\nIn order to verify the performance of the hybrid tree structures to retrieve spatial objects: hybrid-spatial-index is a simple comparison of hybrid trees to search points and rectangles on a 2D plane.\n\nHowever, the project is a WORK IN PROGRESS ⚠️, and stay tuned for more updates 🚀\n\n\n\nReferences:\n[1] Zhang, X, and Du, Z. (2017). Spatial Indexing. The Geographic Information Science &amp; Technology Body of Knowledge (4th Quarter 2017 Edition), John P. Wilson (ed). DOI: 10.22224/gistbok/2017.4.12\n\n[2] Mahmood, Mahmood. (2019). A Proposed Hybrid Spatial Data Structure based on KD Tree and Quad Tree. Jokull. 69. 2–6."

    },
  
  
  
  
    {

      "title"    : "External Merge Sort using Priority Queue",
      "url"      : "/external-merge-sort",
      "index"    : "avocado",
      "content"  : "External Sorting — Totoro.\n\nExternal sorting is a class of sorting algorithms that can handle massive amounts of data. External sorting is required when the data being sorted does not fit into the main memory of a computing device (RAM). Instead, they must reside in the slower external memory (Disk).\n\nTo explain the working of External Merge Sort using a Priority Queue, consider the input array: [5, 8, 6, 3, 7, 1, 4, 9, 10, 2]\n\nOverview:\nIn the Split Phase, the large input file is split into smaller chunks that can be fit into the memory.\nIn the Merge Phase, perform K-way merge with each smaller chunk file one after the other and write the output to a file.\n\nSplit Phase:\n\n\n  Split the input into chunks (5 chunks).\n  Then, sort each of the individual chunks.\n  Finally, store the sorted chunks in files (5 temporary files).\n\n\nDoing so, we have 5 files with:\n\nChunk 1: [5, 8]\nChunk 2: [3, 6]\nChunk 3: [1, 7]\nChunk 4: [4, 9]\nChunk 5: [2, 10]\n\nMerge Phase:\n\nCreate m number of HeapNode(s), where the value of the HeapNode is the chunk’s lowest element, and store the reference to the temporary chunk file.\n\nExample:\n\n{\n\t\"element\": 5,\n\t\"file\": &lt;chunk-file&gt;\n} \n\n\n\n  Now, store all the m HeapNode(s) in a Min Heap, where the top node is always the minimum element in the heap:\n\n\n                         1                       \n                       /  \\\n                      2    5\n                    /  \\                         \n                   4    3     \n\n\n\n  Perform the heapify operation -&gt; store the element in an output file -&gt; replace the min element with the next element in the chuck file, which owns min element\n  Pick the min element in the min-heap 1 and write it to an output file 1\n  Find the next element in the chunk file, which owns min element 1\n  Number 7 from Chunk 3; move it to the heap and perform heapify\n\n\n      7                                    2\n    /  \\                                 /  \\\n   2     5      Heapify --&gt;             3    5\t\n  /  \\                                 / \\\n 4    3                               4   7 \n\n\n\n  Pick the min element 2 and append it to output file 1, 2\n  Find the next element in the chunk file, which owns min element 2\n  Number 10 from Chunk 5; move it to the heap and perform heapify\n\n\n      10                                   3\n    /  \\                                 /  \\\n   3     5      Heapify --&gt;             4    5\t\n  /  \\                                 / \\\n 4    7                               10   7 \n\n\n\n  Pick the min element 3 and append it to output file 1, 2, 3\n  Find the next element in the chunk file, which owns min element 3\n  Number 6 from Chunk 2; move it to the heap and perform heapify\n\n\n      6                                   4\n    /  \\                                 /  \\\n   4     5      Heapify --&gt;             6    5\t\n  /  \\                                 / \\\n10   7                               10   7 \n\n\n\n  Pick the min element 4 and append it to output file 1, 2, 3, 4\n  Find the next element in the chunk file, which owns min element 4\n  Number 9 from Chunk 4; move it to the heap and perform heapify\n\n\n      9                                   5\n    /  \\                                 /  \\\n   6     5      Heapify --&gt;             6    9\t\n  /  \\                                 / \\\n10   7                               10   7 \n\n\n\n  Pick the min element 5 and append it to output file 1, 2, 3, 4, 5\n  Find the next element in the chunk file, which owns min element 5\n  Number 8 from Chunk 1; move it to the heap and perform heapify\n\n\n      8                                   6\n    /  \\                                 /  \\\n   6     9      Heapify --&gt;             7    9\t\n  /  \\                                 / \\\n10   7                               10   8 \n\n\n\n  If the next element in the chunk file is smaller than the current min element, replace the min element in MAX_INTEGER and repeat the process until all the elements in the heap are MAX_INTEGER\n  Pick the min element 6 and append it to output file 1, 2, 3, 4, 5, 6\n  Find the next element in the chunk file, which owns min element 6\n  When you see EOF (End of Line), replace it with MAX_INTEGER\n\n\n   MAX_INT                                 7\n    /  \\                                 /  \\\n   7    9      Heapify --&gt;              8     9\t\n  /  \\                                 / \\\n10   8                               10   MAX_INT\n\n\n\n  Continue until the heap looks like:\n\n\n                       MAX_INT                       \n                        /   \\\n                    MAX_INT  MAX_INT\n                    /    \\                         \n                 MAX_INT MAX_INT        \n\n\nThe final output: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 😎\n\nSince you are anyway here, Checkout sorting-algorithms 🚀 to compare sort algorithms; here’s a comparison of the external sort with in-memory merge sort.\n\n\nFigure 1: For Powers of 2\n\n\nFigure 2: For Powers of 10"

    },
  
  
  
  
    {

      "title"    : "Deploying Django Application on AWS Fargate in 8 minutes",
      "url"      : "/django-fargate-in-8-minutes",
      "index"    : "strawberry",
      "content"  : "Totoro — With logo colors (AWS, Docker, Nginx, Guinicorn, and Django).\n\nSo far, we have seen how to dockerize the Django application and deploy the application on EC2.\n\nInstalling Docker Engine on every EC2 instance and running the Dockerized Django application does not scale and make it harder to maintain ⚠️. However, it’s probably the right choice for a staging environment or an application with 1–2 EC2 instances.\n\nThat said, let’s get to it 😎\n\nThe plan ✈️\n\n\n  Install AWS CLI.\n  Install ECS CLI.\n  Configure Fragate Cluster, Task Definition, and Service.\n  Configure AWS log-group.\n\n\nStep 1: Install AWS and ECS CLI\n\n\n  Install AWS CLI: AWS does an amazing job with documentation; follow the instructions to install AWS CLI.\n  Install ECS CLI: Instructions to install ECS CLI\n\n\nI have seen quite a bit of confusion on this one; all you need to do is:\nDownload and set the right permissions:\n\n\n  sudo curl -Lo /usr/local/bin/ecs-cli https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-darwin-amd64-latest\n  chmod +x /usr/local/bin/ecs-cli\n  Verify the installation: ecs-cli --version\n\n\nTo Configure Fragate Cluster, Task Definition, and Service: The original post by AWS: Tutorial: Creating a cluster with a Fargate task using the Amazon ECS CLI.\n\nBut I guarantee you that you’d see no hiccups if you follow along with this post instead as I make the extra effort to fill in the gaps.\n\nStep 2: Cluster Configuration using Amazon ECS CLI\n\nTo define a &lt;cluster-name&gt; with FARGATE launch type in &lt;aws-region&gt;,\n\nRun: ecs-cli configure --cluster &lt;CLUSTER-NAME&gt; --default-launch-type FARGATE --config-name &lt;CONFIG-NAME&gt; --region &lt;AWS-REGION&gt;\n\nTip ⚠️ I often found instances of mixing up the cluster name and the configuration name, so choose the same name for &lt;CLUSTER-NAME&gt; and &lt;CONFIG-NAME&gt; to avoid confusion. Example: ecs-cli configure --cluster ecs-cluster --default-launch-type FARGATE --config-name ecs-cluster --region ca-central-1\n\nLaunch/deploy the cluster:\n\nIf you already have a VPC and subnet(s): ecs-cli up --cluster-config test --vpc &lt;VPC-ID&gt; --subnets &lt;SUBNET-ID-1&gt;, &lt;SUBNET-ID-2&gt;\nOR let the CLI create one for us: ecs-cli up --cluster-config &lt;CONFIG-NAME&gt;\nNote ⚠️ The &lt;CONFIG-NAME&gt; is the same as the one used before.\n\nMake a note of the VPC and Subnet(s), looks like:\n\nVPC created: vpc-xxxxxxxxxxxxxxxx\nSubnet created: subnet-xxxxxxxxxxxxxxxx\nSubnet created: subnet-xxxxxxxxxxxxxxxx\n\n\nStep 3: Create the Task Execution IAM Role\n\nNote: IAM roles are not region-specific\n\nThe ECS Task Execution IAM role is pre-generated by AWS. Check if it’s already present: AWS Console → IAM → Role → Search for ecsTaskExecutionRole (Verify that it’s attached to AmazonECSTaskExecutionRolePolicy policy).\n\nIf it’s not present, create ECS Task Execution IAM role:\n\nCreate a file named task-execution-assume-role.json with the following contents:\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"ecs-tasks.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    },\n  ]\n}\n\n\n\n  Create the task execution role: aws iam --region &lt;AWS-REGION&gt; create-role --role-name ecsTaskExecutionRole --assume-role-policy-document file://task-execution-assume-role.json\n  Attach the task execution role policy: aws iam --region &lt;AWS-REGION&gt; attach-role-policy --role-name ecsTaskExecutionRole --policy-arn arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\n  Replace &lt;AWS-REGION&gt; with the AWS region, you choose to use; for instance, in my case, it’s ca-central-1\n\n\nStep 4: Configure Security Group\n\nTo keep it simple, instead of using a load balancer, we’ll attach a security group to allow incoming internet traffic at port 80.\n\nCreate a security group associated with the &lt;VPC-ID&gt;\n\n\n  aws ec2 create-security-group --description test --group-name ec2-sg-licensing --vpc-id &lt;VPC-ID&gt;\n  Make a note of the Security Group, looks like: sg-xxxxxxxxxxxxxxxx and add inbound traffic rules: aws ec2 authorize-security-group-ingress --group-id &lt;SECURITY-GROUP-ID&gt; --protocol tcp --port 80 --cidr 0.0.0.0/0 --region &lt;AWS-REGION&gt;\n\n\nStep 5: Docker Compose\n\nDocker Compose file set-up:\n\nAmazon ECS CLI supports version 3 and not sub-versions such as 3.8; hence, keeping the docker-compose.yml simple is essential. Here’s an example docker-compose file of a Django-Gunicorn-Nginx application:\n\nversion: '3'services:\n  web:\n    image: &lt;docker-username&gt;/&lt;project-name&gt;:&lt;tag|latest&gt;\n    command: gunicorn --bind 0.0.0.0:8000 licensing_platform.wsgi --workers=4\n    environment:\n      - DEBUG\n      - DATABASE_NAME\n      - DATABASE_USER\n      - DATABASE_PASSWORD\n      - HOST_ENDPOINT\n      - REDIS_LOCATION\n    volumes:\n      - static_volume:/app/static\n      - media_volume:/app/media\n    expose:\n      - \"8000\"\n    networks:\n      - django-network  nginx:\n    image: &lt;docker-username&gt;/&lt;nginx-for-project-name&gt;:&lt;tag|latest&gt;\n    restart: always\n    volumes:\n      - static_volume:/app/static\n      - media_volume:/app/media\n    ports:\n      - \"80:80\"\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: ecs-cluster-licensing\n        awslogs-region: ca-central-1\n        awslogs-stream-prefix: web\n    depends_on:\n      - web\n    networks:\n      - django-networknetworks:\n  django-network:volumes:\n  media_volume:\n  static_volume:\n\n\n\n  Notice that the logging section in the nginx service; change the awslogs-group and awslogs-region\n  Include the environmental variables required by the application as shown in the above docker-compose.yml file. However, a better practice is to use AWS System Manager Parameter Store.\n\n\nTo create a docker-compose.yml for a Django application, refer: dockerize the Django application\n\nThe Dockerfile for service web:\n\nFROM ubuntu:20.04\nADD . /app\nWORKDIR /appRUN apt-get update -y\nRUN apt-get install software-properties-common -y\nRUN add-apt-repository ppa:deadsnakes/ppa\nRUN apt-get install python3.9 -y\nRUN apt-get install python3-pip -y\nRUN python3.9 -m pip install --upgrade setuptools\nRUN apt-get install sudo ufw build-essential libpq-dev libmysqlclient-dev python3.9-dev default-libmysqlclient-dev libpython3.9-dev -y\nRUN python3.9 -m pip install -r requirements.txt\nRUN python3.9 -m pip install psycopg2-binary\nRUN sudo ufw allow 8000EXPOSE 8000\nThe Dockerfile for service nginx:\n\nFROM nginx:stable-alpineCOPY default.conf /etc/nginx\nCOPY default.conf /etc/nginx/conf.dEXPOSE 80\nNginx Configuration file (default.conf):\n\nserver {    listen 80 default_server;\n    server_name _;    location / {\n        proxy_pass http://127.0.0.1:8000;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header Host $host;\n        proxy_redirect off;\n    }    location /static/ {\n        alias /app/static/;\n    }    location /media/ {\n        alias /app/static/;\n    }}\n\n\n⚠️ Notice that the proxy_pass is http://127.0.0.1:8000 and not http://web:8000.\n\nStep 6: ECS Configuration file\n\nCreate a new file ecs-params.yml at the root of the project (the same level of docker-compose.yml) with contents:\n\nversion: 1\ntask_definition:\n  task_execution_role: ecsTaskExecutionRole\n  ecs_network_mode: awsvpc\n  task_size:\n    mem_limit: 0.5GB\n    cpu_limit: 256\nrun_params:\n  network_configuration:\n    awsvpc_configuration:\n      subnets:\n        - \"&lt;SUBNET_ID-1&gt;\"\n        - \"&lt;SUBNET-ID-2&gt;\"\n      security_groups:\n        - \"&lt;SECURITY-GROUP-ID&gt;\"\n      assign_public_ip: ENABLED\n\n\nWe previously noted subnets(s) and security group for inbound traffic; make sure to update them.\n\nStep 6: ECS Configuration file\n\nFinally! it’s time to spin up the resources, run:\n\n\n  ecs-cli compose --project-name &lt;CLUSTER-NAME&gt; service up --create-log-groups --cluster-config &lt;CONFIG-NAME&gt;\n  Check the status: ecs-cli compose --project-name &lt;CLUSTER-NAME&gt; service ps --cluster-config &lt;CONFIG-NAME&gt;\n\n\nLook for the IP address of the Nginx machine, as shown in Figure 1. Visit http://&lt;ip-address&gt;/admin to view the admin page of the Django application or hit an API of your application 🚀"

    },
  
  
  
  
    {

      "title"    : "Deploying Django Application on AWS EC2 and Docker",
      "url"      : "/django-on-ec2-docker",
      "index"    : "grapes",
      "content"  : "Totoro — With logo colors (AWS, Nginx, Guinicorn, and Django)\n\nIf you are here, you are probably looking to deploy a Django project using AWS EC2 and docker without an orchestrator (ECS, EKS, or Docker Swarm – don’t use swarm), you are at the right place.\n\nPS: I’d still recommend using Kubernetes or at least ECS in the longer run!\n\nLet’s get started 🤪\n\nThis post assumes that you are familiar with AWS basic operations, such as spinning up an EC2 instance or other services and attaching a security group with relevant inbound and outbound traffic.\n\nMost resources selected are eligible under the free tier, and I’m not attaching a screenshot of every other screen on the AWS console; instead, just mentioning the steps.\n\nInstead of installing docker-engine in EC2 every time, let’s create a Docker AMI:\n\n\n  Launch Amazon Linux 2 EC2 instance.\n  Set-up docker AMI: Install Docker Engine and Docker Compose; enable and start docker service at AMI boot time.\n\n\nLaunch Amazon Linux 2 EC2 instance:\n\n\n  Select Region: Typically, a region closest to you.\n  Select AMI: Amazon Linux 2 AMI (HVM) – Kernel 5.10, SSD Volume Type (Free tier).\n  EC2 Family: t2.micro (Free tier).\n  Configure instance: Choose the appropriate VPC and subnet (leave it to default subnet if you aren’t sure) and leave the user-data blank.\n  Add your IP address for SSH inbound traffic in the security group and use a wild card 0.0.0.0 (not recommended).\n  Download the RSA keypair .pem file.\n\n\nSet-up docker AMI:\n\n\n  Login into remote EC2 instance using the ssh: ssh -i &lt;ec2-keypair&gt;.pem ec2-user@&lt;ec2-public-ip-address&gt; (don’t forget to set the right permission to the .pem file: chmod 0400 &lt;ec2-keypair&gt;.pem)\n  Install docker engine and docker-compose.\n\n\nsudo yum update\nsudo yum install docker\nwget https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) \nsudo mv docker-compose-$(uname -s)-$(uname -m) /usr/local/bin/docker-compose\nsudo chmod -v +x /usr/local/bin/docker-compose\nsudo systemctl enable docker.service\nsudo systemctl start docker.service\nsudo chmod 666 /var/run/docker.sock\n\n\nNote: To run docker-compose without sudo: sudo usermod -aG docker $USER\n\nRun the Django application on Docker + EC2:\n\n\n  Launch an EC2 instance with docker pre-installed.\n  Configure user data; to include environment variables and start-up script.\n\n\n\nFigure 1: Create AMI from EC2 instance\n\nLaunch an EC2 instance with docker pre-installed:\n\n\n  👻 We are almost done! On the EC2 dashboard, select the EC2 instance, go to “actions &gt; image and templates &gt; create image” and give the AMI a name.\n  Terminate and launch another EC2 instance (to test) from the new AMI (choose custom AMI while launching EC2 instance).\n  In the configure section, the user-data area is as follows (using cloud-init):\n\n\nContent-Type: multipart/mixed; boundary=\"//\"\nMIME-Version: 1.0\n\n--//\nContent-Type: text/cloud-config; charset=\"us-ascii\"\nMIME-Version: 1.0\nContent-Transfer-Encoding: 7bit\nContent-Disposition: attachment; filename=\"cloud-config.txt\"\n\n#cloud-config\nruncmd:\n - echo \"For the first boot\"\n\ncloud_final_modules:\n- [scripts-user, always]\n\n--//\nContent-Type: text/x-shellscript; charset=\"us-ascii\"\nMIME-Version: 1.0\nContent-Transfer-Encoding: 7bit\nContent-Disposition: attachment; filename=\"userdata.txt\"\n\n#!/bin/sh\nsudo chmod 666 /var/run/docker.sock\necho 'export DEBUG=False DATABASE_NAME=licensing DATABASE_USER=admin DATABASE_PASSWORD=xxxx HOST_ENDPOINT=rds-licensing.xxxx.ca-central-1.rds.amazonaws.com REDIS_LOCATION=redis://127.0.0.1:6379/' &gt; ~/script.sh\nchmod +x ~/script.sh\nsudo cp ~/script.sh /etc/profile.d/script.sh\nsource /etc/profile\n--//--\n\n\nNote: The user data script shown above works on first-boot and re-boots/restarts.\n\n\nFigure 2: Launch EC2 instance from Custom Docker AMI\n\nNote: A better practice is to save the user data in a file, convert it to base64.\n\n\nFigure 3: Set user data for EC2 instance (Docker AMI)\n\nLast but not the least, in the user-data, include how’d want to get your docker-compose.yml file. If it’s git, include a git clone &lt;repo&gt;.git and docker-compose up\n\n\nFigure 4: EC2 Security Group\n\nThe EC2 security group is configured to receive public traffic; typically, HTTP traffic would only be from the load balancer.\n\nIf you are following up with my prior post on dockerizing the Django application with MySQL database and Redis cache, here’s a quick set of steps to create RDS – MySQL.\n\nCreate MySQL Database:\n\n\n  Choose a database creation method: Standard Create.\n  Engine options: MySQL\n  Edition: MySQL Community\n  Version: MySQL 8.0.27\n  Templates: Free Tier\n  Settings: Fill in the database name, master username, and master password.\n  DB instance class: db.t2.micro (Free tier)\n  Storage: General Purpose SSD with 20 GB (20 GB is the minimum storage for RDS).\n  Database authentication: Password authentication (Although not ideal, since the database is only accessible within the private network).\n\n\nIf the database set-up is not for production use, make sure to use free tier, disable auto-scaling, backups, enhanced monitoring, and multi-AZ, and don’t set the RDS to “public”; use it within the private network. Instead, attach the EC2 security group to RDS inbound rules at port 3306.\n\n\nFigure 5: RDS Set-up in the private network\n\nAnd yes 🗄 make sure RDS and EC2 are within the same VPC.\n\nYou are good to go 🚀\n\nDon’t forget to update the hostnames and passwords in user data (env variables) and configure the security group EC2 instances."

    },
  
  
  
  
    {

      "title"    : "Dockerizing Django Application — Gunicorn and Nginx",
      "url"      : "/dockerize-django",
      "index"    : "tangerine",
      "content"  : "Totoro — With logo colors (AWS, Docker, Nginx, Guinicorn, and Django)\n\nA step-by-step tutorial on dockerizing a Django application with MySQL database using Guinicorn and Nginx.\n\nThis post assumes that you have some familiarity with Django and likely have a local Django development project, and looking at how to deploy the project in production.\n\nIn the later section of the post, we’ll see how to deploy the project on AWS EC2.\n\nChecklist 🚧\n\n\n  Check if the application is running in your local machine: python manage.py runserver, accessible at http://localhost:8000\n  Don’t forget to run: python manage.py makemigrations, python manage.py migrate, and python manage.py collectstatic\n\n\nThe plan 🛠\n\n\n  We cannot use the Django development server on production; it’s meant for local development and cannot handle concurrent requests – this is why we’ll use a combination of Guinicorn and Nginx.\n  Create docker-file(s) for the Django application running in port 8000 and use Nginx to proxy the incoming request at port 80 to port 8000.\n  A docker-compose file to put it all together, define the network, links, and volumes to server static and media files.\n\n\nA Dockerfile for the Django application 💻\n\n\n  Make sure you have the requirements.txt file with all the dependencies. You can generate one with: pip freeze &gt; requirements.txt\n  The example Dockerfile below is for a Django Application with MySQL database and might need minor changes for other DBs.\n\n\nFROM ubuntu:20.04\nADD . /app\nWORKDIR /app\n\nRUN apt-get update -y\nRUN apt-get install software-properties-common -y\nRUN add-apt-repository ppa:deadsnakes/ppa\nRUN apt-get install python3.9 -y\nRUN apt-get install python3-pip -y\nRUN python3.9 -m pip install --upgrade setuptools\nRUN apt-get install sudo ufw build-essential libpq-dev python3.9-dev libmysqlclient-dev default-libmysqlclient-dev libpython3.9-dev -y\nRUN python3.9 -m pip install -r requirements.txt\nRUN python3.9 -m pip install psycopg2-binary\nRUN sudo ufw allow 8000\n\nEXPOSE 8000\n\n\nAt the time of writing this post, Ubuntu:20.04 and python:3.9 are the stable latest versions.\nPlace the docker file at the root of the project, in the same directory as manage.py\n\nA Dockerfile for Nginx 📄\n\n\n  Create a directory nginx with two files: Dockerfile and default.conf (the directory name can be anything).\n  The contents of these two files are as follows:\n\n\nFROM nginx:stable-alpine\n\nCOPY default.conf /etc/nginx\nCOPY default.conf /etc/nginx/conf.d\n\nEXPOSE 80\n\n\nserver {\n\n    listen 80 default_server;\n    server_name _;\n\n    location / {\n        proxy_pass http://web:8000;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header Host $host;\n        proxy_redirect off;\n    }\n\n    location /static/ {\n        alias /app/static/;\n    }\n\n    location /media/ {\n        alias /app/static/;\n    }\n\n}\n\n\nWhy is it proxy_pass http://web:8000? We’ll come to that in a second.\n\nThe docker-compose file 📄\n\nEven before pushing the image to the Docker hub, the best practice is to test the application a couple of times by building the docker image locally. Hence, two docker-compose files, one of local development 🛺 and the other for production use ✈️.\n\nversion: '3.8'\n\nservices:\n  web:\n    build:\n      context: .\n    command: gunicorn --bind 0.0.0.0:8000 &lt;project-name&gt;.wsgi --workers=4\n    volumes:\n      - static_volume:/app/static\n      - media_volume:/app/media\n    expose:\n      - \"8000\"\n    networks:\n      - django-network\n\n  nginx:\n    build: nginx\n    restart: always\n    volumes:\n      - static_volume:/app/static\n      - media_volume:/app/media\n    ports:\n      - \"80:80\"\n    depends_on:\n      - web\n    networks:\n      - django-network\n\nnetworks:\n  django-network:\n    name: django-network\n\nvolumes:\n  media_volume:\n  static_volume:\n\n\nCommand to start the docker containers: docker-compose up -f docker-compose.dev.yml\n\nNote:\n\n\n  web refers to the Django web application, the name of the service can be anything, but ensure to reference the same name in the nginx configuration file.\n  Ideally, we would want to serve static content (HTML and CSS) from CDN for faster content delivery and not from the EC2 instance (covered that in a follow-up post).\n  Instead of using the default network, it’s better to define an explicit network, such as django-network\n  Notice that the service web has expose and not ports; expose ensures that the port 8000 is accessible only within the local network and not from the host machine (outside world).\n\n\nSimilarly, for production use, build and push the Django and Nginx images to the Docker hub (make sure not to include sensitive information by using .dockerignore)\n\n*.pyc\nmigrations/\n__pycache__\ndb.sqlite3\n.idea\n*.DS_Store\n.env\nstatic\n\n\nRun at the project’s root (same level of manage.py): docker build -t &lt;docker-username&gt;/&lt;project-name&gt;:&lt;tag|latest&gt; . and docker push &lt;docker-username&gt;/&lt;project-name&gt;:&lt;tag|latest&gt;\n\nRun inside the nginx directory created earlier: docker build -t &lt;docker-username&gt;/&lt;nginx-for-project-name&gt;:&lt;tag|latest&gt; . and docker push &lt;docker-username&gt;/&lt;nginx-for-project-name&gt;:&lt;tag|latest&gt;\n\nThe docker-compose file for production use would look like:\n\nversion: '3.8'\n\nservices:\n  web:\n    image: &lt;docker-username&gt;/&lt;project-name&gt;:&lt;tag|latest&gt;\n    command: gunicorn --bind 0.0.0.0:8000 licensing_platform.wsgi --workers=4\n    volumes:\n      - static_volume:/app/static\n      - media_volume:/app/media\n    expose:\n      - \"8000\"\n    networks:\n      - django-network\n\n  nginx:\n    image: &lt;docker-username&gt;/&lt;nginx-for-project-name&gt;:&lt;tag|latest&gt;\n    restart: always\n    volumes:\n      - static_volume:/app/static\n      - media_volume:/app/media\n    ports:\n      - \"80:80\"\n    depends_on:\n      - web\n    networks:\n      - django-network\n\nnetworks:\n  django-network:\n    name: django-network\n\nvolumes:\n  media_volume:\n  static_volume:\n\n\nThe only change here is to use the images from the docker hub rather than building them locally.\n\nCommand to start the docker containers: docker-compose up\nUse the -d flag to run in the background: docker-compose -d up\n\nYou are good to go; for using the env variables inside the container, I prefer explicitly mentioning the env variables to be used in the docker containers that are in the host machine, so the docker-compose file:\n\nversion: '3.8'\n\nservices:\n  web:\n    image: &lt;docker-username&gt;/&lt;project-name&gt;:&lt;tag|latest&gt;\n    command: gunicorn --bind 0.0.0.0:8000 licensing_platform.wsgi --workers=4\n    environment:\n      - DEBUG\n      - DATABASE_NAME\n      - DATABASE_USER\n      - DATABASE_PASSWORD\n      - HOST_ENDPOINT\n      - REDIS_LOCATION\n    volumes:\n      - static_volume:/app/static\n      - media_volume:/app/media\n    expose:\n      - \"8000\"\n    networks:\n      - django-network\n\n  nginx:\n    image: &lt;docker-username&gt;/&lt;nginx-for-project-name&gt;:&lt;tag|latest&gt;\n    restart: always\n    volumes:\n      - static_volume:/app/static\n      - media_volume:/app/media\n    ports:\n      - \"80:80\"\n    depends_on:\n      - web\n    networks:\n      - django-network\n\nnetworks:\n  django-network:\n    name: django-network\n\nvolumes:\n  media_volume:\n  static_volume:\n\n\nTo access the local MySQL server (to test on local machine), the host-name: host.docker.internal\n\nFor production, use AWS RDS – MySQL with the proper inbound traffic rules in the security group of the EC2 instance(s).\n\nDEBUG=True\nDATABASE_NAME = ''\nDATABASE_USER = ''\nDATABASE_PASSWORD = ''\nHOST_ENDPOINT = 'host.docker.internal'\nREDIS_LOCATION = 'redis://127.0.0.1:6379/'\n\n\nWohoo 🚀 Launch ready!\n\nEverything is a lot easier if there’s a project for reference; here it is: licensing-as-a-platform\n\nDeployment – EC2\n\n\n  While it’s much easier to use ECS, I prefer not to get confined to using managed services of a cloud provider.\n  It’s simple, launch an EC2 instance, install Docker Engine and Docker Compose and create an AMI; this would be your Docker AMI.\n  Terminate and Launch a new instance with Docker AMI previously created as the base AMI for EC2, configure as per your need, include the necessary user data, import docker-compose.yml (most likely by git), and run docker-compose up.\n  You will, of course, have to write a shell script to automate the launch of EC2 instances.\n\n\nTypically, create an ALB (Application Load Balancer) with a target group and auto-scaling group 🚁\n\nI will soon write a follow-up post covering AMI, EC2, RDS, ALB, VPC, Security groups, and everything else necessary for a scalable web application."

    },
  
  
  
  
    {

      "title"    : "Stockpile — Store Request and Response",
      "url"      : "/stock-pile",
      "index"    : "kiwi",
      "content"  : "One Piece — Whaaaat? You don’t use Stock Pile?\n\nFor most applications, it is quite crucial to store the request and response of APIs, especially while integrating with external service providers.\nLogging this information solves for most of the use cases, but might not when the request/ response contains sensitive information.\n\nIn such cases, one way would be to encrypt and store both the request and the response, this is exactly what we will be doing in this post.\n\nOverview :\n\n\n  Methods/ commands whose request and response are to be stored are annotated with @StockPile.\n  Arguments of the method to be stored are annotated with @Item.\n  Method interceptor to encrypt and store the request, response/ exception of these methods.\n\n\n@Retention(RetentionPolicy.RUNTIME)\n@Target({METHOD})\n@BindingAnnotation\npublic @interface StockPile {\n}\n\n\n@StockPile annotation is only for methods, @Target – METHOD\n\n@Retention(RetentionPolicy.RUNTIME)\n@Target({FIELD, PARAMETER})\n@BindingAnnotation\npublic @interface Item {\n}\n\n\n@Item is for arguments of a method, @Target – PARAMETER\n\n@Slf4j\npublic class StockPileModule extends AbstractModule {\n\n    @Override\n    protected void configure() {\n        final StockPileInterceptor stockPileInterceptor = new StockPileInterceptor();\n        requestInjection(stockPileInterceptor);\n        bindInterceptor(Matchers.any(), Matchers.annotatedWith(StockPile.class), stockPileInterceptor);\n    }\n\n    @VisibleForTesting\n    public static class StockPileInterceptor implements MethodInterceptor {\n\n        @VisibleForTesting\n        @Inject\n        protected YourEncryptionService encryptionService;\n\n        @VisibleForTesting\n        @Inject\n        protected YourStorageService storageDao;\n\n        @VisibleForTesting\n        @Inject\n        protected ObjectMapper objectMapper;\n\n        @Override\n        public Object invoke(MethodInvocation invocation) throws Throwable {\n            final Optional&lt;Annotation&gt; optionalAnnotation = Stream.of(invocation.getMethod().getDeclaredAnnotations())\n                    .filter(annotation -&gt; annotation instanceof StockPile)\n                    .findFirst();\n\n            if (optionalAnnotation.isPresent()) {\n                List&lt;Object&gt; requests = new ArrayList&lt;&gt;();\n                final String methodName = invocation.getMethod().getName();\n                final Annotation[][] parameterAnnotations = invocation.getMethod().getParameterAnnotations();\n\n                for (int index = 0; index &lt; parameterAnnotations.length; ++index) {\n                    for (final Annotation annotation : parameterAnnotations[index]) {\n                        if (annotation instanceof Item) {\n                            requests.add(invocation.getArguments()[index]);\n                        }\n                    }\n                }\n                final Object response;\n                try {\n                    response = invocation.proceed();\n                    storageDao.save(requests, response, methodName);\n                } catch (Exception e) {\n                    storageDao.save(requests, e, methodName);\n                    throw e;\n                }\n                return response;\n            }\n            return invocation.proceed();\n        }\n    }   \n}\n\n\nThe scope of this post, is to generalise the storage of request and response.\nIt’s upto to the developer to decide what sort of encryption and data store for storage of these blobs (NoSQL such as AeroSpike is a good option to consider) would be.\n\nAs seen in the method interceptor above, we first validate if the method is annotated with @StockPile, then all the arguments annotated with @Item is added to a List&lt;Object&gt;\n\nstorageDao.save takes three arguments:\n\n\n  Request (Object)\n  Response or Exception (Object)\n  Method name (String)\n\n\nprivate void save(Object request, Object response, String commandName) {\n    try {\n        final String userId = MDC.get(RequestContext.REQUEST_USER_ID);\n        final String requestId = MDC.get(RequestContext.REQUEST_ID);\n        \n        final String workflowId = Objects.nonNull(REQUEST_USER_ID) ? userId.concat(\"_\").concat(requestId) : userId;\n        final Date currentDate = new Date();\n\n        Optional&lt;RequestResponseBlob&gt; requestResponseBlob = storageDao\n                .get(workflowId, commandName);\n\n        final byte[] encryptedRequest = objectMapper.writeValueAsBytes(encryptionService.encrypt(request));\n        final byte[] encryptedResponse = objectMapper.writeValueAsBytes(encryptionService.encrypt(response));\n\n        if (storedOutboundMessage.isPresent()) {\n            RequestResponseBlob presentRequestResponse = requestResponseBlob.get();\n            presentRequestResponse.setRequest(encryptedRequest);\n            presentRequestResponse.setResponse(encryptedResponse);\n            presentRequestResponse.setUpdated(currentDate);\n            storageDao.update(presentRequestResponse);\n        } else {\n            storageDao.save(RequestResponseBlob.builder()\n                    .commandType(commandName)\n                    .workflowId(workflowId)\n                    .requestId(requestId)\n                    .request(encryptedRequest)\n                    .response(encryptedResponse)\n                    .created(currentDate)\n                    .updated(currentDate)\n                    .build());\n        }\n    } catch (Exception e) {\n        log.error(\"[STOCKPILE] Error while storing\");\n    }\n}\n\n\nRefer to the previous post for storing userId and requestId in MDC.\nFor the sake of querying, using userId, requestId along with the method name would be ideal as shown above.\n\nUsage:\n\n@StockPile\npublic Response externalServiceCommandOne(@Item final SensitiveInformationRequest request, final String token) {\n  Response response;\n  // Stuff here\n  return response\n}\n\n\nThat’s it! Done 🚀"

    },
  
  
  
  
    {

      "title"    : "Avoid switch case for ENUMs – Visitor pattern",
      "url"      : "/visitor-pattern",
      "index"    : "pear",
      "content"  : "Don’t force-fit the switch-case (Spirited Away)\n\nEven before going ahead with an example, what is Visitor Pattern?\nHere is the Wikipedia definition.\n\nLet’s take a typical example for a Switch case for an ENUM:\nConsider the ENUM, Country:\n\npublic enum Country {\n    CHINA,\n    INDIA,\n    UNITED_STATES,\n    BRAZIL,\n    MEXICO,\n    GERMANY\n}\n\n\nWrite a function which takes the ENUM Country as the input and returns the happiness score\n\npublic float getHappinessScore(Country country) {\n    float happinessScore;\n    switch (country) {\n            case CHINA:\n                happinessScore = 5.19;\n                break;\n            case INDIA:\n                happinessScore = 4.01;\n                break;\n            case UNITED_STATES:\n                happinessScore = 6.89;\n                break;\n            case BRAZIL:\n                happinessScore = 6.30;\n                break;\n            case MEXICO:\n                happinessScore = 6.59;\n                break;\n            case GERMANY:\n                happinessScore = 6.98;\n                break;\n            default:\n                // Throw an exception\n        }\n       return happinessScore;\n}\n\n\nDon’t worry about the accuracy of these scores,\nNow, what is the problem with this approach? In every single place where we’ve to take an action based on the country, we end up writing the switch case again and again, which comes down to the developer to ensure no Country is missed.\n\n\n  Code would look ugly because of the duplication of the Switch case every where.\n  High possibilities of human error.\n  Incase of addition of a new country, the developer should further ensure all switch cases are updated by manually searching for it (As that would not result in any sort of compilation error).\n\n\nUsing the Visitor pattern:\n\npublic enum Country {\n    CHINA {\n        @Override\n        public &lt;T, J&gt; T accept(CountryVisitor&lt;T, J&gt; visitor, J data) {\n            return visitor.visitChina(data);\n        }\n    },\n    INDIA {\n        @Override\n        public &lt;T, J&gt; T accept(CountryVisitor&lt;T, J&gt; visitor, J data) {\n            return visitor.visitIndia(data);\n        }\n    },\n    UNOTED_STATES {\n        @Override\n        public &lt;T, J&gt; T accept(CountryVisitor&lt;T, J&gt; visitor, J data) {\n            return visitor.visitUnitedStates(data);\n        }\n    },\n    BRAZIL {\n        @Override\n        public &lt;T, J&gt; T accept(CountryVisitor&lt;T, J&gt; visitor, J data) {\n            return visitor.visitBrazil(data);\n        }\n    },\n    MEXICO {\n        @Override\n        public &lt;T, J&gt; T accept(CountryVisitor&lt;T, J&gt; visitor, J data) {\n            return visitor.visitMexico(data);\n        }\n    },\n    GERMANY {\n        @Override\n        public &lt;T, J&gt; T accept(CountryVisitor&lt;T, J&gt; visitor, J data) {\n            return visitor.visitGermany(data);\n        }\n    }\n};\n\npublic abstract &lt;T, J&gt; T accept(CountryVisitor&lt;T, J&gt; visitor, J data);\n\npublic interface CountryVisitor&lt;T, J&gt; {\n        T visitChina(J data);\n        T visitIndia(J data);\n        T visitUnitedStates(J data);\n        T visitBrazil(J data);\n        T visitMexico(J data);\n        T visitGermany(J data);\n    }\n\n\nUsage of the CountryVisitor\n\npublic class HappinessScoreCountryVisitor implements CountryVisitor&lt;OutputDto, InputDto&gt; {\n\n   public OutputDto process(final Country country, final InputDto data) {\n        country.accept(this, data);\n    }\n\n// Implement all the methods and write the logic for each of the method\n\n    @Override\n    public OutputDto visitChina(InputDto data) {\n       OutputDto output;\n       // Logic here\n       return output;\n    }\n\n    @Override\n    public OutputDto visitIndia(InputDto data) {\n       OutputDto output;\n       // Logic here\n       return output;\n    }\n\n    @Override\n    public OutputDto visitUnitedStates(InputDto data) {\n       OutputDto output;\n       // Logic here\n       return output;\n    }\n\n    @Override\n    public OutputDto visitBrazil(InputDto data) {\n       OutputDto output;\n       // Logic here\n       return output;\n    }\n\n    @Override\n    public OutputDto visitMexico(InputDto data) {\n       OutputDto output;\n       // Logic here\n       return output;\n    }\n\n    @Override\n    public OutputDto visitGermany(InputDto data) {\n       OutputDto output;\n       // Logic here\n       return output;\n    }\n}\n\n\nNow, the getHappinessScore method would look like:\n\npublic float getHappinessScore(Country country) {\n   InputDto input;\n   /* Usually there would be an Input, incase Input is not require\n   Write the visitor interface accordingly */\n   return happinessScoreCountryVisitor.process(country, input);\n}\n\n\nAs compared to disadvantages with switch-case\n\n\n  Code would look a lot more cleaner.\n  Possibilities of human error is almost NIL.\n  Incase of addition of a new country, the developer will have to implement in all the Visitor implementations, as it would result in a compilation error.\n\n\nWarning: don’t misuse the Visitor pattern for very simple use cases, it would only complicate it rather than simplifying.\nA typical Visitor pattern example.\n\nThat’s it 🚀"

    },
  
  
  
  
    {

      "title"    : "MDC : Improving debugging/logging",
      "url"      : "/mdc-improve-debugging-logging",
      "index"    : "banana",
      "content"  : "Bob the builder, agrees!\n\nConstruct your logs as good as Bob the Builder!\n\nUsually in most of the backend applications, a separate thread from the thread pool processes a client request. Once the request is completed, the thread is returned to the thread pool and it’s typical logs are generated.\n\nWhat are we trying to solve? Let’s take an example.\n\nExample:\n\nConsider a case where two users have logged in and are retrieving data from the database. It is easy to unravel which log statement belongs to which request by differentiating with thread ID. This thread-based correlation of logs is a useful technique to identify all the logs from a single request. However, with this technique, we could not differentiate which request belongs to which user. Now, things get even messier when multithreading is implemented.\n\nPartial solution:\n\nOne of the approaches could be to generate a unique number when the request enters the service and print it with every log statement, but this would be once again difficult to co-relate to the user\n\nSolution:\n\nAnother solution would be to print the user ID and request ID for every log statement. request ID would be generated on the client side. request ID is unique for every request from the user. It will help in identifying a request in case there is a concurrent requests from multiple users (As we have the user ID as well). These are passed as an HTTP header with each request and appended to each log line.\n\nApproach:\n\nThe MDC provides a simple key/value (map) mechanism to capture small amounts of custom diagnostic data. Since it’s built into the logging framework, it’s really easy to add values from the MDC to each log line. MDC is supported by log4j, log4j2, and SL4J/logback (What is use in MF).\n\nMDC allows us to fill a map-like structure with pieces of information that are accessible to the appender when the log message is actually written. The MDC structure is internally attached to the executing thread in the same way a ThreadLocal variable would be.\n\nThe high-level idea is:\n\n\n  At the start of the thread, fill MDC with pieces of information that are required  to make available to the log appender\n  Log the message\n\n\nImplementation:\n\nMy application stack : Java – Dropwizard\nHowever, the idea behind this implementation holds good for any framework.\n\nA method interceptor for all the resource:\n\n/**\n * @author adesh.nalpet\n * created on 20th December 2019\n * A resource method interceptor for adding request context to MDC (Logger)\n */\n@Slf4j\npublic class RequestContextModule extends AbstractModule {\n\n    @Override\n    protected void configure() {\n        final RequestContextInterceptor requestContextInterceptor = new RequestContextInterceptor();\n        requestInjection(requestContextInterceptor);\n        bindInterceptor(Matchers.any(), Matchers.annotatedWith(Path.class), requestContextInterceptor);\n    }\n\n    @VisibleForTesting\n    public static class RequestContextInterceptor implements MethodInterceptor {\n\n        @Override\n        public Object invoke(MethodInvocation invocation) throws Throwable {\n\n            /* Applicable for methods annotated with @Path (Resources) */\n            final Optional&lt;Annotation&gt; optionalAnnotation = Stream.of(invocation.getMethod().getDeclaredAnnotations())\n                    .filter(annotation -&gt; annotation instanceof Path)\n                    .findFirst();\n\n            if (optionalAnnotation.isPresent()) {\n                final Annotation[][] parameterAnnotations = invocation.getMethod().getParameterAnnotations();\n\n                for (int index = 0; index &lt; parameterAnnotations.length; ++index) {\n                    for (final Annotation annotation : parameterAnnotations[index]) {\n                        /* Check for method arguments annotated with @Auth */\n                        if (annotation instanceof Auth) {\n                            try {\n                                final Auth auth = (Auth) invocation.getArguments()[index];\n                                /* Add request and user ID to MDC\n                                Note :\n                                * Any information in Auth can be logged.\n                                * Consider compliance restrictions before logging any sensitive information.\n                                */\n                                MDC.put(RequestContext.REQUEST_ID, auth.getRequestId());\n                                MDC.put(RequestContext.REQUEST_USER_ID, auth.getUserId());\n                            } catch (Exception e) {\n                                /* Deliberately catching all exceptions,\n                                to ensure application is not affected from logger\n                                TODO : Set-up alerts for the below log */\n                                log.error(\"[Auth to MDC] Error while fetching Auth\");\n                            }\n                        }\n                    }\n                }\n            }\n            return invocation.proceed();\n        }\n    }\n}\n\n\nResponse filter to clear context:\n\n/**\n * @author adesh.nalpet\n * created on 20th December 2019\n * A Servlet response filter to clear MDC.\n */\npublic class ClearContextResponseFilter implements Filter {\n    @Override\n    public void init(FilterConfig filterConfig) throws ServletException {\n        /* deliberately nothing */\n    }\n\n    @Override\n    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {\n        try {\n            chain.doFilter(request, response);\n        } finally {\n            RequestContext.clearContext();\n        }\n    }\n\n    @Override\n    public void destroy() {\n        /* deliberately nothing */\n    }\n}\n\n\nRequest Context:\n\n/**\n * @author adesh.nalpet\n * created on 20th December 2019\n * TODO : Validate storing RequestContext in thread local.\n */\npublic class RequestContext {\n\n    public static final String REQUEST_ID = \"request_id\";\n    public static final String REQUEST_USER_ID = \"user_id\";\n\n    public static void clearContext() {\n        MDC.remove(REQUEST_ID);\n        MDC.remove(REQUEST_USER_ID);\n    }\n}\n\n\nLogFormat:\n\nlogFormat: \"%(%-5level) [%date] %X{request_id} %X{user_id} [%thread] [%logger{0}]: %message%n\""

    },
  
  
  
  
    {

      "title"    : "3D Printing iPhone 8 Privacy Case with Camera Cover",
      "url"      : "/privacy-case",
      "index"    : "watermelon",
      "content"  : "You are being watched 🤨 — Totoro\n\n“Should You Cover Your Smartphone Camera?” is a question trending since 2015 🤭\n\nCovering laptop cameras with a piece of tape/slider to prevent surveillance is a common cybersecurity practice.\n\nHowever, most of us fail to follow the same practice with smartphones. Naked smartphone cameras are more vulnerable as smartphones accompany us everywhere, from the boardroom to the bathroom and bedroom, leaving open the potential for even more susceptible moments to be caught on camera without anyone being aware.\n\nObjectively, there are more reasons to cover smartphone cameras than laptop cameras.\nFurthermore, most laptops have a tiny red LED indicating us when the camera is on, and smartphones did not, but the recent update on IOS and Android, a tiny dot appears in the top right corner of the phone’s display. If it’s green, it means that an app is using your camera.\n\nIn early November of 2019, Google confirmed an Android Smartphone Camera vulnerability discovered in July 2019 that affected millions of users, including Google Pixel and Samsung devices, allowing hackers to eavesdrop on someone. Another vulnerability was revealed in the same month when Facebook acknowledged that a bug let the social network’s app access users’ iPhone cameras as they scrolled through their News Feeds (that’s very creepy for a bug) 🤯\n\nBoth companies (Google and Facebook) have patched their bugs and rolled out updates. Still, vulnerabilities such as these are an excellent indicator to cover smartphone cameras.\n\nAs an individual, even if I’m convinced to cover your smartphone camera, there are not many alternatives available in the market to cover both the front and rear cameras of the smartphone.\nOne way is to use a camera patch/slider and a rear camera protection case (usually available for iPhones and limited Samsung models).\n\nFor this very reason, I’m designing a phone case that can be 3D printed. Like any other design, it all starts with a prototype 🧪\n\n\n  \n    \n      Prototype V1 - 1\n      Prototype V1 - 2\n    \n  \n  \n    \n      \n      \n    \n  \n\n\nFigure 1: Fusion 360 Phone Case Design Version 1\n\nWhile the above design serves the purpose, it has three major drawbacks:\n\n\n  \n    First, in the Front view, the bottom sliding track is delicate and expands over time, hampers the slider’s easy movement.\n  \n  The slider movement in the sliding track is not locked, resulting in the slider’s unintended movements.\n  To know if both rear and front cameras are covered, one has to flip the smartphone every time to check.\n\n\nIn the following design, I will be fixing all of the above drawbacks. Stay tuned!\n\n\n  \n    \n      Prototype V2 - 1\n      Prototype V2 - 2\n    \n  \n  \n    \n      \n      \n    \n  \n\n\nFigure 2: Fusion 360 Phone Case Design Version 2\n\n\n  \n    \n      Prototype V2 - 3\n      Prototype V2 - 4\n    \n  \n  \n    \n      \n      \n    \n  \n\n\nFigure 3: Fusion 360 Phone Case Design Version 2 — 3D Print\n\nThe current design is not cost-friendly, and hence I will be re-designing to separate the camera cover slider (Rigid) and the silicone case (Flexible).\n\nAlthough a typical silicone phone case costs about ₹200 (Manufacturing cost in bulk being ₹50), using techniques such as Vacuum Casting and Injection Moulding can be pretty expensive (~₹1000 per piece).\nAs a way easy out, I decided to manufacture only the slider mechanism and outsource the silicon case with a specific opening cut as per requirement.\n\n\n  \n    \n      Prototype V3 - 1\n      Prototype V3 - 2\n    \n  \n  \n    \n      \n      \n    \n  \n\n\nFigure 4: Fusion 360 Phone Case Design Version 4\n\nSince the partial case (as shown in the above figure) with the sliding mechanism is placed directly on the phone, the material is not rigid and moderately flexible.\nTo be explicit, a silicon phone case would be placed on top of it.\n\n\n  \n    \n      Prototype V4 - 1\n      Prototype V4 - 2\n    \n  \n  \n    \n      \n      \n    \n  \n\n\nFigure 5: Fusion 360 Phone Case Design Version 5\n\n\n  \n    \n      Prototype V4 - 1\n      Prototype V4 - 2\n    \n  \n  \n    \n      \n      \n    \n  \n\n\nFigure 6: Fusion 360 Phone Case Design Version 5–3D Print\n\nI have placed the prototype on the iPhone 8 for the sake of reference, and the actual placement would be on a phone case with an opening to place the upper step of the camera cover.\n\nWell, at the end of it, it was a fun project to work on, and more importantly, understanding the complexities in prototyping and factors to consider to launch it to an open market was worth the time spent 🚀\n\nOhh yeah 🤪, I did sell a few back in 2019"

    },
  
  
  
  
    {

      "title"    : "Myoelectric Prosthetic Arm — MYRO 2.0",
      "url"      : "/myo-electric-prosthetic-arm",
      "index"    : "peach",
      "content"  : "MYRO 2.0 — Ironman with Prosthetics 🤪\n\nDeprecated ⛔️\n\nResources\n\n\n  \n    Pattern Recognition (sEMG Feature Extraction)\n  \n  \n    Myo Band Library (Deprecated)\n  \n  \n    KAI Motion Tracker\n  \n  \n    MYRO (Timeline)\n  \n\n\nIntroduction\n\nWikipedia — Electromyography (EMG) is an electrodiagnostic medicine technique for evaluating and recording the electrical activity produced by skeletal muscles. EMG is performed using an instrument called an electromyograph to produce a record called an electromyogram\n\nEMG Sensor\nA Sensor that measures small electrical signals generated by the muscles when we move them, such as movements of fingers, lifting your arm, tensing your fist, among others.\n\n\n  It all starts with the brain. Neural activity in the motor cortex of the brain signals the spinal cord.\n  The signal is conveyed to the muscle part via motor neurons, which innervate the muscle directly, causing the release of Calcium ions within the muscle and ultimately creating a mechanical change.\n  This change involves depolarization, which is detected by EMG for measurement.\n\n\nThe electrodes of the surface EMG sensor(s) are placed on the muscles’ innervation zone; these electrodes detect electrical activity generated by muscle relaxation/contraction. The output of the sensor is a voltage, which is further amplified.\n\nBroadly there are two types\n\n  Surface EMG sensors (sEMG)\n  Intramuscular EMG sensors.\n\n\nEMG pattern recognition methods using data analytics\n\nRecognition of hand movements using sEMG signals generated during muscle contraction/relaxation is referred to as EMG Pattern Recognition.\nEMG Pattern Recognition has a wide range of applications, one among them being upper-limb prostheses.\nBut because of the advancements in sEMG and its applications (Gesture Control being an important one), capturing and analyzing the EMG data is progressing towards the use of “Big Data Analytics” to translate the vast and complex information in EMG signals into useful data for prosthetic devices.\n\nHow does my project currently work?\n\nData Collection\n\nCollect raw EMG signal data (Output of the EMG sensor), apply digital filters above a raw EMG signal, and then extract time and frequency features using the sliding window method.\nData Collection is the most critical aspect of pattern recognition. A large dataset is preferred to understand the variability of the raw sEMG signals. I collected data from two different individuals (and utilized datasets shared by other researchers and groups):\n\n\n  An amputee with the right arm was amputated about five years back (minimal usage of the amputated hand).\n  A swimmer by profession who lost his right hand in an accident.\n\n\nData from hundreds of subjects would be ideal for a comprehensive generalization and robustness of EMG pattern recognition for myoelectric control. But I could not find a dataset or a collection of datasets of that scale. The other reason for not gathering a more extensive dataset was that different researchers use different scales, units, and equipment types. Not to mention the difference in electrode placement, since I used the MYO band from Thalmic labs — an array-like order of the electrodes placed over the muscle area (Such as forearm and biceps). But this does not mean that increasing the number of electrodes increases the number of patterns we can identify. The placement of the electrodes in the right position takes higher importance in the first place.\n\nMultiple Modalities\n\nThe arm’s motion can be identified in several ways, one of them being Electromyography.\nWhat do I mean by that? — analysis of only the surface EMG signals is considered the analysis of a single modality. While there are various ways to detect hand motions, as mentioned, since I used the Myo band, it was already equipped with a 9-Axis motion tracker (3 Axis Accelerometer, 3 Axis Gyroscope, and a 3 Axis Compass). Therefore, pattern recognition is based on the data from sEMG sensors and the 9 Axis motion tracker.\n\nTechnique\nThe pattern recognition system consists of the following steps: Data pre-processing, feature extraction, dimensionality reduction, and classification.\n\nFeature extraction (based on the time domain, frequency domain, and time-frequency domain) to transform short time windows of the raw EMG signal to generate additional information and improve information density is required before classification.\n\nGiving an example of features in the time domain: Most commonly used for EMG pattern recognition as they are easy and quick to calculate as they do not require any transformation. Time-domain features are computed based upon the input signals amplitude.\n\nFeatures: integrated EMG (IEMG), mean absolute value (MAV), mean absolute value slope (MAVS), Simple Square integral (SSI), variance of EMG (VAR), root mean square (RMS), waveform length (WL), and many more.\n\nfeatures_names = ['VAR', 'RMS', 'IEMG', 'MAV', 'LOG', 'WL', 'ACC', 'DASDV', 'ZC', 'WAMP', 'MYOP', \"FR\", \"MNP\", \"TP\",\n                  \"MNF\", \"MDF\", \"PKF\", \"WENT\"]\n\n\nView on Github\n\nBut the challenge is with the selection of an optimal combination of available features. Although this might seem easy for a small dataset, it is not feasible for big data to try out all the possible combinations.\n\nMYRO 2.0 — My Robotic Arm is back!\nObjective\nRe-design techniques for Big EMG Data to improve pattern recognition.\n\nMYRO initially started as a wearable motion tracking unit, which eventually evolved to use it with prosthetics (Final year project at Dayananda Sagar College of Engineering) — I launched MYRO (Myro Labz Pvt Ltd) back in August 2017.\nFor more details regarding the project, refer to the MYRO website and Github.\n\nTo keep the vision alive and resume my work on the project, I recently got the InMoov right-hand 3D printed (I know what you are thinking; I’m using InMoov for representation purposes as a prototype.\n\n\n  \n    \n      Top View\n      Front View\n    \n  \n  \n    \n      \n      \n    \n  \n\n\n\n  \n    \n      Side View\n      Front View - 2\n    \n  \n  \n    \n      \n      \n    \n  \n\n\nCompared to the first version of MYRO, the 3D printed ARM had 5 DOF (Degree Of Freedom) DC motors for individual finger movement (spools and gears). The InMoov arm, on the other hand, is heavier and uses servo motors and strings as the driver, which isn’t very reliable.\nFurthermore, MYRO 1.0 was primarily based on Myography, which used a combination of surface EMG sensors, a gyroscope, and an accelerometer (6 Axis motion tracking). The more refined version of the project used the MYO Band (9 Axis motion tracking) from Thalmic labs. Unfortunately, project MYO was canceled and is now called byNorth (Acquired by Google).\n\nThe major disadvantage of the MYO band was that EMG sensor signals were poor when the residual arm was not actively used by the user, which was the very reason to rely more on the 9 Axis motion tracker.\nWhile looking for other alternatives to Myo Band in the market, I came across KAI from Vicara.\n\n\n  \n    \n      Kai Controller Box\n      Kai Controller - 1\n    \n  \n  \n    \n      \n      \n    \n  \n\n\n\n  \n    \n      Kai Controller Inside - 2\n      Kai Controller Inside - 3\n    \n  \n  \n    \n      \n      \n    \n  \n\n\nThe plan is to use a combination of Surface EMG sensors and KAI (Gyroscope, Accelerometer, and Magnetometer).\n\n\n  \n    \n      3D Printed Prosthetic Improvements - 1\n      3D Printed Prosthetic Improvements - 2\n    \n  \n  \n    \n      \n      \n    \n  \n\n\n\n  \n    Servo motors (Amazon purchase links): MG995 and MG996R\n  \n  \n    Prosthetic Hand: InMoov 3D printed parts\n  \n  \n    Motion Tracker: KAI Gesture Controller — Unfortunately, KAI developer support and community are close to NIL and inactive.\n  \n  \n    Servo Tester: Steering Gear Tester Servo Motor Tester\n  \n  \n    MPU9250 9 Axis Motion Tracker — Worth a try!\n  \n\n\n\n  \n    \n      3D Printed Upper Prosthetic Arm - 1\n      3D Printed Upper Prosthetic Arm - 2\n    \n  \n  \n    \n      \n      \n    \n  \n\n\n\n  \n    Adafruit 9 DOF IMU Fusion Breakout — The best in class 9 Axis motion tracker.\n  \n  \n    Seeed Studio EMG Grove\n  \n\n\nNote: MYRO 2.0 is no longer in active development."

    },
  

  
  
  
  
    {

      "title"    : "Story Illustration: House of Glass",
      "url"      : "/journals/house-of-glass",
      "index"    : "watermelon",
      "content"  : "Embrace the rage! \n\nAs much as I want to write a deep, heart-felt post on how the last couple of months have been, I often find it less dramatic and simple to convey the message over a bunch of illustrations and leave it up to the reader’s interpretation.\n\n\n\nNot too long ago, fair to say - I was living life to its fullest, from owning most things I could ask for, frequently traveling, living close to family, and sprinting up the career ladder.\n\n \nAlthough contradictory, nothing is more uncomfortable than the comfort zone; maybe I was settling for much less than I could achieve. But, after much thought, a master’s degree in computer science seemed too good to be true; about two years, where a good amount of it to specialize in a subject I constantly get paid for, while I get enough time to work on things I value the most.\n\n \nFast forward, I moved to Canada and ate some good lobsters by the shore. My daily routine was packed with bouldering, top-roping, working out at the gym, and tutoring a broad range of topics in computer science, all of which gave me a sense of betterment. Not to mention, falling in love all over again made everything better.\n\n \nLittle did I know I would come across a series of maddening life events while in the struggles of cooking three healthy meals a day, working out, managing the coursework, working part-time, and having a social life. Especially when you come from a place of comfort, and now you have to do it all by yourself, missing home hits you like a truck.\n\nThe level of anxiety was insane; it was as if I was holding on to a cannonball in my chest, heavy, hard to move, and rugged. I slept for less than 3 hours a day, which got worse over time, and before I realized it, I had lost all the gains.\n\n \nFelt like I got knocked out, got picked up, just to get knocked out harder, again and again. But this time, I knew exactly what was going on; here comes my favorite line from Rocky Balboa (2006):\n\n“The world ain’t all sunshine and rainbows. It’s a very mean and nasty place and I don’t care how tough you are it will beat you to your knees and keep you there permanently if you let it. You, me, or nobody is gonna hit as hard as life”\n\n \nI took a break from all the extra work I had been doing and planned all the changes I would need to bring in. I soon started playing my favorite sports, hitting the gym, and making meal preps; things just got better and better. Believe it or not, I lost over 25 pounds in a matter of two and half months, and I’m constantly working towards getting shredded (Work in Progress).\n\n \nSo, what’s the point of this post? Hit the gym! Physical health is strongly coupled with mental health. “gym” in this context is abstract, it could be playing a sport, dancing, or whatever; find yours and do it with all the love and discipline.\n\n\nYeah buddy, light weight baby"

    },
  
  
  
  
    {

      "title"    : "Rewire stress resilience and unwind anxiety",
      "url"      : "/journals/deal-with-anxiety",
      "index"    : "peach",
      "content"  : "Hang in there! \n\nIt’s funny how I want to start the post with “after the pandemic,” although it’s been a while since covid is no longer a significant concern, fair to say it has had a long-lasting effect on our lives.\n\nDo you\n\n  often worry about things that are very unlikely to happen?\n  criticize yourself, but you are kind to others?\n  have a wave of negative thoughts that things will turn out badly?\n  overthink and procrastinate to either take a long time to act on it or avoid it altogether?\n  have trouble sleeping because you can’t stop thinking and get lost in the train of thoughts?\n\n\nIf you answered yes to more than one of these, you are likely dealing with some level of stress and anxiety. Although that doesn’t mean it’s doomsday but rather uncomfortable and irritable.\n\nAnd now, to break it down, identify the underlying problem, and look at probable solutions:\n\n 1. Anxiety and Stress\n\nPsychologists and researchers mostly define it as an emotional state characterized by feelings of tension, worried thoughts, and physical changes such as sweating, trembling, dizziness, or a rapid heartbeat. It's the human body's physical response to danger. And it's made up of three components: thoughts, feelings, and behaviors [1]. \n\nOn the other hand, stress is not far apart and often co-related with anxiety; after all, it's the response our bodies have to any change, and it's not necessarily bad. The concern is when it's for a prolongated period. When we have less control over the situation creating the stress we are experiencing, our stress reaction is likely to be more intense. Over time, some common symptoms of untreated stress include headaches, skin breaks/rashes, forgetfulness, digestive issues, and lack of focus [5]. \n\nBefore we get down to better understand the causes and solutions/exercises to \"Rewire stress resilience and unwind anxiety.\" \n\nThe top 3 changes in everyday lifestyle \n\n\n Cut down on coffee and energy drinks (caffeine); by cut down, I mean to reduce, of course. \n Build a routine to include physical exercise, such as working out at the gym, running, swimming, or playing a sport, whichever is sustainable. \n Get at least 6 hours of sleep every day; sleep deprivation is not only a symptom of anxiety but also a cause. \n\n\nAlthough those 3 points came out of nowhere, I had to make sure to highlight these as they make an insanely significant difference in a relatively shorter period of time. \n\n\n\n\n\n 2. Rewire stress resilience\n\nTo better understand `Stress Response`, a quick overview of fight-or-flight and feed-and-breed response is a must:\n\nThe fight-or-flight response is an automatic physiological reaction to an event perceived as stressful or frightening (helps the body deal with threats). The perception of threat activates the sympathetic nervous system and triggers an acute stress response that prepares the body to fight or flee; the common signs are tense muscles, tight chest, pounding heart, cold sweats, the urge to run away/freeze, or just plain anger. \n\nInstead, what we want is the feed-and-breed response. The SNS (Sympathetic Nervous System) switches to fight-or-flight, and PNS (Parasympathetic Nervous System) activates the feed-and-breed response, like a braking system that lets us relax and slow down [2]. \n\nSo, how do I activate feed-and-breed response instead of fight-or-flight? \n\nBox breathing exercise \n\n Breathe in for 4 seconds \n Hold your breath for 4 seconds \n Breathe out for 4 seconds \n Hold your lungs empty for 4 seconds \n Repeat for 3-5 minutes. \n\n\nBreakthrough bias exercise \n\n Take a couple of deep breaths, concentrate on breathing and try not to get carried away with the stream of thoughts. \n Now, look around, wherever you are, look at 5 different objects around you; it could be one of your gadgets, pictures, or even a lamp post, don't just glance, but observe individual objects one after the other. \n Choose one object of your choice and zero in on it, focus on it, observe the colors, follow the shape, and imagine the texture. \n Zoom out on the object and focus. Notice the changes in your body; does it feel more relaxed? Forehead and eyebrows relaxed, stomach and chest feel less tense, maybe? Observe any changes at all in your body. \n Does it feel relaxed in several parts of your body? Take a couple of deep breaths again, stay in the zone for some more time, and get out of it when you feel like it in a few seconds or even minutes. \n\n\n\n\n\n\n 3. Dealing with negative thoughts\nEver get into a loop of negative thoughts? So often, we consider these thoughts as facts, although they aren't rational in most cases. These untrue thoughts are called cognitive distortions and constantly fuel the inner critic (the inner voice within you affirming and stating untrue negative thoughts as if they are facts).\n\nSo, how do I reprogram negative thoughts?\n\nCBT (Cognitive Behavioral Therapy) - usually involves efforts to change thinking patterns to understand and change negative perceptions/thoughts [3].\n\nFact check exercise \n\n Try to corner down and identify the common negative thoughts. \n Now, take one of them and try to reason it with logic. Is there enough evidence to make that claim? If so, is there a possible explanation or circumstance for it? \n Form an alternative explanation for these thoughts that makes sense to you. \n\n\nThe key here is to fact-check, give a rational perspective and defend your alternative thinking.\n\nCFT (Compassion Focused Therapy) - is more about finding the root cause or the source of distress and observing it without any judgments. From there, it's only a matter of figuring out what needs to be done to make it right.\n\nInner Coach Excercise \nAdding more points to the fact-check exercise.\nHave a debate with yourself; this time, defend the negative thought, and you'll likely see the inner critic show up (you get to understand the presence of the inner critic better); now, defend the alternative thought with logical reasoning, consider this to be your inner coach/cheerleader.\n\nThe alternative thought is not just a bunch of positive stuff said with affirmation. Instead, have a proper conversation, and experience a back-and-forth between the inner critic and the inner coach for a psychological shift in \"thinking patterns.\" [4]\n\nLast but not the least \n\n\n\n\n 4. Meditation\nStill unexplored territory for me, but I've only heard promising whispers about the wonders of meditation 🧘🏽‍♂️\n\n\n\n\n 5. References\n\n\n[1] American Psychological Association, “Anxiety,” American Psychological Association, 2022. [Online]. Available: https://www.apa.org/topics/anxiety\n[2] L. K. McCorry, “Physiology of the Autonomic Nervous System,” American Journal of Pharmaceutical Education, vol. 71, no. 4, p. 78, Sep. 2007, doi: 10.5688/aj710478.\n[3] American Psychological Association, “What Is cognitive behavioral therapy?,” American Psychological Association, Jul. 2017. [Online]. Available: https://www.apa.org/ptsd-guideline/patients-and-families/cognitive-behavioral\n[4] “How To Silence Your Inner Critic | BetterHelp,” www.betterhelp.com. https://www.betterhelp.com/advice/self-esteem/how-to-silence-your-inner-critic/ (accessed Nov. 15, 2022).\n[5]“What Is The Difference Between Stress And Anxiety? | BetterHelp,” www.betterhelp.com. https://www.betterhelp.com/advice/stress/what-is-the-difference-between-stress-and-anxiety (accessed Nov. 16, 2022).\n\n\n\n\nThis post is not intended to give medical advice and does not substitute professional help. It's based on my own research, understanding and personal experience that worked well for me, along with a bunch of friends and family :)"

    },
  
  
  
  
    {

      "title"    : "Better Fitness; Better Programming",
      "url"      : "/journals/better-fitness-better-programming",
      "index"    : "redapple",
      "content"  : "There's more to bro-split!\n\nGreetings 🖖🏽 fellow coders! On the other side of all the tech in the world are software engineers who spend countless hours in front of their computers, diving deep into solving problems, and often feeling the strain of long, sedentary days. Early in my career, I thought grinding through hours of coding without breaks was the only way to get things done. But I’ve come to realize that staying physically active has made me a better programmer. Let’s talk about how fitness can transform your life as a software engineer.\n\nThe Turning Point\nPicture this: you're buried in a particularly challenging project, pulling late nights, surviving on coffee and takeout, and spending most of your days glued to your chair. Eventually, it starts to take a toll. You feel constantly tired, your back hurts, and your mind is always foggy. Sound familiar? Something has to change - RIGHT NOW! 😵‍💫\n\n\n\nThe Discovery\nOne day, a friend might convince you to join them for a morning run or an evening workout. At first, it seems like a crazy idea. How could running possibly help with coding? But after that first run, you might feel surprisingly refreshed. Your mind feels clearer, and you feel more energized than you have in weeks.\n\nStart running regularly, and soon, you notice significant changes. Your focus improves, and you can solve problems more efficiently. It's like your brain is firing on all cylinders. The exercise sharpens your cognitive abilities, making it easier to learn new concepts and tackle complex algorithms.\n\n\n\nStaying Active: A Secret Productivity Hack\nWe all know that sitting for hours on end isn't doing us any favors. After a long coding session, it's common to feel sluggish and drained. But incorporating regular exercise into your routine can make a huge difference, by huge, I mean astronomical 🚀\n\nTake short, active breaks get a pull-up bar 🤫 during your workday. A quick walk around the block or a few minutes of stretching can do wonders. These breaks help you stay focused and productive throughout the day, preventing that mid-afternoon slump.\n\n\n\nWhy Software Engineers Should Get Moving\nSo, why should we, as software engineers, prioritize fitness? Here are a few reasons based on experiences that many of us share:\n\nStress Buster: At times, coding can be super stressful, especially when deadlines are looming. Regular exercise helps manage stress much better. After a good workout, those stressors don’t seem as daunting.\nBoosted Creativity: Some of the best ideas come while away from the desk. Whether on a run or taking a leisurely walk, being active seems to spark creativity. Perfect for brainstorming a new feature or solving a tricky problem.\nTeam Spirit: Playing team sports or joining group fitness classes can improve communication and teamwork skills. Great news for collaborating on projects at work.\n\n\n\nFitting Fitness into a Busy Schedule\nFinding time to exercise can be tough when busy coding, but it doesn’t have to be a chore. Here are some tips that have worked for many:\n\nSet Small Goals: Start with manageable fitness goals and gradually build up. Begin with a 10-minute walk each day, and then increase as you feel more comfortable.\nDo What You Love: Find an activity you enjoy, whether it’s hitting the gym 🏋🏽, rock climbing 🧗🏽‍♂️, yoga 🧘🏽, swimming 🏊🏼‍♀️, or even dancing 🏊🏼‍♀️. If it’s fun, you’re more likely to stick with it.\nActive Breaks: Take short breaks throughout your day to stretch or take a quick walk. It’ll help you stay refreshed and focused.\nJoin a Community: Working out with others can make it more enjoyable and keep you motivated. Maybe there’s a local running group or a friend who wants to join for workouts.\n\nSo, there you have it! Better fitness really can lead to better programming. By keeping your body and mind in top shape, you’ll be ready to tackle any coding challenge that comes your way. Give it a try and see how getting active can improve your coding game."

    },
  
  
  
  
    {

      "title"    : "Back in Black",
      "url"      : "/journals/startup-life",
      "index"    : "avocado",
      "content"  : "I'm super pumped to say that I'm joining an early-stage startup soon  The co-working space is buzzing, downtown vibes are incredible, and the drive to make an impact is real!\n\nI'll be keeping this post open-ended to share interesting experiences, and lessons learned along the way—be it challenges, triumphs, or even the unexpected moments."

    }
  
]