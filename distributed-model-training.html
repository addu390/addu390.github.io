<!doctype html>
<html lang="en"><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Jekyll">
    <link rel="license" href="LICENSE">

    <meta name="application-name" content="PYBLOG">
    <meta name="theme-color" content="">
    <link rel="archives" href="https://pyblog.xyz/archives/">
    <link rel="me" href="https://mastodon.social/@addu390">

    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">

    <link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/pure-min.css">
    <link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/grids-responsive-min.css">
    <link rel="stylesheet" href="/assets/main.css">
    <link rel="stylesheet" href="/assets/code.css">
    <link rel="alternate" type="application/rss+xml" href="https://pyblog.xyz/feed.xml">
    <!-- TODO: Upgrade Google Analytics -->
    <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Distributed Model Training | PYBLOG</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Distributed Model Training" />
<meta name="author" content="Adesh Nalpet Adimurthy" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Porco Rosso." />
<meta property="og:description" content="Porco Rosso." />
<link rel="canonical" href="https://pyblog.xyz/distributed-model-training" />
<meta property="og:url" content="https://pyblog.xyz/distributed-model-training" />
<meta property="og:site_name" content="PYBLOG" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-06-26T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Distributed Model Training" />
<meta name="twitter:site" content="@" />
<meta name="twitter:creator" content="@Adesh Nalpet Adimurthy" />
<script type="application/ld+json">
{"url":"https://pyblog.xyz/distributed-model-training","@type":"BlogPosting","description":"Porco Rosso.","author":{"@type":"Person","name":"Adesh Nalpet Adimurthy"},"headline":"Distributed Model Training","dateModified":"2022-06-26T00:00:00+00:00","datePublished":"2022-06-26T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://pyblog.xyz/distributed-model-training"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LWDR3LD16H"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-LWDR3LD16H');
    </script>
    <script src="https://cdn.jsdelivr.net/npm/darkreader@4.9.44/darkreader.min.js"></script>
    <script src="/assets/main.js"></script>

    <!-- MailerLite Universal -->
    <script async>
        (function (w, d, e, u, f, l, n) {
            w[f] = w[f] || function () {
                (w[f].q = w[f].q || [])
                    .push(arguments);
            }, l = d.createElement(e), l.async = 1, l.src = u,
                n = d.getElementsByTagName(e)[0], n.parentNode.insertBefore(l, n);
        })
            (window, document, 'script', 'https://assets.mailerlite.com/js/universal.js', 'ml');
        ml('account', '1023335');
    </script>
    <!-- End MailerLite Universal -->
</head><body>

    <div id="layout" class="pure-g"><aside class="sidebar pure-u-1 pure-u-md-1-5">
    <div class="sidebar-info">
        <span hidden>PYBLOG</span>
        <img title="Start/Stop tear of joy!" id="profile-gif" class="brand-photo"
            src="https://pyblog.xyz/assets/img/profile/3.gif" alt="Photo of Adesh Nalpet Adimurthy" />
        <h1 class="brand-title">
            <a href="/">· PYBLOG ·</a>
        </h1>
        <br>
        <h2 class="brand-tagline">How do you know which watermelon <img id="showerButton" class="twemoji" src="https://pyblog.xyz/assets/img/emoji/watermelon.svg" alt=""> to pick?</h2>
        <p></p>
    </div>

    <ul class="sidebar-list">
        <li class="clickable-div high-featured" onclick="location.href='https://pyblog.xyz';"><span class="sidebar-index"><img
                    class="twemoji" src="https://pyblog.xyz/assets/img/emoji/home.svg" alt="home"></span>&nbsp; Home</li>
        <li class="clickable-div low-featured" onclick="location.href='https://pyblog.xyz/journals';"><span
                class="sidebar-index"><img class="twemoji" src="https://pyblog.xyz/assets/img/emoji/book.svg"
                    alt="journal"></span>&nbsp;
            Journal</li>
        <li class="clickable-div high-featured" onclick="location.href='https://pyblog.xyz/subscribe';"><span
                class="sidebar-index"><img class="twemoji" src="https://pyblog.xyz/assets/img/emoji/loveletter.svg"
                    alt="subscribe"></span>&nbsp; Subscribe</li>


        <li class="clickable-div low-featured" onclick="location.href='https://pyblog.xyz/contact';"><span
                class="sidebar-index"><img class="twemoji" src="https://pyblog.xyz/assets/img/emoji/horn.svg"
                    alt="contact"></span>&nbsp;
            Contact</li>

        <li class="clickable-div low-featured" onclick="location.href='https://pyblog.xyz/about';"><span
                class="sidebar-index"><img style="vertical-align: top; width: 21px;" class="twemoji"
                    src="https://pyblog.xyz/assets/img/emoji/face.svg" alt="archive"></span>&nbsp;
            About</li>
        <li class="clickable-div low-featured"><a href="https://www.buymeacoffee.com/pyblog" target="_blank"><span
                    class="sidebar-index"><img class="twemoji" src="https://pyblog.xyz/assets/img/emoji/luck.svg"
                        alt="sponsor"></span>&nbsp;
                Sponsor</a></li>
        <li class="clickable-div low-featured"><a href="https://medium.com/@pyblog/lists" target="_blank"><span
                    class="sidebar-index"><img class="twemoji" src="https://pyblog.xyz/assets/img/emoji/medium.svg"
                        alt="medium"></span>&nbsp;
                Medium</a></li>
        <li class="clickable-div low-featured" onclick="location.href='https://pyblog.xyz/notes';"><span
                class="sidebar-index"><img class="twemoji" src="https://pyblog.xyz/assets/img/emoji/notes.svg"
                    alt="notes"></span>&nbsp;
            Notes</li>
        <li class="clickable-div low-featured" onclick="location.href='https://pyblog.xyz/privacy';"><span
                class="sidebar-index"><img class="twemoji" src="https://pyblog.xyz/assets/img/emoji/cookie.svg"
                    alt="privacy policy"></span>&nbsp;
            Privacy Policy</li>
        <li class="clickable-div low-featured" onclick="location.href='https://pyblog.xyz/categories';"><span
                class="sidebar-index"><img class="twemoji" src="https://pyblog.xyz/assets/img/emoji/folder.svg"
                    alt="categories"></span>&nbsp;
            Categories</li>
        <li class="clickable-div low-featured" onclick="location.href='https://pyblog.xyz/archive';"><span
                class="sidebar-index"><img class="twemoji" src="https://pyblog.xyz/assets/img/emoji/archive.svg"
                    alt="archive"></span>&nbsp;
            Archive</li>
    </ul>
</aside>

<script src="/assets/sidebar.js"></script><div class="content pure-u-1 pure-u-md-4-5">
            <div id="showerContainer"></div>
            <div class="md-display"><div class="menu"><div id="search">
    <label>
        <svg class="icon svg-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" class="crayons-icon c-btn__icon" focusable="false"><path d="M18.031 16.617l4.283 4.282-1.415 1.415-4.282-4.283A8.96 8.96 0 0111 20c-4.968 0-9-4.032-9-9s4.032-9 9-9 9 4.032 9 9a8.96 8.96 0 01-1.969 5.617zm-2.006-.742A6.977 6.977 0 0018 11c0-3.868-3.133-7-7-7-3.868 0-7 3.132-7 7 0 3.867 3.132 7 7 7a6.977 6.977 0 004.875-1.975l.15-.15z"></path></svg>
        <input autocomplete="off" id="search-input" oninput="changeResultContainerDisp(this.value)" placeholder=" Search..." type="text"/>
        <span class="search-power-text">Powered by Simple Search</span>
    </label>
</div>
<ul id="results-container" style="display: none"></ul>
<script src="/assets/simple-jekyll-search.js"></script>
<script>
    function changeResultContainerDisp(val) {
        if (val) {
            document
                .getElementById("results-container")
                .style
                .display = "block";
            document.getElementById("search-input").addEventListener("blur", function () {
                document.addEventListener("click", function (event) {
                    var isClickInside = document.getElementById("results-container").contains(event.target);
                    if (! isClickInside) {
                        document
                            .getElementById("results-container")
                            .style
                            .display = "none";
                    }
                })
            })
        } else {
            document
                .getElementById("results-container")
                .style
                .display = "none";
        }
    }
    var sjs = SimpleJekyllSearch({
        searchInput: document.getElementById("search-input"),
        resultsContainer: document.getElementById("results-container"),
        json: "/search.json",
        searchResultTemplate: "<li class='search_res' style='list-style: none;'><a href='https://pyblog.xyz{url}' color: #555555;'><p><span class='dice-index'><img class='twemoji' src='../assets/img/emoji/{index}.svg'></span> &nbsp;{title}</p></a></li>",
        noResultsText: "No results found. Try another search.",
        fuzzy: false,
        limit: 5
    })
</script><a id="dark-mode-button">
        <img id="icon-light" class="twemoji theme-icon" src="https://pyblog.xyz/assets/img/emoji/light.svg" alt="">
        <img id="icon-dark" class="twemoji theme-icon" src="https://pyblog.xyz/assets/img/emoji/dark.svg" style="display: none;" alt="">
    </a>
    <a href="/feed.xml" target="_blank"><img class="svg-icon twemoji rss-icon" src="https://pyblog.xyz/assets/img/common/rss.svg"
            alt=""></a>
    <a id="sound-button">
        <img id="icon-sound-on" class="twemoji audio-icon svg-icon" src="https://pyblog.xyz/assets/img/common/audio-on.svg" style="display: none;" alt="">
        <img id="icon-sound-off" class="twemoji audio-icon svg-icon" src="https://pyblog.xyz/assets/img/common/audio-off.svg" alt="">
    </a>
</div>

<audio id="darkModeOn">
    <source src="https://pyblog.xyz/assets/sounds/switch-on.mp3" type="audio/mpeg">
    Your browser does not support the audio element.
</audio>

<audio id="darkModeOff">
    <source src="https://pyblog.xyz/assets/sounds/switch-off.mp3" type="audio/mpeg">
    Your browser does not support the audio element.
</audio>

<audio id="soundModeOn">
    <source src="https://pyblog.xyz/assets/sounds/enable-sound.mp3" type="audio/mpeg">
    Your browser does not support the audio element.
</audio>

<audio id="soundModeOff">
    <source src="https://pyblog.xyz/assets/sounds/disable-sound.mp3" type="audio/mpeg">
    Your browser does not support the audio element.
</audio></div>
            <div>
                <div></div><div class="pure-g post-entry-toc" style="width: 100%; justify-content: center">
    <div class="posts pure-u-md-5-6">
        <article class="post-entry" itemscope itemtype="http://schema.org/BlogPosting">

            <header class="feed-header">
                <h2 class="post-title" itemprop="name headline">Distributed Model Training</h2><p class="post-meta" data-id="/distributed-model-training" data-url="/distributed-model-training" data-title="Distributed Model Training">
    <time datetime="2022-06-26T00:00:00+00:00" itemprop="datePublished">
        Jun 26, 2022
    </time><b class="post-meta-dots">·</b>
    <a href="https://pyblog.xyz/categories/system-wisdom">System Wisdom</a><b class="post-meta-dots">·</b>
    <a href="https://pyblog.xyz/tags/system-design"> #System Design</a>&#8239; 
    <a href="https://pyblog.xyz/tags/machine-learning"> #Machine Learning</a><span class="save-button float-right"><img class="twemoji svg-icon" src="https://pyblog.xyz/assets/img/common/bookmark-initial.svg" alt=""></span>
    <span class="float-right">7 min read</span>
</p></header>

            <div articlebody" class="post-body itemprop=">
                <p><img class="center-image" src="./assets/featured/porco-rosso.png" /></p>
<p style="text-align: center;">Porco Rosso. </p>

<h2 id="distributed-training">Distributed Training</h2>
<p>Deep learning is a subset of machine learning, a branch of artificial intelligence to perform tasks through experience. Deep learning algorithms are well suited and perform the best with large datasets, not to mention the need for high computation power. With the pay-per-use serverless service model, such as the google collab, training large neural networks on the cloud is easier than ever.
While it’s possible to train huge models in a single multi-core GPU machine, it could take days and even weeks. Hence, this leads to the fundamental problem of reducing the training time.</p>

<p>Typically, any scaling problem is broadly addressed by scaling-up or scaling-out, i.e., horizontal and vertical scaling. Depending on the use case, vertical scaling has the limitation of maxing out at a point and often tends to be a lot more expensive in the long run, both in price and technical backlog.</p>

<p><strong>One-liner:</strong> Distributed training distributes training workloads across multiple computation processors. Where a cluster of worker nodes works in parallel to accelerate the training process, parallelism is achieved by data parallelism or model parallelism.</p>

<hr class="hr" />

<h2 id="types-of-distributed-training">Types of Distributed Training</h2>
<h3 id="data-parallelism">Data Parallelism</h3>
<p>As the name suggests, the dataset is horizontally/vertically sharded and processed parallelly. Each worker node in the cluster trains a copy of the model on a different batch of training data, communicating the computation results to keep the model parameters and gradients in sync across all nodes. The computation results can be shared synchronously, i.e., at the end of each batch computation or asynchronously.</p>

<p><img class="center-image" src="./assets/posts/machine-learning/data-parallel-training.png" /></p>
<p style="text-align: center;">Figure 1: Data-Parallel training. </p>

<p><strong>One-liner:</strong> The entire model is deployed to multiple nodes of the cluster, and each node represents the horizontal/vertical split of the sharded dataset and the model.</p>

<h3 id="model-parallelism">Model Parallelism</h3>
<p>On the contrary, in model parallelism, the model itself is divided into parts/layers in situations where the model size is too large for a single worker; hence a set of layers are trained simultaneously across different worker nodes. The entire dataset is copied/available to all worker nodes, and they only share the global model parameters with other workers—typically just before forward or backward propagation. Furthermore, the layers can be partitioned vertically or horizontally.</p>

<p><img class="center-image" src="./assets/posts/machine-learning/model-parallel-training.png" /></p>
<p style="text-align: center;">Figure 2: Model-Parallel training. </p>

<p><strong>One-liner:</strong> A layer or a group of layers of the model is deployed to multiple nodes of the cluster, and the entire dataset is copied to every node.</p>

<p><img class="center-image" src="./assets/posts/machine-learning/model-partitioning.png" /></p>
<p style="text-align: center;">Figure 3: Model-Partitioning horizontally or
vertically. </p>

<p>Among the two, data parallelism is commonly used and easier to implement. The ability to train a model in batches of data (non-sequential) and contribute to the overall performance of the model is the crux of the solution. In other words, the model parameters and gradients are calculated for every small batch of data in the worker node, and at the end of it → updated weights are sent back to the initiating node → the weighted average/mean of the weights from each worker node is applied to the model parameters → updated model parameters are sent back all worker nodes for the next iteration; this leads to questions about how and when model parameters are stored and updated.</p>

<hr class="hr" />

<h2 id="distributed-training-loops">Distributed Training Loops</h2>
<p>The two ways of carrying out distributed training loops are as follows:</p>

<h3 id="synchronous-training">Synchronous training</h3>
<p>Once again, taking the example of data parallelism, where we divide the data into partitions/batches for each worker node to process. Every worker node has a full replica of the model and the batch of data.</p>

<ul>
  <li>The forward pass starts at the same time for all workers, and each worker node computes the gradients (Output).</li>
  <li>Workers wait until all the other workers have completed the training loop. Then, once all the workers have computed the gradients, they start communicating with each other to aggregate the gradients.</li>
  <li>After all the gradients are combined, a copy of the updated gradients is sent to all the workers.</li>
  <li>Then, each worker continues with the backward pass and updates the local copy of the weights.</li>
  <li>Until all the workers have updated their weights, the next forward pass does not start; hence the name “synchronous”.</li>
</ul>

<p>Note: All the workers produce different gradients as they are trained on different subsets of data, and eventually, all workers have the same weight.</p>

<h3 id="reduce-algorithm">Reduce Algorithm</h3>
<p>Typically, a single node is used to complete aggregation. For instance, in the case shown in Figure 3, the bandwidth for Machine A increases as the number of machines/parameters increases.</p>

<p><img class="center-image" style="width: 65%" src="./assets/posts/machine-learning/single-reduce.png" /></p>
<p style="text-align: center;">Figure 4: Single node aggregator.</p>

<p>Following up on the reduce-algorithm mentioned in synchronous training, the idea behind the all-reduce algorithm is to share the load of storing and maintaining the global parameters to overcome the limitation of using the parameter server method. There are serval all-reduce algorithms that dictate how parameters are calculated and shared:</p>

<p><img class="center-image" style="width: 45%" src="./assets/posts/machine-learning/all-reduce.png" /></p>
<p style="text-align: center;">Figure 5: All Reduce: Aggregation task distributed to all nodes instead of a single node.</p>

<p>Like AllReduce, each node performs the aggregation task on a subset of parameters: machine A – parameter 1, machine B – parameter 2, etc. Instead of sending its version of parameters to all other nodes, each worker node sends its version to the next one.</p>

<p><img class="center-image" style="width: 45%" src="./assets/posts/machine-learning/ring-all-reduce.png" /></p>
<p style="text-align: center;">Figure 6: Ring All Reduce.</p>

<p>Similarly, in tree-all-reduce, parameters are shared via a tree structure. Irrespective of the topology, all-reduce algorithms reduce synchronization overhead and make it easier to scale horizontally.</p>

<p><img class="center-image" style="width: 65%" src="./assets/posts/machine-learning/tree-all-reduce.png" /></p>
<p style="text-align: center;">Figure 7: Tree All Reduce.</p>

<p>Each worker node holds a subset of data and computes the gradient(s); those values are passed up the tree and aggregated until a global aggregate value is calculated in the root node. Then, the global value is passed down to all other nodes.</p>

<h3 id="asynchronous-training">Asynchronous training</h3>
<p>The evident problem with the synchronous approach is the lack of efficient resource usage since a worker must wait for all the other workers in the cluster to move forward in the pipeline. Furthermore, the problem amplifies when the computation time for workers is significantly different, which could be because of dataset or computation power variations - because of which the whole process is only as fast as the slowest worker in the cluster. Hence in asynchronous training, the workers work independently in such a way that a worker need not wait for any other worker in the cluster. One way to achieve this is by using a parameter server.</p>

<hr class="hr" />

<h2 id="communication-approaches">Communication Approaches</h2>
<p>The two communication approaches, centralized and de-centralized patterns, apply to both data-parallel and model-parallel training. The key here is the communication between the worker nodes, how the parameters are initiated, and how the weights/biases are updated.</p>

<h3 id="centralized-training">Centralized Training</h3>
<p>In distributed training, the cluster of workers performs just one task: training. However, in the centralized communication pattern, we assign a different role to each worker, where some workers act as parameter servers and the rest as training workers.</p>

<p>The parameter servers are responsible for holding the parameters of the model and are responsible for updating the global state of our model. At the same time, the training workers run the actual training loop and produce the gradients from the batch of data assigned to them.</p>

<p><img class="center-image" src="./assets/posts/machine-learning/centralized-data-parallel-training.png" /></p>
<p style="text-align: center;">Figure 8: Centralized training. </p>

<p>Hence the entire process for Centralized data-parallel training is as follows:</p>
<ul>
  <li>Replicate the model across the training worker nodes; each worker node uses a subset of the data.</li>
  <li>Each training worker fetches the parameters from the parameter server(s).</li>
  <li>Each training worker node performs a training loop and sends back the gradients to all parameter servers.</li>
  <li>Parameter servers update the model parameters and ensures all the worker models are in sync.</li>
</ul>

<p>Some known disadvantages are:</p>
<ul>
  <li>At a given point in time, only one of the workers may be using the updated version of the model, while the rest use a stale version.</li>
  <li>Using only one worker as a parameter server can become a bottleneck and lead to a single point of failure.</li>
</ul>

<h3 id="de-centralized-training">De-centralized Training</h3>
<p>On the flip side, In a de-centralized communication pattern, each worker node communicates with every other node to update the model parameters. The advantage of this approach is that peer-peer updates are faster, and there is no single point of failure.</p>

<p><img class="center-image" src="./assets/posts/machine-learning/decentralized-data-parallel-training.png" /></p>
<p style="text-align: center;">Figure 9: De-centralized training. </p>

<hr class="hr" />

<h2 id="conclusion">Conclusion</h2>
<p>Deep learning models become more ambitious by the day, and their supporting infrastructures struggle to keep up. Employing distributed model training techniques is only a matter of time to solve the problem of training a complex machine learning model on huge datasets. Moreover, the advantages supersede the development time/bandwidth with better Fault tolerance and reliability, higher Efficiency,  horizontally scalable to handle massive scale, and cost-effective in the long run.</p>

<hr class="hr" />

<h2 id="references">References</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] “Distributed Training: Guide for Data Scientists,” neptune.ai, Jan. 19, 2022. https://neptune.ai/blog/distributed-training (accessed Jun. 23, 2022).

[2] “Distributed Training,” www.run.ai. https://www.run.ai/guides/gpu-deep-learning/distributed-training (accessed Jun. 24, 2022).

[3] “Distributed Training for Machine Learning – Amazon Web Services,” Amazon Web Services, Inc. https://aws.amazon.com/sagemaker/distributed-training/ (accessed Jun. 26, 2022).

[4] “Distributed model training II: Parameter Server and AllReduce – Ju Yang.” http://www.juyang.co/distributed-model-training-ii-parameter-server-and-allreduce/ (accessed Jun. 26, 2022).
</code></pre></div></div>

            </div>
            <img style="float: right; width: 5em;" src="../assets/img/common/puppy.gif" />
            <a class="u-url" href="/distributed-model-training" hidden></a>
        </article>
        <div class="blog-reference">
            <p>Cite this article as: Adesh Nalpet Adimurthy. (Jun 26, 2022). Distributed Model Training.
                PyBlog. <a href="/distributed-model-training">https://www.pyblog.xyz/distributed-model-training</a>
            </p>
        </div>
        <div id="comments-section"></div>
        <div id="comments-script">
            <script src="https://utteranc.es/client.js" repo="addu390/addu390.github.io" issue-term="pathname"
            theme="github-light" crossorigin="anonymous" async>
        </script>
        </div>
        
    </div>
    <div class="posts-right-bar pure-u-md-1-6" style="display: flex; flex-direction: column;">
        <section class="post sidebar-scrollable-div sidebar-toc-div">
            <h3><img class="twemoji" src="../assets/img/emoji/snake.svg" alt=""> #index <span class="sub-header">table
                    of contents</span></h3>
            <div id="toc">
                <!-- Table of Contents will be injected here -->
            </div>
        </section>
    </div>
</div>
<script src="/assets/toc.js"></script>
            </div>
        </div>
    </div>
    <script src="/assets/reading-list.js"></script>
</body>

</html>